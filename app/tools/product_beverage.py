import os
import logging
import asyncio
import json
import re
from pinecone import Pinecone
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np

# ‚≠ê Import ApiKeyManager CH√çNH X√ÅC
from app.services.api_key_manager import get_api_key_manager
# ‚≠ê IMPORT GEMINI MODEL POOL ƒë·ªÉ thread-safe API access
from app.tools.gemini_model_pool import get_gemini_model_pool

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- C·∫•u h√¨nh ---
PINECONE_API_KEY = os.getenv("PRODUCT_DB_PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "product-index")
EMBEDDING_MODEL_FOR_DIM_ONLY = "sentence-transformers/all-mpnet-base-v2"
# ‚≠ê T·ªêI ∆ØU H√ìA M·∫†NH: Gi·∫£m batch size ƒë·ªÉ tr√°nh l·ªói JSON v√† token limit
PRODUCTS_TO_FETCH_FROM_PINECONE = 1610
PRODUCTS_PER_GEMINI_BATCH = 60           # Gi·∫£m t·ª´ 100 xu·ªëng 60
MAX_GEMINI_OUTPUT_TOKENS = 4096          # Gi·∫£m t·ª´ 8192 xu·ªëng 4096

# ‚≠ê C·∫§U H√åNH ASYNC V√Ä CONCURRENCY - TƒÉng ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa API keys
MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS = 7  # TƒÉng t·ª´ 6 l√™n 7 ƒë·ªÉ match v·ªõi s·ªë API keys

# ‚≠ê DYNAMIC BATCHING CONSTANTS - th√™m ƒë·ªÉ t∆∞∆°ng ƒë·ªìng v·ªõi recipe_tool
MAX_CHAR_PER_BATCH = 150000              # Gi·ªõi h·∫°n k√Ω t·ª± cho m·ªói batch
MAX_SAFE_BATCH_SIZE = 50                 # Fallback limit n·∫øu dynamic batching fails

# ‚≠ê CONFIGURATION CHO PARALLEL PROCESSING
WORKER_SLEEP_BETWEEN_TASKS = 0.1  # Gi·∫£m ƒë·ªÉ tƒÉng throughput
WORKER_ERROR_SLEEP = 2  # Sleep khi c√≥ l·ªói

# ‚≠ê MODEL NAME CONSTANT
GEMINI_MODEL_NAME = "gemini-2.0-flash-lite"

# ‚≠ê KH·ªûI T·∫†O GEMINI MODEL POOL
gemini_model_pool = get_gemini_model_pool(GEMINI_MODEL_NAME)

def get_embedding_model_for_dimension():
    """‚≠ê L·∫•y embedding model ƒë·ªÉ x√°c ƒë·ªãnh dimension vector"""
    try:
        # Import function ƒë·ªÉ l·∫•y global embedding model
        from main import get_global_embedding_model
        global_model = get_global_embedding_model()
        
        if global_model is not None:
            logger.info("‚úÖ S·ª≠ d·ª•ng pre-loaded embedding model ƒë·ªÉ x√°c ƒë·ªãnh dimension")
            return global_model
        else:
            logger.warning("‚ö†Ô∏è Global embedding model ch∆∞a ƒë∆∞·ª£c load, t·∫°o m·ªõi ƒë·ªÉ x√°c ƒë·ªãnh dimension...")
            return HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_FOR_DIM_ONLY,
                model_kwargs={'device': 'cpu'}
            )
    except ImportError:
        logger.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ import global embedding model, t·∫°o m·ªõi...")
        return HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_FOR_DIM_ONLY,
            model_kwargs={'device': 'cpu'}
        )

def init_services():
    """‚≠ê Kh·ªüi t·∫°o k·∫øt n·ªëi Pinecone v√† l·∫•y dimension c·ªßa vector. KH√îNG tr·∫£ v·ªÅ gemini_model."""
    try:
        # ‚≠ê KI·ªÇM TRA API KEY MANAGER
        api_key_manager = get_api_key_manager()
        if not api_key_manager.is_healthy():
            logger.error("ApiKeyManager kh√¥ng kh·ªèe m·∫°nh ho·∫∑c kh√¥ng c√≥ API key")
            raise ValueError("ApiKeyManager kh√¥ng kh·ªèe m·∫°nh")
        logger.info(f"‚úÖ ApiKeyManager ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n kh·ªèe m·∫°nh v·ªõi {api_key_manager.total_keys()} keys")
        
        pc = Pinecone(api_key=PINECONE_API_KEY)
        
        # C·∫£i thi·ªán logic ki·ªÉm tra index
        try:
            index_list = pc.list_indexes()
            index_names = []
            
            if hasattr(index_list, 'indexes') and isinstance(index_list.indexes, list):
                index_names = [idx.get('name') for idx in index_list.indexes if isinstance(idx, dict) and idx.get('name')]
            elif hasattr(index_list, 'names'):
                if callable(index_list.names):
                    index_names = index_list.names()
                else:
                    index_names = index_list.names
            
            if PINECONE_INDEX_NAME not in index_names:
                logger.error(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i trong danh s√°ch: {index_names}")
                raise ValueError(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i.")
                
        except Exception as e_list:
            logger.warning(f"Kh√¥ng th·ªÉ ki·ªÉm tra danh s√°ch index: {e_list}. Th·ª≠ describe_index tr·ª±c ti·∫øp...")

        index_description = pc.describe_index(PINECONE_INDEX_NAME)
        logger.info(f"Pinecone index '{PINECONE_INDEX_NAME}' ƒë√£ t·ªìn t·∫°i. Th√¥ng tin: {index_description}")
        pinecone_index = pc.Index(PINECONE_INDEX_NAME)
        vector_dimension = index_description.dimension
        
        if not vector_dimension:
            logger.info("Kh√¥ng l·∫•y ƒë∆∞·ª£c dimension t·ª´ describe_index, th·ª≠ t·∫£i embedding model...")
            try:
                # ‚≠ê S·ª¨ D·ª§NG GLOBAL EMBEDDING MODEL
                temp_embeddings_model = get_embedding_model_for_dimension()
                sample_embedding = temp_embeddings_model.embed_query("sample text")
                vector_dimension = len(sample_embedding)
                logger.info(f"Dimension c·ªßa vector x√°c ƒë·ªãnh t·ª´ embedding model: {vector_dimension}")
            except Exception as e_embed:
                logger.error(f"Kh√¥ng th·ªÉ t·ª± ƒë·ªông x√°c ƒë·ªãnh dimension: {e_embed}")
                raise ValueError("Kh√¥ng th·ªÉ x√°c ƒë·ªãnh dimension c·ªßa vector.") from e_embed
        else:
            logger.info(f"Dimension c·ªßa vector t·ª´ Pinecone: {vector_dimension}")

        logger.info(f"üöÄ ƒê√£ kh·ªüi t·∫°o Pinecone index v·ªõi vector dimension: {vector_dimension}")
        # ‚≠ê CH·ªà TR·∫¢ V·ªÄ pinecone_index V√Ä vector_dimension (KH√îNG TR·∫¢ V·ªÄ gemini_model)
        return pinecone_index, vector_dimension
        
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o d·ªãch v·ª•: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def clean_json_response(response_text: str) -> str:
    """‚≠ê C·∫¢I THI·ªÜN: L√†m s·∫°ch response t·ª´ Gemini tr∆∞·ªõc khi parse JSON v·ªõi nhi·ªÅu fix patterns h∆°n"""
    try:
        cleaned = response_text.strip()
        
        # Lo·∫°i b·ªè markdown wrapper
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
            
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
            
        cleaned = cleaned.strip()
        
        # ‚≠ê C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu pattern fix JSON ph·ªï bi·∫øn h∆°n
        
        # 1. Trailing comma tr∆∞·ªõc } ho·∫∑c ]
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
        
        # 2. Missing quotes cho keys (ch·ªâ fix n·∫øu ch∆∞a c√≥ quotes)
        cleaned = re.sub(r'(\w+):\s*(["\[])', r'"\1": \2', cleaned)
        
        # 3. Single quotes thay v√¨ double quotes cho keys v√† strings
        cleaned = re.sub(r"'([^']*?)'(\s*:\s*)", r'"\1"\2', cleaned)  # Keys
        cleaned = re.sub(r':\s*\'([^\']*?)\'', r': "\1"', cleaned)    # String values
        
        # 4. Th√™m d·∫•u ph·∫©y thi·∫øu gi·ªØa objects trong array
        cleaned = re.sub(r'}\s*{', r'}, {', cleaned)
        
        # 5. Lo·∫°i b·ªè control characters c√≥ th·ªÉ g√¢y l·ªói JSON
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', cleaned)
        
        # 6. Fix escaped quotes th·ª´a
        cleaned = re.sub(r'\\"', '"', cleaned)
        
        # 7. ƒê·∫£m b·∫£o JSON b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c ƒë√∫ng format
        if not cleaned.startswith('[') and not cleaned.startswith('{'):
            # T√¨m JSON array ho·∫∑c object ƒë·∫ßu ti√™n
            array_match = re.search(r'(\[[\s\S]*\])', cleaned)
            object_match = re.search(r'(\{[\s\S]*\})', cleaned)
            
            if array_match:
                cleaned = array_match.group(1)
            elif object_match:
                cleaned = object_match.group(1)
        
        return cleaned
    except Exception as e:
        logger.warning(f"L·ªói khi clean JSON response: {e}")
        return response_text.strip()

def parse_json_with_fallback(response_text: str) -> dict:
    """‚≠ê M·ªöI: Parse JSON v·ªõi multiple fallback strategies"""
    try:
        cleaned = clean_json_response(response_text)
        
        # Th·ª≠ parse tr·ª±c ti·∫øp
        try:
            result = json.loads(cleaned)
            return {"success": True, "data": result}
        except json.JSONDecodeError as e:
            logger.warning(f"Direct JSON parse failed: {e}")
        
        # ‚≠ê Fallback 1: T√¨m v√† extract JSON object/array t·ª´ text
        json_patterns = [
            r'\[[\s\S]*?\]',  # JSON array
            r'\{[\s\S]*?\}',  # JSON object
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, cleaned)
            for match in matches:
                try:
                    result = json.loads(match)
                    logger.info("‚úÖ JSON parsed successfully v·ªõi pattern matching")
                    return {"success": True, "data": result}
                except json.JSONDecodeError:
                    continue
        
        # ‚≠ê Fallback 2: N·∫øu l√† object nh∆∞ng mong ƒë·ª£i array, extract array t·ª´ object
        try:
            obj_result = json.loads(cleaned)
            if isinstance(obj_result, dict):
                # T√¨m key ch·ª©a array
                for key, value in obj_result.items():
                    if isinstance(value, list):
                        logger.info(f"‚úÖ Extracted array t·ª´ object key: {key}")
                        return {"success": True, "data": value}
        except json.JSONDecodeError:
            pass
        
        # ‚≠ê Fallback 3: Return structured error
        return {
            "success": False, 
            "error": "Invalid JSON from Gemini", 
            "raw_response": response_text[:500],  # Gi·ªõi h·∫°n ƒë·ªô d√†i
            "cleaned_response": cleaned[:500]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"JSON parsing exception: {str(e)}",
            "raw_response": response_text[:500]
        }

async def call_gemini_api_batch_classification_async(products_batch: list[dict], api_key_for_this_call: str, task_description: str, max_retries: int = 2) -> list[dict]:
    """
    ‚≠ê ASYNC VERSION v·ªõi API KEY ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ Worker Pool
    
    Args:
        products_batch: list of {"id": "product_id", "name": "product_name"}
        api_key_for_this_call: API key ƒë∆∞·ª£c truy·ªÅn t·ª´ worker
        task_description: M√¥ t·∫£ task ƒë·ªÉ logging
        max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
        
    Returns: 
        list of {"product_id": "...", "product_name": "..."} cho c√°c s·∫£n ph·∫©m l√† ƒë·ªì u·ªëng
        Ho·∫∑c [{"error_too_large": True}] n·∫øu l√¥ qu√° l·ªõn
    """
    if not products_batch:
        return []

    if not api_key_for_this_call:
        logger.error("‚ùå Kh√¥ng c√≥ API key Gemini ƒë∆∞·ª£c truy·ªÅn v√†o")
        return []

    # X√¢y d·ª±ng ph·∫ßn danh s√°ch s·∫£n ph·∫©m cho prompt
    product_list_str = ""
    for i, p_data in enumerate(products_batch):
        safe_id = json.dumps(p_data['id'], ensure_ascii=False)
        safe_name = json.dumps(p_data['name'], ensure_ascii=False)
        product_list_str += f'{i+1}. ID: {safe_id}, T√™n: {safe_name}\n'

    # ‚≠ê PROMPT T·ªêI ∆ØU H√ìA M·∫†NH CHO JSON - Y√äU C·∫¶U C·ª∞C K·ª≤ CH·∫∂T CH·∫º
    prompt = f"""NHI·ªÜM V·ª§: Ph√¢n lo·∫°i CH·ªà nh·ªØng s·∫£n ph·∫©m th·ª±c s·ª± l√† ƒê·ªí U·ªêNG ho·∫∑c NGUY√äN LI·ªÜU PHA CH·∫æ.

üö® QUY T·∫ÆC TUY·ªÜT ƒê·ªêI:
- CH·ªà tr·∫£ v·ªÅ JSON array h·ª£p l·ªá
- KH√îNG th√™m text, gi·∫£i th√≠ch, markdown
- KH√îNG s·ª≠ d·ª•ng d·∫•u ngo·∫∑c k√©p th√¥ng minh (" ")
- CH·ªà s·ª≠ d·ª•ng d·∫•u ngo·∫∑c k√©p ASCII chu·∫©n (")
- KH√îNG c√≥ trailing comma
- N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£: []

üìã ƒê·ªäNH D·∫†NG B·∫ÆT BU·ªòC:
[{{"product_id":"id","product_name":"t√™n"}}]

‚≠ê CH·ªà CH·ªåN:
- ƒê·ªì u·ªëng s·∫µn s√†ng: n∆∞·ªõc, tr√†, c√† ph√™, s·ªØa, n∆∞·ªõc √©p
- Nguy√™n li·ªáu pha ch·∫ø: b·ªôt c√† ph√™, tr√† l√°, siro, matcha

‚ùå TUY·ªÜT ƒê·ªêI KH√îNG CH·ªåN:
- Gia v·ªã n·∫•u ƒÉn: n∆∞·ªõc t∆∞∆°ng, d·∫•m, mu·ªëi
- Th·ª±c ph·∫©m kh√¥: b√°nh, k·∫πo, snack
- Nguy√™n li·ªáu n·∫•u ƒÉn: h√†nh, t·ªèi, gia v·ªã

‚úÖ V√ç D·ª§ CH√çNH X√ÅC:
[{{"product_id":"123","product_name":"Tr√† xanh"}},{{"product_id":"456","product_name":"C√† ph√™ ƒëen"}}]

‚ùå TUY·ªÜT ƒê·ªêI KH√îNG:
- Kh√¥ng markdown: ```json
- Kh√¥ng text th√™m: "D∆∞·ªõi ƒë√¢y l√†..."
- Kh√¥ng trailing comma: }},]
- Kh√¥ng smart quotes: "text"

DANH S√ÅCH S·∫¢N PH·∫®M:
{product_list_str}

TR·∫¢ V·ªÄ JSON ARRAY:"""

    # Log key rotation (an to√†n)
    masked_key = f"{api_key_for_this_call[:8]}..." if len(api_key_for_this_call) > 8 else "***"
    logger.info(f"üîç {task_description}: G·ªçi Gemini v·ªõi key {masked_key} - {len(products_batch)} s·∫£n ph·∫©m")

    for attempt in range(max_retries):
        try:
            # ‚≠ê S·ª¨ D·ª§NG MODEL POOL THAY V√å genai.configure() (thread-safe)
            temp_gemini_model = gemini_model_pool.get_model_for_key(api_key_for_this_call)
            if not temp_gemini_model:
                logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y model cho key {masked_key}")
                return []

            # ‚≠ê CH·∫†Y GENERATE_CONTENT TRONG EXECUTOR
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: temp_gemini_model.generate_content(
                    prompt,
                    generation_config={"max_output_tokens": MAX_GEMINI_OUTPUT_TOKENS, "temperature": 0.0}
                )
            )

            if response.parts:
                response_text = response.text

                # ‚≠ê S·ª¨ D·ª§NG PARSE_JSON_WITH_FALLBACK THAY V√å MANUAL PARSING
                parse_result = parse_json_with_fallback(response_text)
                
                if parse_result["success"]:
                    classified_drinks = parse_result["data"]
                    
                    # ‚≠ê X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P GEMINI TR·∫¢ V·ªÄ OBJECT THAY V√å LIST
                    if isinstance(classified_drinks, dict):
                        logger.warning(f"‚ö†Ô∏è {task_description}: Gemini tr·∫£ v·ªÅ object thay v√¨ array")
                        # Th·ª≠ extract array t·ª´ object
                        extracted_array = None
                        for key, value in classified_drinks.items():
                            if isinstance(value, list):
                                logger.info(f"‚úÖ {task_description}: Extracted array t·ª´ key '{key}'")
                                extracted_array = value
                                break
                        
                        if extracted_array:
                            classified_drinks = extracted_array
                        else:
                            logger.warning(f"‚ö†Ô∏è {task_description}: Kh√¥ng t√¨m th·∫•y array trong object, retry")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(5 + attempt * 5)
                                continue
                            return []
                    
                    if not isinstance(classified_drinks, list):
                        logger.warning(f"‚ö†Ô∏è {task_description}: Response v·∫´n kh√¥ng ph·∫£i list sau x·ª≠ l√Ω: {type(classified_drinks)}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(5 + attempt * 5)
                            continue
                        return []
                    
                    # X√°c th·ª±c th√™m: m·ªói item trong list ph·∫£i l√† dict c√≥ product_id v√† product_name
                    valid_drinks = []
                    original_ids = {p['id'] for p in products_batch}
                    seen_product_ids = set()
                    
                    for drink in classified_drinks:
                        if isinstance(drink, dict) and "product_id" in drink and "product_name" in drink:
                            product_id = drink["product_id"]
                            
                            if product_id in seen_product_ids:
                                logger.warning(f"Gemini tr·∫£ v·ªÅ product_id '{product_id}' b·ªã tr√πng l·∫∑p trong c√πng response. B·ªè qua.")
                                continue
                            
                            if product_id in original_ids:
                                valid_drinks.append(drink)
                                seen_product_ids.add(product_id)
                                
                                # Break s·ªõm n·∫øu ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒë·∫ßu v√†o
                                if len(valid_drinks) >= len(products_batch):
                                    logger.info(f"ƒê√£ x·ª≠ l√Ω ƒë·ªß {len(products_batch)} s·∫£n ph·∫©m, d·ª´ng ƒë·ªÉ tr√°nh hallucination")
                                    break
                            else:
                                logger.warning(f"Gemini tr·∫£ v·ªÅ product_id '{product_id}' kh√¥ng c√≥ trong l√¥ s·∫£n ph·∫©m g·ªëc. B·ªè qua.")
                        else:
                            logger.warning(f"M·ª•c kh√¥ng h·ª£p l·ªá trong danh s√°ch JSON t·ª´ Gemini: {drink}")
                    
                    if len(classified_drinks) > len(products_batch):
                        logger.warning(f"Gemini tr·∫£ v·ªÅ {len(classified_drinks)} items nh∆∞ng ch·ªâ c√≥ {len(products_batch)} s·∫£n ph·∫©m trong l√¥. "
                                       f"C√≥ th·ªÉ c√≥ hallucination.")
                    
                    # L·ªçc l·∫ßn cu·ªëi ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ duplicate
                    final_unique_drinks = []
                    final_seen_ids = set()
                    for drink_item in valid_drinks:
                        if drink_item['product_id'] not in final_seen_ids:
                            final_unique_drinks.append(drink_item)
                            final_seen_ids.add(drink_item['product_id'])
                    
                    logger.info(f"‚úÖ {task_description}: Th√†nh c√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c {len(final_unique_drinks)} ƒë·ªì u·ªëng t·ª´ {len(products_batch)} s·∫£n ph·∫©m")
                    return final_unique_drinks
                    
                else:
                    # Parse failed v·ªõi structured error
                    logger.error(f"üí• {task_description} JSON parse failed: {parse_result.get('error')}")
                    if attempt < max_retries - 1:
                        logger.info(f"üîÑ Retry {task_description} attempt {attempt + 1}/{max_retries} due to JSON parse error")
                        await asyncio.sleep(10 + attempt * 10)
                        continue
                    return []

        except Exception as e:
            error_str = str(e).lower()
            if "token" in error_str or "size limit" in error_str or "request payload" in error_str or "too large" in error_str:
                logger.error(f"üí• L·ªói k√≠ch th∆∞·ªõc prompt/request khi g·ªçi Gemini ({task_description}): {str(e)}. "
                             f"L√¥ hi·ªán t·∫°i c√≥ {len(products_batch)} s·∫£n ph·∫©m. H√£y th·ª≠ gi·∫£m PRODUCTS_PER_GEMINI_BATCH.")
                logger.warning(f"‚ö†Ô∏è C·∫ßn gi·∫£m PRODUCTS_PER_GEMINI_BATCH hi·ªán t·∫°i ({PRODUCTS_PER_GEMINI_BATCH}) ƒë·ªÉ tr√°nh l·ªói n√†y")
                return [{"error_too_large": True, "batch_size": len(products_batch), "suggestion": "Gi·∫£m PRODUCTS_PER_GEMINI_BATCH"}]

            if "429" in error_str or "resource_exhausted" in error_str or "too many requests" in error_str or "quota" in error_str:
                retry_delay = 15 + attempt * 10
                logger.warning(f"‚ö†Ô∏è L·ªói quota/rate limit Gemini ({task_description}), th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                await asyncio.sleep(retry_delay)
            elif "500" in error_str or "internal server error" in error_str or "service temporarily unavailable" in error_str:
                retry_delay = 20 + attempt * 10
                logger.warning(f"‚ö†Ô∏è L·ªói server Gemini (5xx) ({task_description}), th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"üí• L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API Gemini ({task_description}) (L·∫ßn {attempt+1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    break
                await asyncio.sleep(15 + attempt * 5)

    logger.error(f"‚ùå V·∫´n g·∫∑p l·ªói sau {max_retries} l·∫ßn th·ª≠ l·∫°i v·ªõi Gemini ({task_description}) ho·∫∑c kh√¥ng th·ªÉ parse k·∫øt qu·∫£.")
    return []

async def fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension: int) -> list[dict]:
    """
    ‚≠ê ASYNC VERSION: T√¨m ki·∫øm v√† ph√¢n lo·∫°i ƒë·ªì u·ªëng t·ª´ Pinecone v·ªõi x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô.
    
    Args:
        pinecone_index: Pinecone index instance
        vector_dimension: Dimension c·ªßa vector trong Pinecone
        
    Returns:
        list[dict]: Danh s√°ch c√°c ƒë·ªì u·ªëng ƒë√£ ph√¢n lo·∫°i
    """
    identified_drinks_overall = []
    query_vector = np.random.rand(vector_dimension).tolist()

    logger.info(f"üîç B·∫Øt ƒë·∫ßu qu√©t s·∫£n ph·∫©m t·ª´ Pinecone (t·ªëi ƒëa {PRODUCTS_TO_FETCH_FROM_PINECONE} s·∫£n ph·∫©m)...")
    
    try:
        # ‚≠ê CH·∫†Y PINECONE QUERY TRONG EXECUTOR
        loop = asyncio.get_event_loop()
        query_response = await loop.run_in_executor(
            None,
            lambda: pinecone_index.query(
                vector=query_vector,
                top_k=PRODUCTS_TO_FETCH_FROM_PINECONE,
                include_metadata=True
            )
        )
        
        products_from_pinecone = []
        if query_response.matches:
            for match in query_response.matches:
                prod_id = match.metadata.get("doc_id", match.id) if match.metadata else match.id
                prod_name = match.metadata.get("name") if match.metadata else None
                if prod_id and prod_name:
                    products_from_pinecone.append({"id": prod_id, "name": prod_name})
                else:
                    logger.warning(f"S·∫£n ph·∫©m t·ª´ Pinecone (ID vector: {match.id}) thi·∫øu 'name' ho·∫∑c 'doc_id'. B·ªè qua.")
        
        if not products_from_pinecone:
            logger.info("‚ùå Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c t√¨m th·∫•y t·ª´ Pinecone.")
            return []

        logger.info(f"üì¶ L·∫•y ƒë∆∞·ª£c {len(products_from_pinecone)} s·∫£n ph·∫©m t·ª´ Pinecone.")
        
        # ‚≠ê KH·ªûI T·∫†O WORKER POOL CONFIGURATION
        api_key_manager = get_api_key_manager()
        NUM_GEMINI_WORKERS = min(api_key_manager.total_keys(), MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS)
        if NUM_GEMINI_WORKERS == 0: 
            NUM_GEMINI_WORKERS = 1  # √çt nh·∫•t 1 worker n·∫øu c√≥ key
        
        logger.info(f"ü§ñ Kh·ªüi t·∫°o Worker Pool v·ªõi {NUM_GEMINI_WORKERS} workers (c√≥ {api_key_manager.total_keys()} API keys)")
        
        # T√≠nh to√°n s·ªë batches s·∫Ω ƒë∆∞·ª£c t·∫°o
        total_batches = (len(products_from_pinecone) + PRODUCTS_PER_GEMINI_BATCH - 1) // PRODUCTS_PER_GEMINI_BATCH
        logger.info(f"üìä S·∫Ω x·ª≠ l√Ω {total_batches} batches v·ªõi worker pool parallelism")
        
        # ‚≠ê KH·ªûI T·∫†O QUEUE V√Ä RESULT STORAGE
        work_queue = asyncio.Queue()
        results_list = []  # Thread-safe v·ªõi coroutine
        error_reports_list = []
        
        # ‚≠ê CHIA TH√ÄNH C√ÅC L√î V√Ä ƒê∆ØA V√ÄO QUEUE
        current_batch_size = PRODUCTS_PER_GEMINI_BATCH
        
        for i in range(0, len(products_from_pinecone), current_batch_size):
            batch_to_send = products_from_pinecone[i : i + current_batch_size]
            if batch_to_send:
                batch_num = i // current_batch_size + 1
                task_data = {
                    "products_batch": batch_to_send,
                    "batch_num": batch_num,
                    "batch_size": len(batch_to_send)
                }
                await work_queue.put(task_data)

        # ‚≠ê H√ÄM GEMINI WORKER
        async def gemini_worker(worker_id: int):
            logger.info(f"ü§ñ Worker {worker_id}: B·∫Øt ƒë·∫ßu ho·∫°t ƒë·ªông.")
            while True:
                try:
                    task_data = await work_queue.get()
                    if task_data is None:  # T√≠n hi·ªáu d·ª´ng
                        work_queue.task_done()
                        logger.info(f"ü§ñ Worker {worker_id}: Nh·∫≠n t√≠n hi·ªáu d·ª´ng.")
                        break

                    products_batch = task_data["products_batch"]
                    batch_num_log = task_data["batch_num"]
                    batch_size_log = task_data["batch_size"]
                    
                    task_description = f"Worker {worker_id} - Batch {batch_num_log}"
                    
                    # M·ªói worker t·ª± l·∫•y key m·ªõi cho m·ªói task n√≥ x·ª≠ l√Ω
                    api_key_for_call = api_key_manager.get_next_key()
                    if not api_key_for_call:
                        logger.error(f"ü§ñ Worker {worker_id}: Kh√¥ng c√≥ API key, b·ªè qua batch {batch_num_log}")
                        error_reports_list.append({
                            "error_batch": batch_num_log, "error_type": "no_api_key",
                            "message": "Kh√¥ng c√≥ API key kh·∫£ d·ª•ng", "batch_size": batch_size_log
                        })
                        work_queue.task_done()
                        continue

                    logger.info(f"ü§ñ Worker {worker_id}: ƒêang x·ª≠ l√Ω Batch {batch_num_log} ({batch_size_log} s·∫£n ph·∫©m) v·ªõi key ...{api_key_for_call[-4:]}")
                    
                    start_time = asyncio.get_event_loop().time()
                    
                    # G·ªçi API Gemini
                    classified_drinks = await call_gemini_api_batch_classification_async(
                        products_batch, 
                        api_key_for_call,
                        task_description
                    )
                    
                    end_time = asyncio.get_event_loop().time()
                    elapsed = end_time - start_time
                    
                    # ‚≠ê X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P PROMPT QU√Å L·ªöN V·ªöI LOGGING R√ï R√ÄNG H·ª¢N
                    if classified_drinks and isinstance(classified_drinks[0], dict) and classified_drinks[0].get("error_too_large"):
                        error_info = classified_drinks[0]
                        logger.error(f"üí• Batch {batch_num_log}: L√¥ qu√° l·ªõn cho Gemini API")
                        logger.error(f"   - Batch size hi·ªán t·∫°i: {batch_size_log} s·∫£n ph·∫©m")
                        logger.error(f"   - PRODUCTS_PER_GEMINI_BATCH hi·ªán t·∫°i: {PRODUCTS_PER_GEMINI_BATCH}")
                        logger.warning(f"‚ö†Ô∏è C·∫ßn ƒëi·ªÅu ch·ªânh c·∫•u h√¨nh ƒë·ªÉ tr√°nh l·ªói n√†y ti·∫øp t·ª•c x·∫£y ra")
                        error_reports_list.append({
                            "error_batch": batch_num_log, "error_type": "batch_too_large",
                            "message": "Batch qu√° l·ªõn cho Gemini", "batch_size": batch_size_log
                        })
                    elif classified_drinks:
                        results_list.extend(classified_drinks)
                        logger.info(f"‚úÖ Worker {worker_id}: Batch {batch_num_log} x√°c ƒë·ªãnh ƒë∆∞·ª£c {len(classified_drinks)} ƒë·ªì u·ªëng trong {elapsed:.2f}s")
                    else:
                        logger.info(f"‚ö™ Worker {worker_id}: Batch {batch_num_log} kh√¥ng c√≥ ƒë·ªì u·ªëng ho·∫∑c c√≥ l·ªói ({elapsed:.2f}s)")
                        error_reports_list.append({
                            "error_batch": batch_num_log, "error_type": "empty_result",
                            "message": "Kh√¥ng c√≥ k·∫øt qu·∫£", "batch_size": batch_size_log
                        })
                    
                    work_queue.task_done()
                    # Th·ªùi gian ngh·ªâ nh·ªè sau m·ªói batch c·ªßa m·ªôt worker
                    await asyncio.sleep(WORKER_SLEEP_BETWEEN_TASKS)

                except asyncio.CancelledError:
                    logger.info(f"ü§ñ Worker {worker_id}: B·ªã cancel.")
                    break
                except Exception as e:
                    logger.error(f"ü§ñ Worker {worker_id}: L·ªói kh√¥ng mong mu·ªën: {e}", exc_info=True)
                    if 'task_data' in locals() and task_data and 'batch_num' in task_data:
                         error_reports_list.append({"error_batch": task_data['batch_num'], "error_type": "worker_exception", "message": str(e), "batch_size": task_data.get('batch_size',0)})
                    if 'task_data' in locals() and task_data is not None:
                         work_queue.task_done()
                    await asyncio.sleep(WORKER_ERROR_SLEEP)

        # ‚≠ê KH·ªûI T·∫†O V√Ä CH·∫†Y C√ÅC WORKER
        worker_tasks = []
        for i in range(NUM_GEMINI_WORKERS):
            worker_tasks.append(asyncio.create_task(gemini_worker(i + 1)))

        # Ch·ªù t·∫•t c·∫£ c√°c item trong queue ƒë∆∞·ª£c x·ª≠ l√Ω
        await work_queue.join()

        # G·ª≠i t√≠n hi·ªáu d·ª´ng cho t·∫•t c·∫£ worker
        for _ in range(NUM_GEMINI_WORKERS):
            await work_queue.put(None)

        # Ch·ªù t·∫•t c·∫£ worker ho√†n th√†nh
        await asyncio.gather(*worker_tasks, return_exceptions=True)

        logger.info(f"üèÅ T·∫•t c·∫£ c√°c worker ƒë√£ ho√†n th√†nh. T·ªïng h·ª£p k·∫øt qu·∫£...")
        
        # ‚≠ê L·ªåC TR√ôNG L·∫∂P B·∫∞NG T√äN CHU·∫®N H√ìA
        def normalize_product_name(name: str) -> str:
            """Chu·∫©n h√≥a t√™n s·∫£n ph·∫©m ƒë·ªÉ so s√°nh tr√πng l·∫∑p"""
            if not name:
                return ""
            # Chuy·ªÉn v·ªÅ lowercase, lo·∫°i b·ªè d·∫•u c√°ch, d·∫•u g·∫°ch ngang, k√Ω t·ª± ƒë·∫∑c bi·ªát
            import unicodedata
            normalized = unicodedata.normalize('NFD', str(name).lower())
            normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')  # Lo·∫°i b·ªè d·∫•u
            normalized = re.sub(r'[^a-z0-9]', '', normalized)  # Ch·ªâ gi·ªØ ch·ªØ v√† s·ªë
            return normalized

        if results_list:
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu l·ªçc tr√πng l·∫∑p t·ª´ {len(results_list)} beverages...")
            
            final_unique_beverages = []
            seen_normalized_names = set()
            seen_product_ids = set()
            
            for beverage_item in results_list:
                if not isinstance(beverage_item, dict):
                    continue
                    
                product_id = beverage_item.get("product_id")
                product_name = beverage_item.get("product_name")
                
                if not product_id or not product_name:
                    continue
                
                # L·ªçc theo ID tr∆∞·ªõc (∆∞u ti√™n cao nh·∫•t)
                if product_id in seen_product_ids:
                    logger.debug(f"ƒê√£ l·ªçc beverage tr√πng ID: {product_id}")
                    continue
                
                # L·ªçc theo t√™n chu·∫©n h√≥a
                normalized_name = normalize_product_name(product_name)
                if normalized_name and normalized_name not in seen_normalized_names:
                    final_unique_beverages.append(beverage_item)
                    seen_normalized_names.add(normalized_name)
                    seen_product_ids.add(product_id)
                else:
                    logger.debug(f"ƒê√£ l·ªçc beverage tr√πng t√™n: {product_name}")
            
            results_list = final_unique_beverages
            logger.info(f"‚úÖ Sau khi l·ªçc tr√πng l·∫∑p: {len(results_list)} beverages duy nh·∫•t")

        # ‚≠ê X·ª¨ L√ù K·∫æT QU·∫¢ CU·ªêI C√ôNG
        identified_drinks_overall.extend(results_list)
        
        if error_reports_list:
            logger.warning(f"‚ö†Ô∏è C√≥ {len(error_reports_list)} batch l·ªói nh∆∞ng v·∫´n t√¨m ƒë∆∞·ª£c {len(results_list)} ƒë·ªì u·ªëng")
            # Log m·ªôt s·ªë l·ªói ƒë·∫ßu
            for error in error_reports_list[:3]:
                logger.warning(f"   - Batch {error['error_batch']}: {error['message']}")
        
        logger.info(f"üìä K·∫øt qu·∫£: {len(results_list)} ƒë·ªì u·ªëng ƒë∆∞·ª£c t√¨m th·∫•y, {len(error_reports_list)} batch l·ªói")
        
    except Exception as e:
        logger.error(f"üí• L·ªói khi query Pinecone ho·∫∑c x·ª≠ l√Ω l√¥ s·∫£n ph·∫©m: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

    return identified_drinks_overall

# ‚≠ê WRAPPER ƒê·ªíNG B·ªò CHO BACKWARD COMPATIBILITY
def fetch_and_filter_drinks_in_batches_sync_wrapper() -> list[dict]:
    """
    ‚≠ê WRAPPER ƒê·ªíNG B·ªò: ƒê·ªÉ chat_flow.py c√≥ th·ªÉ g·ªçi t·ª´ synchronous context.
    """
    try:
        # Kh·ªüi t·∫°o c√°c d·ªãch v·ª• c·∫ßn thi·∫øt
        pinecone_index, vector_dimension = init_services()
        
        # Ki·ªÉm tra xem c√≥ event loop ƒëang ch·∫°y kh√¥ng
        try:
            loop = asyncio.get_running_loop()
            # N·∫øu c√≥ event loop ƒëang ch·∫°y, t·∫°o task m·ªõi
            logger.info("üí° Ph√°t hi·ªán event loop ƒëang ch·∫°y, s·ª≠ d·ª•ng create_task")
            task = loop.create_task(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))
            # Wait cho task ho√†n th√†nh (blocking)
            import concurrent.futures
            import threading
            
            def run_in_new_loop():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension)
                    )
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_new_loop)
                return future.result()
                
        except RuntimeError:
            # Kh√¥ng c√≥ event loop ƒëang ch·∫°y, c√≥ th·ªÉ d√πng asyncio.run
            logger.info("üí° Kh√¥ng c√≥ event loop ƒëang ch·∫°y, s·ª≠ d·ª•ng asyncio.run")
            return asyncio.run(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))
            
    except Exception as e:
        logger.error(f"üí• L·ªói trong wrapper ƒë·ªìng b·ªô c·ªßa fetch_and_filter_drinks: {e}", exc_info=True)
        return []

# ‚≠ê GI·ªÆ L·∫†I H√ÄM C≈® ƒê·ªÇ T∆Ø∆†NG TH√çCH NG∆Ø·ª¢C (Deprecated)
def fetch_and_filter_drinks_in_batches(pinecone_index, gemini_model, vector_dimension: int) -> list[dict]:
    """H√†m c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c - DEPRECATED: s·ª≠ d·ª•ng async version"""
    logger.warning("‚ö†Ô∏è S·ª≠ d·ª•ng h√†m fetch_and_filter_drinks_in_batches C≈® - khuy·∫øn ngh·ªã s·ª≠ d·ª•ng async version")
    return asyncio.run(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))

if __name__ == "__main__":
    async def main_beverage_test_async():
        """‚≠ê H√ÄM TEST ASYNC CH√çNH"""
        try:
            print("üîß ƒêang kh·ªüi t·∫°o c√°c d·ªãch v·ª•...")
            pinecone_index, vector_dim = init_services()
            
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh t√¨m ki·∫øm ƒë·ªì u·ªëng t·ª´ Pinecone index '{PINECONE_INDEX_NAME}' (ASYNC VERSION)...")
            print(f"üìä C·∫•u h√¨nh:")
            print(f"   - S·∫£n ph·∫©m t·ªëi ƒëa t·ª´ Pinecone: {PRODUCTS_TO_FETCH_FROM_PINECONE}")
            print(f"   - K√≠ch th∆∞·ªõc m·ªói batch: {PRODUCTS_PER_GEMINI_BATCH}")
            print(f"   - ƒê·ªìng th·ªùi t·ªëi ƒëa: {MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS} batch")
            print(f"   - Vector dimension: {vector_dim}")
            print(f"   - Gemini model: {GEMINI_MODEL_NAME}")
            
            # L·∫•y th·ªëng k√™ API key
            api_key_manager = get_api_key_manager()
            print(f"   - API keys available: {api_key_manager.total_keys()}")
            
            import time
            start_time = time.time()
            all_drinks_found = await fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dim)
            end_time = time.time()
            
            print("\nüìã --- DANH S√ÅCH ƒê·ªí U·ªêNG ƒê∆Ø·ª¢C T√åM TH·∫§Y ---")
            if all_drinks_found:
                # Lo·∫°i b·ªè tr√πng l·∫∑p n·∫øu c√≥
                unique_drinks = []
                seen_ids = set()
                for drink in all_drinks_found:
                    if drink['product_id'] not in seen_ids:
                        unique_drinks.append(drink)
                        seen_ids.add(drink['product_id'])
                
                print(json.dumps(unique_drinks[:10], indent=2, ensure_ascii=False))  # Ch·ªâ hi·ªÉn th·ªã 10 ƒë·∫ßu ti√™n
                if len(unique_drinks) > 10:
                    print(f"... v√† {len(unique_drinks) - 10} ƒë·ªì u·ªëng kh√°c")
                
                print(f"\n‚úÖ T·ªïng c·ªông t√¨m th·∫•y {len(unique_drinks)} s·∫£n ph·∫©m ƒë·ªì u·ªëng duy nh·∫•t.")
                
                # Th·ªëng k√™ hi·ªáu su·∫•t
                total_processing_time = end_time - start_time
                products_per_second = len(unique_drinks) / total_processing_time if total_processing_time > 0 else 0
                print(f"‚ö° Hi·ªáu su·∫•t: {products_per_second:.2f} ƒë·ªì u·ªëng/gi√¢y")
                
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ƒë·ªì u·ªëng n√†o ho·∫∑c kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c ph√¢n lo·∫°i l√† ƒë·ªì u·ªëng.")
                
            print(f"‚è±Ô∏è T·ªïng th·ªùi gian th·ª±c hi·ªán: {end_time - start_time:.2f} gi√¢y.")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ s·ª≠ d·ª•ng API key
            usage_stats = api_key_manager.get_usage_statistics()
            print(f"\nüìä Th·ªëng k√™ s·ª≠ d·ª•ng API key:")
            for key_masked, usage_count in usage_stats.items():
                print(f"   - Key {key_masked}: {usage_count} l·∫ßn s·ª≠ d·ª•ng")

        except ValueError as ve: 
            print(f"‚ùå L·ªói c·∫•u h√¨nh ho·∫∑c gi√° tr·ªã: {ve}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën ·ªü main: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

    # ‚≠ê S·ª¨ D·ª§NG ASYNCIO.RUN CHO MAIN
    asyncio.run(main_beverage_test_async())