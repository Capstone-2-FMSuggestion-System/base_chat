import os
import logging
import asyncio
import json
import re
from pinecone import Pinecone
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np

# ‚≠ê Import ApiKeyManager CH√çNH X√ÅC
from app.services.api_key_manager import get_api_key_manager

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- C·∫•u h√¨nh ---
PINECONE_API_KEY = os.getenv("PRODUCT_DB_PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "product-index")
EMBEDDING_MODEL_FOR_DIM_ONLY = "sentence-transformers/all-mpnet-base-v2"
PRODUCTS_TO_FETCH_FROM_PINECONE = 1610
PRODUCTS_PER_GEMINI_BATCH = 100
MAX_GEMINI_OUTPUT_TOKENS = 8192

# ‚≠ê C·∫§U H√åNH ASYNC V√Ä CONCURRENCY - ƒê√∫ng t√™n theo y√™u c·∫ßu
MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS = 3

# ‚≠ê MODEL NAME CONSTANT
GEMINI_MODEL_NAME = "gemini-2.0-flash-lite"

# Ki·ªÉm tra API keys c∆° b·∫£n
if not PINECONE_API_KEY or not PINECONE_INDEX_NAME:
    missing_keys = [
        key for key, value in {
            "PINECONE_API_KEY": PINECONE_API_KEY,
            "PINECONE_INDEX_NAME": PINECONE_INDEX_NAME
        }.items() if not value
    ]
    logger.error(f"C√°c bi·∫øn m√¥i tr∆∞·ªùng sau kh√¥ng ƒë∆∞·ª£c thi·∫øt l·∫≠p: {', '.join(missing_keys)}")
    raise ValueError(f"C√°c bi·∫øn m√¥i tr∆∞·ªùng sau kh√¥ng ƒë∆∞·ª£c thi·∫øt l·∫≠p: {', '.join(missing_keys)}")

def clean_json_response(response_text: str) -> str:
    """‚≠ê C·∫¢I THI·ªÜN: L√†m s·∫°ch response t·ª´ Gemini tr∆∞·ªõc khi parse JSON v·ªõi nhi·ªÅu fix patterns h∆°n"""
    try:
        cleaned = response_text.strip()
        
        # Lo·∫°i b·ªè markdown wrapper
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
            
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
            
        cleaned = cleaned.strip()
        
        # ‚≠ê C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu pattern fix JSON ph·ªï bi·∫øn h∆°n
        
        # 1. Trailing comma tr∆∞·ªõc } ho·∫∑c ]
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
        
        # 2. Missing quotes cho keys (ch·ªâ fix n·∫øu ch∆∞a c√≥ quotes)
        cleaned = re.sub(r'(\w+):\s*(["\[])', r'"\1": \2', cleaned)
        
        # 3. Single quotes thay v√¨ double quotes cho keys v√† strings
        cleaned = re.sub(r"'([^']*?)'(\s*:\s*)", r'"\1"\2', cleaned)  # Keys
        cleaned = re.sub(r':\s*\'([^\']*?)\'', r': "\1"', cleaned)    # String values
        
        # 4. Th√™m d·∫•u ph·∫©y thi·∫øu gi·ªØa objects trong array
        cleaned = re.sub(r'}\s*{', r'}, {', cleaned)
        
        # 5. Lo·∫°i b·ªè control characters c√≥ th·ªÉ g√¢y l·ªói JSON
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', cleaned)
        
        # 6. Fix escaped quotes th·ª´a
        cleaned = re.sub(r'\\"', '"', cleaned)
        
        # 7. ƒê·∫£m b·∫£o JSON b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c ƒë√∫ng format
        if not cleaned.startswith('[') and not cleaned.startswith('{'):
            # T√¨m JSON array ho·∫∑c object ƒë·∫ßu ti√™n
            array_match = re.search(r'(\[[\s\S]*\])', cleaned)
            object_match = re.search(r'(\{[\s\S]*\})', cleaned)
            
            if array_match:
                cleaned = array_match.group(1)
            elif object_match:
                cleaned = object_match.group(1)
        
        return cleaned
    except Exception as e:
        logger.warning(f"L·ªói khi clean JSON response: {e}")
        return response_text.strip()

def parse_json_with_fallback(response_text: str) -> dict:
    """‚≠ê M·ªöI: Parse JSON v·ªõi multiple fallback strategies"""
    try:
        cleaned = clean_json_response(response_text)
        
        # Th·ª≠ parse tr·ª±c ti·∫øp
        try:
            result = json.loads(cleaned)
            return {"success": True, "data": result}
        except json.JSONDecodeError as e:
            logger.warning(f"Direct JSON parse failed: {e}")
        
        # ‚≠ê Fallback 1: T√¨m v√† extract JSON object/array t·ª´ text
        json_patterns = [
            r'\[[\s\S]*?\]',  # JSON array
            r'\{[\s\S]*?\}',  # JSON object
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, cleaned)
            for match in matches:
                try:
                    result = json.loads(match)
                    logger.info("‚úÖ JSON parsed successfully v·ªõi pattern matching")
                    return {"success": True, "data": result}
                except json.JSONDecodeError:
                    continue
        
        # ‚≠ê Fallback 2: N·∫øu l√† object nh∆∞ng mong ƒë·ª£i array, extract array t·ª´ object
        try:
            obj_result = json.loads(cleaned)
            if isinstance(obj_result, dict):
                # T√¨m key ch·ª©a array
                for key, value in obj_result.items():
                    if isinstance(value, list):
                        logger.info(f"‚úÖ Extracted array t·ª´ object key: {key}")
                        return {"success": True, "data": value}
        except json.JSONDecodeError:
            pass
        
        # ‚≠ê Fallback 3: Return structured error
        return {
            "success": False, 
            "error": "Invalid JSON from Gemini", 
            "raw_response": response_text[:500],  # Gi·ªõi h·∫°n ƒë·ªô d√†i
            "cleaned_response": cleaned[:500]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"JSON parsing exception: {str(e)}",
            "raw_response": response_text[:500]
        }

def init_services():
    """‚≠ê Kh·ªüi t·∫°o k·∫øt n·ªëi Pinecone v√† l·∫•y dimension c·ªßa vector. KH√îNG tr·∫£ v·ªÅ gemini_model."""
    try:
        # ‚≠ê KI·ªÇM TRA API KEY MANAGER
        api_key_manager = get_api_key_manager()
        if not api_key_manager.is_healthy():
            logger.error("ApiKeyManager kh√¥ng kh·ªèe m·∫°nh ho·∫∑c kh√¥ng c√≥ API key")
            raise ValueError("ApiKeyManager kh√¥ng kh·ªèe m·∫°nh")
        logger.info(f"‚úÖ ApiKeyManager ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n kh·ªèe m·∫°nh v·ªõi {api_key_manager.total_keys()} keys")
        
        pc = Pinecone(api_key=PINECONE_API_KEY)
        
        # C·∫£i thi·ªán logic ki·ªÉm tra index
        try:
            index_list = pc.list_indexes()
            index_names = []
            
            if hasattr(index_list, 'indexes') and isinstance(index_list.indexes, list):
                index_names = [idx.get('name') for idx in index_list.indexes if isinstance(idx, dict) and idx.get('name')]
            elif hasattr(index_list, 'names'):
                if callable(index_list.names):
                    index_names = index_list.names()
                else:
                    index_names = index_list.names
            
            if PINECONE_INDEX_NAME not in index_names:
                logger.error(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i trong danh s√°ch: {index_names}")
                raise ValueError(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i.")
                
        except Exception as e_list:
            logger.warning(f"Kh√¥ng th·ªÉ ki·ªÉm tra danh s√°ch index: {e_list}. Th·ª≠ describe_index tr·ª±c ti·∫øp...")

        index_description = pc.describe_index(PINECONE_INDEX_NAME)
        logger.info(f"Pinecone index '{PINECONE_INDEX_NAME}' ƒë√£ t·ªìn t·∫°i. Th√¥ng tin: {index_description}")
        pinecone_index = pc.Index(PINECONE_INDEX_NAME)
        vector_dimension = index_description.dimension
        
        if not vector_dimension:
            logger.info("Kh√¥ng l·∫•y ƒë∆∞·ª£c dimension t·ª´ describe_index, th·ª≠ t·∫£i embedding model...")
            try:
                temp_embeddings_model = HuggingFaceEmbeddings(
                    model_name=EMBEDDING_MODEL_FOR_DIM_ONLY, model_kwargs={'device': 'cpu'}
                )
                sample_embedding = temp_embeddings_model.embed_query("sample text")
                vector_dimension = len(sample_embedding)
                logger.info(f"Dimension c·ªßa vector x√°c ƒë·ªãnh t·ª´ embedding model: {vector_dimension}")
            except Exception as e_embed:
                logger.error(f"Kh√¥ng th·ªÉ t·ª± ƒë·ªông x√°c ƒë·ªãnh dimension: {e_embed}")
                raise ValueError("Kh√¥ng th·ªÉ x√°c ƒë·ªãnh dimension c·ªßa vector.") from e_embed
        else:
            logger.info(f"Dimension c·ªßa vector t·ª´ Pinecone: {vector_dimension}")

        logger.info(f"üöÄ ƒê√£ kh·ªüi t·∫°o Pinecone index v·ªõi vector dimension: {vector_dimension}")
        # ‚≠ê CH·ªà TR·∫¢ V·ªÄ pinecone_index V√Ä vector_dimension (KH√îNG TR·∫¢ V·ªÄ gemini_model)
        return pinecone_index, vector_dimension
        
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o d·ªãch v·ª•: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

async def call_gemini_api_batch_classification_async(products_batch: list[dict], max_retries: int = 2) -> list[dict]:
    """
    ‚≠ê ASYNC VERSION: G·ª≠i m·ªôt l√¥ s·∫£n ph·∫©m ƒë·∫øn Gemini ƒë·ªÉ ph√¢n lo·∫°i v√† tr·∫£ v·ªÅ danh s√°ch ƒë·ªì u·ªëng.
    
    Args:
        products_batch: list of {"id": "product_id", "name": "product_name"}
        max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
        
    Returns: 
        list of {"product_id": "...", "product_name": "..."} cho c√°c s·∫£n ph·∫©m l√† ƒë·ªì u·ªëng
        Ho·∫∑c [{"error_too_large": True}] n·∫øu l√¥ qu√° l·ªõn
    """
    if not products_batch:
        return []

    # ‚≠ê L·∫§Y API KEY MANAGER
    api_key_manager = get_api_key_manager()
    if not api_key_manager.is_healthy():
        logger.error("ApiKeyManager kh√¥ng kh·ªèe m·∫°nh, kh√¥ng th·ªÉ g·ªçi Gemini API")
        return []

    # X√¢y d·ª±ng ph·∫ßn danh s√°ch s·∫£n ph·∫©m cho prompt
    product_list_str = ""
    for i, p_data in enumerate(products_batch):
        safe_id = json.dumps(p_data['id'], ensure_ascii=False)
        safe_name = json.dumps(p_data['name'], ensure_ascii=False)
        product_list_str += f'{i+1}. ID: {safe_id}, T√™n: {safe_name}\n'

    # ‚≠ê PROMPT T·ªêI ∆ØU H√ìA CHO JSON - C·∫¢I THI·ªÜN THEO Y√äU C·∫¶U
    prompt = f"""B·∫°n ƒë∆∞·ª£c cung c·∫•p m·ªôt danh s√°ch c√°c s·∫£n ph·∫©m. H√£y ph√¢n lo·∫°i CH·ªà nh·ªØng s·∫£n ph·∫©m th·ª±c s·ª± l√† ƒê·ªí U·ªêNG ho·∫∑c NGUY√äN LI·ªÜU CH√çNH ƒë·ªÉ pha ch·∫ø ƒë·ªì u·ªëng.

‚ö†Ô∏è TUY·ªÜT ƒê·ªêI CH·ªà TR·∫¢ V·ªÄ M·ªòT DANH S√ÅCH JSON (JSON ARRAY) H·ª¢P L·ªÜ. KH√îNG TH√äM b·∫•t k·ª≥ vƒÉn b·∫£n n√†o tr∆∞·ªõc ho·∫∑c sau danh s√°ch JSON.

üìã TI√äU CH√ç PH√ÇN LO·∫†I:
‚≠ê CH·ªà CH·ªåN C√ÅC S·∫¢N PH·∫®M SAU:
- ƒê·ªí U·ªêNG S·∫¥N S√ÄNG: n∆∞·ªõc gi·∫£i kh√°t, tr√†, c√† ph√™ pha s·∫µn, s·ªØa, n∆∞·ªõc √©p, sinh t·ªë, bia, r∆∞·ª£u vang, n∆∞·ªõc l·ªçc
- NGUY√äN LI·ªÜU PHA CH·∫æ CH√çNH: b·ªôt c√† ph√™, tr√† t√∫i l·ªçc, tr√† l√°, siro pha ch·∫ø, s·ªØa ƒë·∫∑c, b·ªôt cacao, matcha, coffee bean

‚≠ê TUY·ªÜT ƒê·ªêI KH√îNG CH·ªåN:
- Gia v·ªã n·∫•u ƒÉn: n∆∞·ªõc m√†u d·ª´a, n∆∞·ªõc t∆∞∆°ng, d·∫•m, mu·ªëi
- Th·ª±c ph·∫©m kh√¥: b√°nh k·∫πo, snack, m√¨ t√¥m
- ƒê∆∞·ªùng ph√®n (tr·ª´ khi c√≥ b·ªëi c·∫£nh "Tr√† ƒë∆∞·ªùng ph√®n" ho·∫∑c ƒë·ªì u·ªëng c·ª• th·ªÉ)
- Nguy√™n li·ªáu n·∫•u ƒÉn kh√°c: h√†nh t√¢y, t·ªèi, gia v·ªã

üìã Y√äU C·∫¶U JSON FORMAT:
1. ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c chu·ªói trong JSON ƒë∆∞·ª£c ƒë·∫∑t trong d·∫•u ngo·∫∑c k√©p chu·∫©n (\")
2. ƒê·∫£m b·∫£o kh√¥ng c√≥ d·∫•u ph·∫©y th·ª´a ·ªü cu·ªëi danh s√°ch ho·∫∑c cu·ªëi ƒë·ªëi t∆∞·ª£ng
3. M·ªói product_id trong danh s√°ch JSON tr·∫£ v·ªÅ ph·∫£i l√† DUY NH·∫§T - KH√îNG l·∫∑p l·∫°i s·∫£n ph·∫©m
4. M·ªói object ph·∫£i c√≥ ƒë√∫ng 2 tr∆∞·ªùng: "product_id" v√† "product_name"
5. ID v√† name ph·∫£i kh·ªõp ch√≠nh x√°c v·ªõi input
6. N·∫øu kh√¥ng c√≥ ƒë·ªì u·ªëng n√†o, tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON r·ªóng: []

‚úÖ V√ç D·ª§ V·ªÄ OUTPUT JSON H·ª¢P L·ªÜ (n·∫øu s·∫£n ph·∫©m 1 v√† 3 l√† ƒë·ªì u·ªëng):
[
  {{
    "product_id": "ID_SAN_PHAM_1",
    "product_name": "TEN_SAN_PHAM_1"
  }},
  {{
    "product_id": "ID_SAN_PHAM_3", 
    "product_name": "TEN_SAN_PHAM_3"
  }}
]

DANH S√ÅCH S·∫¢N PH·∫®M:
{product_list_str}

üî• CH·ªà TR·∫¢ V·ªÄ JSON ARRAY - KH√îNG GI·∫¢I TH√çCH, KH√îNG MARKDOWN, KH√îNG VƒÇN B·∫¢N TH√äM:"""
    task_desc = f"Ph√¢n lo·∫°i l√¥ {len(products_batch)} s·∫£n ph·∫©m (async)"

    for attempt in range(max_retries):
        try:
            # ‚≠ê L·∫§Y API KEY T·ª™ API_KEY_MANAGER
            api_key = api_key_manager.get_next_key()
            if not api_key:
                logger.error("Kh√¥ng th·ªÉ l·∫•y API key t·ª´ api_key_manager")
                return []

            # ‚≠ê LOG API KEY USAGE (AN TO√ÄN)
            key_info = f"...{api_key[-6:]}" if len(api_key) > 6 else "short_key"
            logger.debug(f"üîë ƒêang s·ª≠ d·ª•ng API key k·∫øt th√∫c b·∫±ng {key_info} cho {task_desc}")

            # ‚≠ê C·∫§U H√åNH GEMINI V·ªöI API KEY M·ªöI CHO M·ªñI CALL
            genai.configure(api_key=api_key)
            temp_gemini_model = genai.GenerativeModel(GEMINI_MODEL_NAME)

            # ‚≠ê CH·∫†Y GENERATE_CONTENT TRONG EXECUTOR
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: temp_gemini_model.generate_content(
                    prompt,
                    generation_config={"max_output_tokens": MAX_GEMINI_OUTPUT_TOKENS, "temperature": 0.0}
                )
            )

            if response.parts:
                response_text = response.text

                # ‚≠ê S·ª¨ D·ª§NG PARSE_JSON_WITH_FALLBACK THAY V√å MANUAL PARSING
                parse_result = parse_json_with_fallback(response_text)
                
                if parse_result["success"]:
                    classified_drinks = parse_result["data"]
                    
                    # ‚≠ê X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P GEMINI TR·∫¢ V·ªÄ OBJECT THAY V√å LIST
                    if isinstance(classified_drinks, dict):
                        logger.warning(f"‚ö†Ô∏è {task_desc}: Gemini tr·∫£ v·ªÅ object thay v√¨ array")
                        # Th·ª≠ extract array t·ª´ object
                        extracted_array = None
                        for key, value in classified_drinks.items():
                            if isinstance(value, list):
                                logger.info(f"‚úÖ {task_desc}: Extracted array t·ª´ key '{key}'")
                                extracted_array = value
                                break
                        
                        if extracted_array:
                            classified_drinks = extracted_array
                        else:
                            logger.warning(f"‚ö†Ô∏è {task_desc}: Kh√¥ng t√¨m th·∫•y array trong object, retry")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(5 + attempt * 5)
                                continue
                            return []
                    
                    if not isinstance(classified_drinks, list):
                        logger.warning(f"‚ö†Ô∏è {task_desc}: Response v·∫´n kh√¥ng ph·∫£i list sau x·ª≠ l√Ω: {type(classified_drinks)}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(5 + attempt * 5)
                            continue
                        return []
                    
                    # X√°c th·ª±c th√™m: m·ªói item trong list ph·∫£i l√† dict c√≥ product_id v√† product_name
                    valid_drinks = []
                    original_ids = {p['id'] for p in products_batch}
                    seen_product_ids = set()
                    
                    for drink in classified_drinks:
                        if isinstance(drink, dict) and "product_id" in drink and "product_name" in drink:
                            product_id = drink["product_id"]
                            
                            if product_id in seen_product_ids:
                                logger.warning(f"Gemini tr·∫£ v·ªÅ product_id '{product_id}' b·ªã tr√πng l·∫∑p trong c√πng response. B·ªè qua.")
                                continue
                            
                            if product_id in original_ids:
                                valid_drinks.append(drink)
                                seen_product_ids.add(product_id)
                                
                                # Break s·ªõm n·∫øu ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒë·∫ßu v√†o
                                if len(valid_drinks) >= len(products_batch):
                                    logger.info(f"ƒê√£ x·ª≠ l√Ω ƒë·ªß {len(products_batch)} s·∫£n ph·∫©m, d·ª´ng ƒë·ªÉ tr√°nh hallucination")
                                    break
                            else:
                                logger.warning(f"Gemini tr·∫£ v·ªÅ product_id '{product_id}' kh√¥ng c√≥ trong l√¥ s·∫£n ph·∫©m g·ªëc. B·ªè qua.")
                        else:
                            logger.warning(f"M·ª•c kh√¥ng h·ª£p l·ªá trong danh s√°ch JSON t·ª´ Gemini: {drink}")
                    
                    if len(classified_drinks) > len(products_batch):
                        logger.warning(f"Gemini tr·∫£ v·ªÅ {len(classified_drinks)} items nh∆∞ng ch·ªâ c√≥ {len(products_batch)} s·∫£n ph·∫©m trong l√¥. "
                                       f"C√≥ th·ªÉ c√≥ hallucination.")
                    
                    # L·ªçc l·∫ßn cu·ªëi ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ duplicate
                    final_unique_drinks = []
                    final_seen_ids = set()
                    for drink_item in valid_drinks:
                        if drink_item['product_id'] not in final_seen_ids:
                            final_unique_drinks.append(drink_item)
                            final_seen_ids.add(drink_item['product_id'])
                    
                    logger.info(f"‚úÖ {task_desc}: Th√†nh c√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c {len(final_unique_drinks)} ƒë·ªì u·ªëng t·ª´ {len(products_batch)} s·∫£n ph·∫©m")
                    return final_unique_drinks
                    
                else:
                    # Parse failed v·ªõi structured error
                    logger.error(f"üí• {task_desc} JSON parse failed: {parse_result.get('error')}")
                    if attempt < max_retries - 1:
                        logger.info(f"üîÑ Retry {task_desc} attempt {attempt + 1}/{max_retries} due to JSON parse error")
                        await asyncio.sleep(10 + attempt * 10)
                        continue
                    return []

        except Exception as e:
            error_str = str(e).lower()
            if "token" in error_str or "size limit" in error_str or "request payload" in error_str or "too large" in error_str:
                logger.error(f"üí• L·ªói k√≠ch th∆∞·ªõc prompt/request khi g·ªçi Gemini ({task_desc}): {str(e)}. "
                             f"L√¥ hi·ªán t·∫°i c√≥ {len(products_batch)} s·∫£n ph·∫©m. H√£y th·ª≠ gi·∫£m PRODUCTS_PER_GEMINI_BATCH.")
                logger.warning(f"‚ö†Ô∏è C·∫ßn gi·∫£m PRODUCTS_PER_GEMINI_BATCH hi·ªán t·∫°i ({PRODUCTS_PER_GEMINI_BATCH}) ƒë·ªÉ tr√°nh l·ªói n√†y")
                return [{"error_too_large": True, "batch_size": len(products_batch), "suggestion": "Gi·∫£m PRODUCTS_PER_GEMINI_BATCH"}]

            if "429" in error_str or "resource_exhausted" in error_str or "too many requests" in error_str or "quota" in error_str:
                retry_delay = 15 + attempt * 10
                logger.warning(f"‚ö†Ô∏è L·ªói quota/rate limit Gemini ({task_desc}), th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                await asyncio.sleep(retry_delay)
            elif "500" in error_str or "internal server error" in error_str or "service temporarily unavailable" in error_str:
                retry_delay = 20 + attempt * 10
                logger.warning(f"‚ö†Ô∏è L·ªói server Gemini (5xx) ({task_desc}), th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"üí• L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API Gemini ({task_desc}) (L·∫ßn {attempt+1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    break
                await asyncio.sleep(15 + attempt * 5)

    logger.error(f"‚ùå V·∫´n g·∫∑p l·ªói sau {max_retries} l·∫ßn th·ª≠ l·∫°i v·ªõi Gemini ({task_desc}) ho·∫∑c kh√¥ng th·ªÉ parse k·∫øt qu·∫£.")
    return []

async def fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension: int) -> list[dict]:
    """
    ‚≠ê ASYNC VERSION: T√¨m ki·∫øm v√† ph√¢n lo·∫°i ƒë·ªì u·ªëng t·ª´ Pinecone v·ªõi x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô.
    
    Args:
        pinecone_index: Pinecone index instance
        vector_dimension: Dimension c·ªßa vector trong Pinecone
        
    Returns:
        list[dict]: Danh s√°ch c√°c ƒë·ªì u·ªëng ƒë√£ ph√¢n lo·∫°i
    """
    identified_drinks_overall = []
    query_vector = np.random.rand(vector_dimension).tolist()

    logger.info(f"üîç B·∫Øt ƒë·∫ßu qu√©t s·∫£n ph·∫©m t·ª´ Pinecone (t·ªëi ƒëa {PRODUCTS_TO_FETCH_FROM_PINECONE} s·∫£n ph·∫©m)...")
    
    try:
        # ‚≠ê CH·∫†Y PINECONE QUERY TRONG EXECUTOR
        loop = asyncio.get_event_loop()
        query_response = await loop.run_in_executor(
            None,
            lambda: pinecone_index.query(
                vector=query_vector,
                top_k=PRODUCTS_TO_FETCH_FROM_PINECONE,
                include_metadata=True
            )
        )
        
        products_from_pinecone = []
        if query_response.matches:
            for match in query_response.matches:
                prod_id = match.metadata.get("doc_id", match.id) if match.metadata else match.id
                prod_name = match.metadata.get("name") if match.metadata else None
                if prod_id and prod_name:
                    products_from_pinecone.append({"id": prod_id, "name": prod_name})
                else:
                    logger.warning(f"S·∫£n ph·∫©m t·ª´ Pinecone (ID vector: {match.id}) thi·∫øu 'name' ho·∫∑c 'doc_id'. B·ªè qua.")
        
        if not products_from_pinecone:
            logger.info("‚ùå Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c t√¨m th·∫•y t·ª´ Pinecone.")
            return []

        logger.info(f"üì¶ L·∫•y ƒë∆∞·ª£c {len(products_from_pinecone)} s·∫£n ph·∫©m t·ª´ Pinecone.")
        
        # ‚≠ê CHIA TH√ÄNH C√ÅC L√î V√Ä X·ª¨ L√ù B·∫∞NG ASYNCIO.GATHER V·ªöI SEMAPHORE
        current_batch_size = PRODUCTS_PER_GEMINI_BATCH
        batch_tasks = []
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS)
        
        async def process_beverage_classification_batch_with_semaphore(products_in_batch, batch_num_log):
            """‚≠ê H√†m x·ª≠ l√Ω m·ªôt batch v·ªõi semaphore control"""
            async with semaphore:
                logger.info(f"ü•§ Batch {batch_num_log}: ƒêang x·ª≠ l√Ω {len(products_in_batch)} s·∫£n ph·∫©m v·ªõi Gemini...")
                start_time = asyncio.get_event_loop().time()
                
                classified_drinks = await call_gemini_api_batch_classification_async(products_in_batch)
                
                end_time = asyncio.get_event_loop().time()
                elapsed = end_time - start_time
                
                # ‚≠ê X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P PROMPT QU√Å L·ªöN V·ªöI LOGGING R√ï R√ÄNG H·ª¢N
                if classified_drinks and isinstance(classified_drinks[0], dict) and classified_drinks[0].get("error_too_large"):
                    error_info = classified_drinks[0]
                    batch_size = error_info.get("batch_size", len(products_in_batch))
                    suggestion = error_info.get("suggestion", "Gi·∫£m batch size")
                    
                    logger.error(f"üí• Batch {batch_num_log}: L√¥ qu√° l·ªõn cho Gemini API")
                    logger.error(f"   - Batch size hi·ªán t·∫°i: {batch_size} s·∫£n ph·∫©m")
                    logger.error(f"   - PRODUCTS_PER_GEMINI_BATCH hi·ªán t·∫°i: {PRODUCTS_PER_GEMINI_BATCH}")
                    logger.error(f"   - G·ª£i √Ω: {suggestion}")
                    logger.warning(f"‚ö†Ô∏è C·∫ßn ƒëi·ªÅu ch·ªânh c·∫•u h√¨nh ƒë·ªÉ tr√°nh l·ªói n√†y ti·∫øp t·ª•c x·∫£y ra")
                    return []
                
                if classified_drinks:
                    logger.info(f"‚úÖ Batch {batch_num_log}: X√°c ƒë·ªãnh ƒë∆∞·ª£c {len(classified_drinks)} ƒë·ªì u·ªëng trong {elapsed:.2f}s")
                else:
                    logger.info(f"‚ö™ Batch {batch_num_log}: Kh√¥ng c√≥ ƒë·ªì u·ªëng n√†o ho·∫∑c c√≥ l·ªói ({elapsed:.2f}s)")
                
                return classified_drinks
        
        # ‚≠ê T·∫†O TASKS CHO T·∫§T C·∫¢ C√ÅC BATCH
        for i in range(0, len(products_from_pinecone), current_batch_size):
            batch_to_send = products_from_pinecone[i : i + current_batch_size]
            if batch_to_send:
                batch_num = i // current_batch_size + 1
                task = process_beverage_classification_batch_with_semaphore(batch_to_send, batch_num)
                batch_tasks.append(task)
        
        # ‚≠ê CH·∫†Y T·∫§T C·∫¢ BATCH ƒê·ªíNG TH·ªúI V·ªöI ASYNCIO.GATHER
        if batch_tasks:
            total_batches = len(batch_tasks)
            logger.info(f"üöÄ ƒêang x·ª≠ l√Ω {total_batches} batch ƒë·ªìng th·ªùi v·ªõi gi·ªõi h·∫°n {MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS} batch c√πng l√∫c...")
            
            start_gather_time = asyncio.get_event_loop().time()
            results_from_gather = await asyncio.gather(*batch_tasks, return_exceptions=True)
            end_gather_time = asyncio.get_event_loop().time()
            
            logger.info(f"‚ö° Ho√†n th√†nh t·∫•t c·∫£ {total_batches} batch trong {end_gather_time - start_gather_time:.2f}s")
            
            # ‚≠ê X·ª¨ L√ù K·∫æT QU·∫¢ T·ª™ GATHER
            successful_batches = 0
            failed_batches = 0
            for i, result in enumerate(results_from_gather):
                if isinstance(result, Exception):
                    logger.error(f"‚ùå L·ªói trong batch {i+1}: {str(result)}")
                    failed_batches += 1
                elif isinstance(result, list):
                    identified_drinks_overall.extend(result)
                    successful_batches += 1
                else:
                    logger.warning(f"‚ö†Ô∏è K·∫øt qu·∫£ kh√¥ng mong ƒë·ª£i t·ª´ batch {i+1}: {type(result)}")
                    failed_batches += 1
            
            logger.info(f"üìä K·∫øt qu·∫£: {successful_batches} batch th√†nh c√¥ng, {failed_batches} batch th·∫•t b·∫°i")
        
    except Exception as e:
        logger.error(f"üí• L·ªói khi query Pinecone ho·∫∑c x·ª≠ l√Ω l√¥ s·∫£n ph·∫©m: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

    return identified_drinks_overall

# ‚≠ê WRAPPER ƒê·ªíNG B·ªò CHO BACKWARD COMPATIBILITY
def fetch_and_filter_drinks_in_batches_sync_wrapper() -> list[dict]:
    """
    ‚≠ê WRAPPER ƒê·ªíNG B·ªò: ƒê·ªÉ chat_flow.py c√≥ th·ªÉ g·ªçi t·ª´ synchronous context.
    """
    try:
        # Kh·ªüi t·∫°o c√°c d·ªãch v·ª• c·∫ßn thi·∫øt
        pinecone_index, vector_dimension = init_services()
        
        # Ki·ªÉm tra xem c√≥ event loop ƒëang ch·∫°y kh√¥ng
        try:
            loop = asyncio.get_running_loop()
            # N·∫øu c√≥ event loop ƒëang ch·∫°y, t·∫°o task m·ªõi
            logger.info("üí° Ph√°t hi·ªán event loop ƒëang ch·∫°y, s·ª≠ d·ª•ng create_task")
            task = loop.create_task(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))
            # Wait cho task ho√†n th√†nh (blocking)
            import concurrent.futures
            import threading
            
            def run_in_new_loop():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension)
                    )
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_new_loop)
                return future.result()
                
        except RuntimeError:
            # Kh√¥ng c√≥ event loop ƒëang ch·∫°y, c√≥ th·ªÉ d√πng asyncio.run
            logger.info("üí° Kh√¥ng c√≥ event loop ƒëang ch·∫°y, s·ª≠ d·ª•ng asyncio.run")
            return asyncio.run(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))
            
    except Exception as e:
        logger.error(f"üí• L·ªói trong wrapper ƒë·ªìng b·ªô c·ªßa fetch_and_filter_drinks: {e}", exc_info=True)
        return []

# ‚≠ê GI·ªÆ L·∫†I H√ÄM C≈® ƒê·ªÇ T∆Ø∆†NG TH√çCH NG∆Ø·ª¢C (Deprecated)
def fetch_and_filter_drinks_in_batches(pinecone_index, gemini_model, vector_dimension: int) -> list[dict]:
    """H√†m c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c - DEPRECATED: s·ª≠ d·ª•ng async version"""
    logger.warning("‚ö†Ô∏è S·ª≠ d·ª•ng h√†m fetch_and_filter_drinks_in_batches C≈® - khuy·∫øn ngh·ªã s·ª≠ d·ª•ng async version")
    return asyncio.run(fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dimension))

if __name__ == "__main__":
    async def main_beverage_test_async():
        """‚≠ê H√ÄM TEST ASYNC CH√çNH"""
        try:
            print("üîß ƒêang kh·ªüi t·∫°o c√°c d·ªãch v·ª•...")
            pinecone_index, vector_dim = init_services()
            
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh t√¨m ki·∫øm ƒë·ªì u·ªëng t·ª´ Pinecone index '{PINECONE_INDEX_NAME}' (ASYNC VERSION)...")
            print(f"üìä C·∫•u h√¨nh:")
            print(f"   - S·∫£n ph·∫©m t·ªëi ƒëa t·ª´ Pinecone: {PRODUCTS_TO_FETCH_FROM_PINECONE}")
            print(f"   - K√≠ch th∆∞·ªõc m·ªói batch: {PRODUCTS_PER_GEMINI_BATCH}")
            print(f"   - ƒê·ªìng th·ªùi t·ªëi ƒëa: {MAX_CONCURRENT_GEMINI_BEVERAGE_CLASSIFICATION_CALLS} batch")
            print(f"   - Vector dimension: {vector_dim}")
            print(f"   - Gemini model: {GEMINI_MODEL_NAME}")
            
            # L·∫•y th·ªëng k√™ API key
            api_key_manager = get_api_key_manager()
            print(f"   - API keys available: {api_key_manager.total_keys()}")
            
            import time
            start_time = time.time()
            all_drinks_found = await fetch_and_filter_drinks_in_batches_async(pinecone_index, vector_dim)
            end_time = time.time()
            
            print("\nüìã --- DANH S√ÅCH ƒê·ªí U·ªêNG ƒê∆Ø·ª¢C T√åM TH·∫§Y ---")
            if all_drinks_found:
                # Lo·∫°i b·ªè tr√πng l·∫∑p n·∫øu c√≥
                unique_drinks = []
                seen_ids = set()
                for drink in all_drinks_found:
                    if drink['product_id'] not in seen_ids:
                        unique_drinks.append(drink)
                        seen_ids.add(drink['product_id'])
                
                print(json.dumps(unique_drinks[:10], indent=2, ensure_ascii=False))  # Ch·ªâ hi·ªÉn th·ªã 10 ƒë·∫ßu ti√™n
                if len(unique_drinks) > 10:
                    print(f"... v√† {len(unique_drinks) - 10} ƒë·ªì u·ªëng kh√°c")
                
                print(f"\n‚úÖ T·ªïng c·ªông t√¨m th·∫•y {len(unique_drinks)} s·∫£n ph·∫©m ƒë·ªì u·ªëng duy nh·∫•t.")
                
                # Th·ªëng k√™ hi·ªáu su·∫•t
                total_processing_time = end_time - start_time
                products_per_second = len(unique_drinks) / total_processing_time if total_processing_time > 0 else 0
                print(f"‚ö° Hi·ªáu su·∫•t: {products_per_second:.2f} ƒë·ªì u·ªëng/gi√¢y")
                
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ƒë·ªì u·ªëng n√†o ho·∫∑c kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c ph√¢n lo·∫°i l√† ƒë·ªì u·ªëng.")
                
            print(f"‚è±Ô∏è T·ªïng th·ªùi gian th·ª±c hi·ªán: {end_time - start_time:.2f} gi√¢y.")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ s·ª≠ d·ª•ng API key
            usage_stats = api_key_manager.get_usage_statistics()
            print(f"\nüìä Th·ªëng k√™ s·ª≠ d·ª•ng API key:")
            for key_masked, usage_count in usage_stats.items():
                print(f"   - Key {key_masked}: {usage_count} l·∫ßn s·ª≠ d·ª•ng")

        except ValueError as ve: 
            print(f"‚ùå L·ªói c·∫•u h√¨nh ho·∫∑c gi√° tr·ªã: {ve}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën ·ªü main: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

    # ‚≠ê S·ª¨ D·ª§NG ASYNCIO.RUN CHO MAIN
    asyncio.run(main_beverage_test_async())