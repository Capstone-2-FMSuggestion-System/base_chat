import os
import sys
import logging
import asyncio
import json
import re
from dotenv import load_dotenv
import google.generativeai as genai
from typing import Optional, Tuple, List, Dict, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.services.api_key_manager import get_api_key_manager
from app.services.cache_service import CacheService

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- C·∫•u h√¨nh ---
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-lite")
PRODUCTS_PER_GEMINI_BATCH = 200  # Gi·∫£m ƒë·ªÉ tr√°nh rate limit, v·∫´n hi·ªáu qu·∫£ h∆°n 80
MAX_CONCURRENT_GEMINI_PRODUCT_CALLS = min(get_api_key_manager().total_keys(), 3)  # Gi·∫£m ƒë·ªÉ tr√°nh quota overwhelm
REQUEST_DELAY = 0.5  # Delay 500ms gi·ªØa c√°c requests ƒë·ªÉ tr√°nh rate limit

api_key_manager = get_api_key_manager()

# Ki·ªÉm tra API keys
if not api_key_manager.is_healthy():
    logger.error("ApiKeyManager kh√¥ng kh·∫£ d·ª•ng. Ki·ªÉm tra c·∫•u h√¨nh API keys trong file .env")
    raise ValueError("ApiKeyManager kh√¥ng kh·∫£ d·ª•ng. Ki·ªÉm tra c·∫•u h√¨nh API keys trong file .env")

logger.info(f"‚úÖ Product Find Tool (Cache Version) ƒë√£ kh·ªüi t·∫°o v·ªõi {len(api_key_manager.get_all_keys())} API keys c√≥ s·∫µn")

# C√°c functions Pinecone kh√¥ng c√≤n c·∫ßn thi·∫øt n·ªØa

def deduplicate_products_cache(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    L·ªçc b·ªè s·∫£n ph·∫©m tr√πng l·∫∑p theo product_id v√† t√™n t∆∞∆°ng t·ª±
    """
    if not products:
        return []
    
    # Step 1: Lo·∫°i b·ªè tr√πng product_id
    seen_ids = set()
    unique_by_id = []
    for product in products:
        product_id = str(product.get("product_id", ""))
        if product_id and product_id not in seen_ids:
            seen_ids.add(product_id)
            unique_by_id.append(product)
    
    # Step 2: Lo·∫°i b·ªè tr√πng t√™n g·∫ßn gi·ªëng (fuzzy matching)
    import difflib
    final_products = []
    seen_names = []
    
    for product in unique_by_id:
        name = product.get("name", "").lower().strip()
        if not name:
            continue
            
        # Ki·ªÉm tra similarity v·ªõi c√°c t√™n ƒë√£ c√≥
        is_duplicate = False
        for existing_name in seen_names:
            similarity = difflib.SequenceMatcher(None, name, existing_name).ratio()
            if similarity >= 0.85:  # 85% gi·ªëng nhau = duplicate
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_names.append(name)
            final_products.append(product)
    
    logger.info(f"DEDUP: {len(products)} ‚Üí {len(final_products)} s·∫£n ph·∫©m (lo·∫°i b·ªè {len(products) - len(final_products)} tr√πng l·∫∑p)")
    return final_products

async def ensure_products_cache_available() -> List[Dict[str, Any]]:
    """
    ƒê·∫£m b·∫£o cache s·∫£n ph·∫©m c√≥ s·∫µn, n·∫øu ch∆∞a c√≥ th√¨ t·∫°o ngay l·∫≠p t·ª©c.
    ƒê∆∞·ª£c g·ªçi t·ª± ƒë·ªông khi ng∆∞·ªùi d√πng b·∫Øt ƒë·∫ßu chat v√† c·∫ßn t√¨m s·∫£n ph·∫©m.
    """
    logger.info("PRODUCT_FIND_TOOL: Ki·ªÉm tra cache s·∫£n ph·∫©m...")
    
    # Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_products = CacheService.get_all_products_list()
    if cached_products and len(cached_products) > 0:
        logger.info(f"PRODUCT_FIND_TOOL: Cache s·∫£n ph·∫©m ƒë√£ c√≥ s·∫µn v·ªõi {len(cached_products)} s·∫£n ph·∫©m")
        return deduplicate_products_cache(cached_products)
    
    logger.info("PRODUCT_FIND_TOOL: Cache s·∫£n ph·∫©m ch∆∞a c√≥, ƒëang t·∫°o ngay l·∫≠p t·ª©c...")
    
    # Import background task function
    try:
        from app.services.background_products_cache import refresh_products_cache_task
        
        # Ch·∫°y task refresh cache ngay l·∫≠p t·ª©c (ƒë·ªìng b·ªô)
        cache_result = await refresh_products_cache_task()
        
        if cache_result.get("success"):
            logger.info(f"PRODUCT_FIND_TOOL: ƒê√£ t·∫°o cache th√†nh c√¥ng v·ªõi {cache_result.get('products_count', 0)} s·∫£n ph·∫©m")
            # L·∫•y l·∫°i t·ª´ cache sau khi ƒë√£ t·∫°o v√† dedup
            raw_products = CacheService.get_all_products_list() or []
            return deduplicate_products_cache(raw_products)
        else:
            logger.error(f"PRODUCT_FIND_TOOL: T·∫°o cache th·∫•t b·∫°i: {cache_result.get('error')}")
            return []
            
    except Exception as e:
        logger.error(f"PRODUCT_FIND_TOOL: L·ªói khi t·∫°o cache s·∫£n ph·∫©m: {e}", exc_info=True)
        return []


async def call_gemini_api_generic_async(prompt: str, task_description: str, max_retries: int = 3) -> str:
    """H√†m g·ªçi Gemini b·∫•t ƒë·ªìng b·ªô v·ªõi smart rate limiting."""
    loop = asyncio.get_event_loop()
    
    # Smart delay ƒë·ªÉ tr√°nh overwhelm quota
    await asyncio.sleep(REQUEST_DELAY)
    
    current_api_key = api_key_manager.get_next_key()
    if not current_api_key:
        error_msg = f"Kh√¥ng c√≥ API key kh·∫£ d·ª•ng cho {task_description}"
        logger.error(error_msg)
        return json.dumps({"error": error_msg})
    
    logger.info(f"üîë {task_description} s·ª≠ d·ª•ng key: {current_api_key[-8:]}...")
    
    for attempt in range(max_retries):
        try:
            genai.configure(api_key=current_api_key)
            temp_gemini_model = genai.GenerativeModel(GEMINI_MODEL_NAME)
            
            response = await loop.run_in_executor(
                None,
                lambda: temp_gemini_model.generate_content(
                    prompt,
                    generation_config={"max_output_tokens": 2048}  # Gi·∫£m output ƒë·ªÉ ti·∫øt ki·ªám quota
                )
            )
            
            if response.parts:
                return response.text
            else:
                logger.warning(f"Gemini kh√¥ng tr·∫£ v·ªÅ n·ªôi dung ({task_description}). Response: {response}")
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    logger.warning(f"B·ªã ch·∫∑n b·ªüi ({task_description}): {response.prompt_feedback.block_reason_message}")
                return json.dumps({"error": f"Gemini kh√¥ng tr·∫£ v·ªÅ n·ªôi dung ({task_description})."})
                
        except Exception as e:
            error_str = str(e).lower()
            
            if "429" in error_str or "resource_exhausted" in error_str or "too many requests" in error_str:
                # Exponential backoff th√¥ng minh h∆°n
                base_delay = 15  # Base delay l√¢u h∆°n
                retry_delay = base_delay * (2 ** attempt) + (attempt * 5)  # Exponential + linear
                logger.warning(f"‚ö†Ô∏è Rate limit ({task_description}), ƒë·ª£i {retry_delay}s tr∆∞·ªõc retry {attempt+1}/{max_retries}")
                await asyncio.sleep(retry_delay)
                
                # Th·ª≠ key kh√°c n·∫øu c√≥
                current_api_key = api_key_manager.get_next_key()
                if not current_api_key:
                    logger.error("H·∫øt API keys kh·∫£ d·ª•ng")
                    return json.dumps({"error": "H·∫øt API keys kh·∫£ d·ª•ng"})
                
            elif "invalid_argument" in error_str and ("token" in error_str or "request payload" in error_str or "size limit" in error_str):
                logger.error(f"L·ªói: Prompt qu√° l·ªõn ({task_description}). {str(e)}")
                return json.dumps({"error": f"Prompt qu√° l·ªõn cho model x·ª≠ l√Ω ({task_description})."})
                
            else:
                logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API Gemini ({task_description}) (L·∫ßn {attempt+1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    return json.dumps({"error": f"L·ªói kh√¥ng x√°c ƒë·ªãnh v√† kh√¥ng th·ªÉ ph·ª•c h·ªìi t·ª´ Gemini ({task_description})."})
                await asyncio.sleep(10 + attempt * 5)
                
    logger.error(f"V·∫´n g·∫∑p l·ªói sau {max_retries} l·∫ßn th·ª≠ l·∫°i v·ªõi Gemini ({task_description}).")
    return json.dumps({"error": f"Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Gemini sau nhi·ªÅu l·∫ßn th·ª≠ ({task_description})."})


async def analyze_user_request_with_gemini_async(user_request_text: str) -> dict:
    """B∆∞·ªõc 1: S·ª≠ d·ª•ng Gemini ƒë·ªÉ ph√¢n t√≠ch y√™u c·∫ßu ng∆∞·ªùi d√πng."""
    prompt = f"""
    Ph√¢n t√≠ch ƒëo·∫°n vƒÉn b·∫£n ƒë·ªÉ t√¨m **Y√äU C·∫¶U TH·ª∞C T·∫æ** c·ªßa ng∆∞·ªùi d√πng v·ªÅ vi·ªác mua nguy√™n li·ªáu n·∫•u ƒÉn.

    **VƒÇN B·∫¢N C·∫¶N PH√ÇN T√çCH:**
    "{user_request_text}"

    **QUY T·∫ÆC QUAN TR·ªåNG:**
    1. **CH·ªà EXTRACT NGUY√äN LI·ªÜU T·ª™ Y√äU C·∫¶U TH·ª∞C T·∫æ** c·ªßa ng∆∞·ªùi d√πng (c√¢u h·ªèi, ƒë·∫∑t h√†ng, mua s·∫Øm)
    2. **B·ªé QUA PH·∫¶N G·ª¢I √ù/M√î T·∫¢ C√îNG TH·ª®C** t·ª´ AI assistant ho·∫∑c ng∆∞·ªùi kh√°c
    3. **T√åM C√ÅC KEYWORD:** "t√¥i c·∫ßn", "mua", "l√†m m√≥n", "n·∫•u", "t√¥i mu·ªën"
    4. **PH√ÇN BI·ªÜT NG·ªÆ C·∫¢NH:** G·ª£i √Ω vs Y√™u c·∫ßu th·ª±c t·∫ø

    **C√ÅCH NH·∫¨N DI·ªÜN:**
    - **Y√äU C·∫¶U TH·ª∞C T·∫æ:** "T√¥i mu·ªën n·∫•u canh chua, c·∫ßn mua c√° l√≥c"
    - **G·ª¢I √ù (B·ªé QUA):** "Canh b√≠ ƒë·ªè ƒë·∫≠u xanh - Nguy√™n li·ªáu: Th·ªãt bƒÉm, b√≠ ƒë·ªè..."

    **OUTPUT JSON:**
    {{
        "dish_name": "T√™n m√≥n ƒÉn ng∆∞·ªùi d√πng TH·ª∞C S·ª∞ mu·ªën n·∫•u (null n·∫øu ch·ªâ h·ªèi g·ª£i √Ω)",
        "requested_ingredients": ["Danh s√°ch nguy√™n li·ªáu ng∆∞·ªùi d√πng TH·ª∞C S·ª∞ y√™u c·∫ßu mua"]
    }}

    **V√ç D·ª§:**

    Input 1: "T√¥i mu·ªën n·∫•u canh chua c√° l√≥c, c·∫ßn mua c√° l√≥c, me, c√† chua"
    {{
        "dish_name": "Canh chua c√° l√≥c", 
        "requested_ingredients": ["c√° l√≥c", "me", "c√† chua"]
    }}

    Input 2: "G·ª£i √Ω m√≥n ƒÉn: Canh b√≠ ƒë·ªè - Nguy√™n li·ªáu: th·ªãt bƒÉm, b√≠ ƒë·ªè. C∆°m sen - Nguy√™n li·ªáu: g·∫°o l·ª©t, h·∫°t sen"
    {{
        "dish_name": null,
        "requested_ingredients": []
    }}

    Input 3: "C·∫£m ∆°n g·ª£i √Ω! T√¥i s·∫Ω l√†m canh b√≠ ƒë·ªè, c·∫ßn mua th·ªãt bƒÉm v√† b√≠ ƒë·ªè"
    {{
        "dish_name": "Canh b√≠ ƒë·ªè",
        "requested_ingredients": ["th·ªãt bƒÉm", "b√≠ ƒë·ªè"]
    }}

    CH·ªà TR·∫¢ V·ªÄ JSON - KH√îNG GI·∫¢I TH√çCH TH√äM.
    """
    logger.info("G·ª≠i y√™u c·∫ßu ph√¢n t√≠ch nguy√™n li·ªáu ƒë·∫øn Gemini...")
    response_text = await call_gemini_api_generic_async(prompt, "Ph√¢n t√≠ch y√™u c·∫ßu ng∆∞·ªùi d√πng")
    
    try:
        cleaned_response_text = response_text.strip()
        if cleaned_response_text.startswith("```json"):
            cleaned_response_text = cleaned_response_text[7:]
        if cleaned_response_text.endswith("```"):
            cleaned_response_text = cleaned_response_text[:-3]
        
        analysis_result = json.loads(cleaned_response_text)
        if isinstance(analysis_result, list) and len(analysis_result) > 0:
            # N·∫øu Gemini tr·∫£ v·ªÅ list, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
            analysis_result = analysis_result[0]
            
        if "error" in analysis_result:
            logger.error(f"L·ªói t·ª´ Gemini khi ph√¢n t√≠ch y√™u c·∫ßu: {analysis_result['error']}")
            return {
                "dish_name": None,
                "requested_ingredients": [],
                "error": analysis_result['error']
            }
        
        # ƒê·∫£m b·∫£o format ƒë√∫ng
        result = {
            "dish_name": analysis_result.get("dish_name"),
            "requested_ingredients": analysis_result.get("requested_ingredients", [])
        }
        
        logger.info(f"K·∫øt qu·∫£ ph√¢n t√≠ch t·ª´ Gemini: {result}")
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"L·ªói gi·∫£i m√£ JSON t·ª´ ph√¢n t√≠ch y√™u c·∫ßu c·ªßa Gemini: {e}")
        logger.error(f"Ph·∫£n h·ªìi g·ªëc t·ª´ Gemini (ph√¢n t√≠ch): {response_text}")
        return {
            "dish_name": None,
            "requested_ingredients": [],
            "error": "L·ªói parse JSON ph√¢n t√≠ch y√™u c·∫ßu."
        }
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ph√¢n t√≠ch y√™u c·∫ßu: {str(e)}")
        return {
            "dish_name": None,
            "requested_ingredients": [],
            "error": f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}"
        }


async def find_product_for_ingredient_async_from_cache(
    ingredient_name: str,
    all_cached_products: List[Dict[str, Any]]
) -> Tuple[Optional[str], Optional[str]]:
    """
    T√¨m v√† ch·ªçn product_id cho m·ªôt nguy√™n li·ªáu c·ª• th·ªÉ t·ª´ danh s√°ch s·∫£n ph·∫©m ƒë√£ cache.
    S·ª≠ d·ª•ng Gemini ƒë·ªÉ ch·ªçn s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t t·ª´ c√°c l√¥ (batches).
    Tr·∫£ v·ªÅ (product_id, product_name) ho·∫∑c (None, None) n·∫øu kh√¥ng t√¨m th·∫•y.
    """
    if not all_cached_products:
        logger.warning(f"PRODUCT_FIND_TOOL: Kh√¥ng c√≥ s·∫£n ph·∫©m trong cache ƒë·ªÉ t√¨m '{ingredient_name}'")
        return None, None

    logger.info(f"PRODUCT_FIND_TOOL: B·∫Øt ƒë·∫ßu t√¨m s·∫£n ph·∫©m cho '{ingredient_name}' t·ª´ {len(all_cached_products)} s·∫£n ph·∫©m cache")

    # Chia th√†nh c√°c l√¥ ho·∫∑c g·ª≠i to√†n b·ªô n·∫øu cache nh·ªè
    if len(all_cached_products) <= PRODUCTS_PER_GEMINI_BATCH:
        # Cache nh·ªè - g·ª≠i to√†n b·ªô trong 1 l√¥
        num_batches = 1
        logger.info(f"PRODUCT_FIND_TOOL: Cache nh·ªè ({len(all_cached_products)} s·∫£n ph·∫©m) - g·ª≠i to√†n b·ªô trong 1 l√¥")
    else:
        # Cache l·ªõn - chia l√¥
        num_batches = (len(all_cached_products) + PRODUCTS_PER_GEMINI_BATCH - 1) // PRODUCTS_PER_GEMINI_BATCH
        logger.info(f"PRODUCT_FIND_TOOL: Chia th√†nh {num_batches} l√¥ (m·ªói l√¥ {PRODUCTS_PER_GEMINI_BATCH} s·∫£n ph·∫©m)")

    for i in range(num_batches):
        if num_batches == 1:
            # G·ª≠i to√†n b·ªô cache
            current_product_batch = all_cached_products
        else:
            # Chia l√¥ th√¥ng th∆∞·ªùng
            batch_start_index = i * PRODUCTS_PER_GEMINI_BATCH
            current_product_batch = all_cached_products[batch_start_index : batch_start_index + PRODUCTS_PER_GEMINI_BATCH]

        if not current_product_batch:
            continue

        # T·∫°o chu·ªói s·∫£n ph·∫©m cho prompt
        product_batch_str = "\n".join(
            [f"- ID: {p['product_id']}, T√™n S·∫£n Ph·∫©m: {p['name']}" for p in current_product_batch]
        )
        
        # Prompt t·ªëi ∆∞u cho Top 3 v·ªõi confidence score
        prompt_for_selection = f"""
        T√¨m s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng v·ªõi nguy√™n li·ªáu: "{ingredient_name}".
        L√¥ s·∫£n ph·∫©m (ID v√† T√™n):
        {product_batch_str}

        **NHI·ªÜM V·ª§:** Ch·ªçn T·ªêI ƒêA 3 s·∫£n ph·∫©m PH√ô H·ª¢P NH·∫§T, s·∫Øp x·∫øp theo m·ª©c ƒë·ªô ph√π h·ª£p gi·∫£m d·∫ßn.

        **QUY T·∫ÆC:**
        1. **KH·ªöP T√äN:** ∆Øu ti√™n s·∫£n ph·∫©m c√≥ t√™n ch·ª©a ƒë√∫ng ho·∫∑c g·∫ßn t√™n nguy√™n li·ªáu "{ingredient_name}"
        2. **LO·∫†I TR·ª™ CH·∫æ BI·∫æN:** KH√îNG ch·ªçn s·∫£n ph·∫©m ch·∫ø bi·∫øn s·∫µn nh∆∞ "M√¨ g√≥i v·ªã {ingredient_name}"
        3. **∆ØU TI√äN NGUY√äN LI·ªÜU TH√î/S∆† CH·∫æ**
        4. **CONFIDENCE:** ƒê√°nh gi√° ƒë·ªô tin c·∫≠y t·ª´ 0.0-1.0 (1.0 = ho√†n to√†n ch·∫Øc ch·∫Øn)

        **TR·∫¢ V·ªÄ JSON ARRAY (KH√îNG MARKDOWN, KH√îNG GI·∫¢I TH√çCH):**
        [
            {{
                "selected_product_id": "ID_1",
                "selected_product_name": "T√äN_1", 
                "confidence_score": 0.95,
                "reason": "Kh·ªõp ch√≠nh x√°c t√™n nguy√™n li·ªáu"
            }},
            {{
                "selected_product_id": "ID_2",
                "selected_product_name": "T√äN_2",
                "confidence_score": 0.8,
                "reason": "G·∫ßn ƒë√∫ng, d·∫°ng s∆° ch·∫ø"
            }}
        ]
        N·∫øu KH√îNG t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p: []
        """
        
        task_desc = f"PRODUCT_FIND_TOOL: Ch·ªçn s·∫£n ph·∫©m cho '{ingredient_name}' t·ª´ cache batch {i+1}/{num_batches}"
        logger.info(f"{task_desc} - G·ª≠i {len(current_product_batch)} s·∫£n ph·∫©m cho Gemini")
        
        response_text = await call_gemini_api_generic_async(prompt_for_selection, task_desc)
        
        try:
            # Clean and parse JSON response cho Top 3 results
            cleaned_response_text = response_text.strip()
            
            # T√¨m JSON array trong response
            array_match = re.search(r'\[.*\]', cleaned_response_text, re.DOTALL)
            if array_match:
                cleaned_response_text = array_match.group(0)
            else:
                logger.warning(f"PRODUCT_FIND_TOOL: Kh√¥ng t√¨m th·∫•y JSON array trong response (batch {i+1} cho '{ingredient_name}')")
                continue

            selection_results = json.loads(cleaned_response_text)
            
            # Ki·ªÉm tra n·∫øu c√≥ l·ªói ho·∫∑c array r·ªóng
            if not isinstance(selection_results, list) or not selection_results:
                logger.info(f"PRODUCT_FIND_TOOL: Gemini kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong batch {i+1} cho '{ingredient_name}'")
                continue
            
            # X·ª≠ l√Ω top 3 results, t√¨m result t·ªët nh·∫•t
            best_result = None
            for result in selection_results:
                if not isinstance(result, dict):
                    continue
                    
                product_id_raw = result.get("selected_product_id")
                product_name = result.get("selected_product_name")
                confidence = result.get("confidence_score", 0.0)
                
                # Validate product_id
                product_id = None
                if product_id_raw is not None and product_id_raw != "null":
                    try:
                        product_id = str(int(product_id_raw))
                    except (ValueError, TypeError):
                        continue
                
                if product_id and product_name and confidence > 0.0:
                    if best_result is None or confidence > best_result["confidence"]:
                        best_result = {
                            "product_id": product_id,
                            "product_name": product_name,
                            "confidence": confidence,
                            "reason": result.get("reason", "")
                        }
            
                            # X·ª≠ l√Ω multiple results ƒë·ªÉ t√¨m alternative n·∫øu best b·ªã duplicate
            if best_result:
                confidence = best_result["confidence"]
                
                # Th·ª≠ t·∫•t c·∫£ results theo th·ª© t·ª± confidence gi·∫£m d·∫ßn
                for result_candidate in sorted(selection_results, key=lambda x: x.get("confidence_score", 0), reverse=True):
                    if not isinstance(result_candidate, dict):
                        continue
                        
                    candidate_id_raw = result_candidate.get("selected_product_id")
                    candidate_name = result_candidate.get("selected_product_name")
                    candidate_confidence = result_candidate.get("confidence_score", 0.0)
                    
                    # Validate candidate_id
                    candidate_id = None
                    if candidate_id_raw is not None and candidate_id_raw != "null":
                        try:
                            candidate_id = str(int(candidate_id_raw))
                        except (ValueError, TypeError):
                            continue
                    
                    if candidate_id and candidate_name and candidate_confidence >= 0.5:
                        if candidate_confidence >= 0.75:  # Ng∆∞·ª°ng tin c·∫≠y cao - d·ª´ng ngay
                            logger.info(f"PRODUCT_FIND_TOOL: ‚ö° EARLY STOP - Gemini ch·ªçn ID: {candidate_id}, T√™n: {candidate_name} cho '{ingredient_name}' (batch {i+1}, confidence: {candidate_confidence:.2f})")
                            return candidate_id, candidate_name
                        elif i == num_batches - 1:  # Batch cu·ªëi, ch·∫•p nh·∫≠n k·∫øt qu·∫£ n√†y
                            logger.info(f"PRODUCT_FIND_TOOL: Ch·ªçn candidate cu·ªëi ID: {candidate_id}, T√™n: {candidate_name} cho '{ingredient_name}' (confidence: {candidate_confidence:.2f})")
                            return candidate_id, candidate_name
                        else:  # Ch∆∞a ph·∫£i batch cu·ªëi, ti·∫øp t·ª•c t√¨m
                            logger.info(f"PRODUCT_FIND_TOOL: T√¨m th·∫•y candidate ID: {candidate_id} (confidence: {candidate_confidence:.2f}), ti·∫øp t·ª•c t√¨m batch ti·∫øp theo...")
                            break  # Tho√°t kh·ªèi v√≤ng l·∫∑p candidates, chuy·ªÉn sang batch ti·∫øp theo
        
        except json.JSONDecodeError as e:
            logger.warning(f"PRODUCT_FIND_TOOL: L·ªói parse JSON t·ª´ Gemini (batch {i+1} cho '{ingredient_name}'): {e}")
            logger.warning(f"PRODUCT_FIND_TOOL: Raw response: {response_text[:200]}...")
            continue
        except Exception as e:
            logger.error(f"PRODUCT_FIND_TOOL: L·ªói x·ª≠ l√Ω response Gemini (batch {i+1} cho '{ingredient_name}'): {e}")
            continue

    logger.info(f"PRODUCT_FIND_TOOL: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p cho '{ingredient_name}' sau khi duy·ªát h·∫øt cache")
    return None, None


async def process_user_request_async(user_request_text: str) -> dict:
    """
    Quy tr√¨nh ch√≠nh: Ph√¢n t√≠ch y√™u c·∫ßu, t√¨m s·∫£n ph·∫©m cho t·ª´ng nguy√™n li·ªáu t·ª´ cache.
    """
    analysis = await analyze_user_request_with_gemini_async(user_request_text)
    
    if "error" in analysis:  # Ki·ªÉm tra l·ªói t·ª´ b∆∞·ªõc ph√¢n t√≠ch
        return {
            "error": analysis["error"],
            "dish_name_identified": analysis.get("dish_name"),
            "ingredient_mapping_results": [],
            "ingredients_not_found_product_id": analysis.get("requested_ingredients", [])
        }

    if not analysis.get("requested_ingredients"):
        logger.info("PRODUCT_FIND_TOOL: Kh√¥ng c√≥ nguy√™n li·ªáu n√†o ƒë∆∞·ª£c y√™u c·∫ßu sau ph√¢n t√≠ch")
        return {
            "dish_name_identified": analysis.get("dish_name"),
            "processed_request": user_request_text,
            "ingredient_mapping_results": [],
            "ingredients_not_found_product_id": []
        }

    dish_name = analysis.get("dish_name")
    requested_ingredients = list(set(analysis.get("requested_ingredients", [])))  # Lo·∫°i b·ªè tr√πng l·∫∑p
    
    logger.info(f"PRODUCT_FIND_TOOL: M√≥n ƒÉn: {dish_name}, Nguy√™n li·ªáu y√™u c·∫ßu (ƒë√£ l·ªçc tr√πng): {requested_ingredients}")

    # L·∫•y to√†n b·ªô danh s√°ch s·∫£n ph·∫©m t·ª´ cache, n·∫øu ch∆∞a c√≥ th√¨ t·∫°o ngay
    all_cached_products = await ensure_products_cache_available()
    if not all_cached_products:
        logger.error("PRODUCT_FIND_TOOL: L·ªói nghi√™m tr·ªçng - Kh√¥ng th·ªÉ l·∫•y ho·∫∑c t·∫°o cache s·∫£n ph·∫©m")
        return {
            "error": "L·ªói h·ªá th·ªëng: Kh√¥ng th·ªÉ truy c·∫≠p d·ªØ li·ªáu s·∫£n ph·∫©m.",
            "dish_name_identified": dish_name,
            "ingredient_mapping_results": [],
            "ingredients_not_found_product_id": requested_ingredients
        }
    logger.info(f"PRODUCT_FIND_TOOL: S·∫Ω t√¨m ki·∫øm t·ª´ {len(all_cached_products)} s·∫£n ph·∫©m trong cache")

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_GEMINI_PRODUCT_CALLS)
    used_product_ids = set()  # Track ƒë·ªÉ tr√°nh tr√πng l·∫∑p product_id
    used_product_ids_lock = asyncio.Lock()  # Thread-safe access to used_product_ids
    
    async def process_single_ingredient_with_semaphore(ingredient_name: str):
        async with semaphore:
            logger.info(f"PRODUCT_FIND_TOOL: üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω '{ingredient_name}' (concurrency: {MAX_CONCURRENT_GEMINI_PRODUCT_CALLS})")
            start_time = asyncio.get_event_loop().time()
            
            product_id, product_name = await find_product_for_ingredient_async_from_cache(
                ingredient_name, 
                all_cached_products
            )
            
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # T·∫°o k·∫øt qu·∫£ cho nguy√™n li·ªáu n√†y
            mapping_result_for_ingredient = {
                "requested_ingredient": ingredient_name,
                "product_id": None,
                "product_name": None,
                "status": "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p"
            }

            if product_id and product_name:
                # Thread-safe check v√† add product_id
                async with used_product_ids_lock:
                    if product_id in used_product_ids:
                        logger.warning(f"PRODUCT_FIND_TOOL: ‚ö†Ô∏è Product ID {product_id} ƒë√£ ƒë∆∞·ª£c d√πng cho nguy√™n li·ªáu kh√°c, b·ªè qua cho '{ingredient_name}'")
                        mapping_result_for_ingredient["status"] = f"S·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c g√°n cho nguy√™n li·ªáu kh√°c"
                    else:
                        used_product_ids.add(product_id)  # ƒê√°nh d·∫•u ƒë√£ d√πng
                        mapping_result_for_ingredient.update({
                            "product_id": product_id,
                            "product_name": product_name,
                            "status": "ƒê√£ t√¨m th·∫•y s·∫£n ph·∫©m"
                        })
                        logger.info(f"PRODUCT_FIND_TOOL: ‚úÖ '{ingredient_name}' -> ID: {product_id}, T√™n: {product_name} ({elapsed:.2f}s)")
            else:
                logger.info(f"PRODUCT_FIND_TOOL: ‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m cho '{ingredient_name}' ({elapsed:.2f}s)")
            
            return mapping_result_for_ingredient

    tasks = [process_single_ingredient_with_semaphore(ing) for ing in requested_ingredients]
    logger.info(f"PRODUCT_FIND_TOOL: üõí T·∫°o {len(tasks)} tasks song song ƒë·ªÉ t√¨m s·∫£n ph·∫©m (MAX_CONCURRENT: {MAX_CONCURRENT_GEMINI_PRODUCT_CALLS})")
    
    # Ch·∫°y c√°c task song song v·ªõi 7 API keys
    all_mapping_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # X·ª≠ l√Ω k·∫øt qu·∫£ cu·ªëi c√πng v·ªõi counting ch√≠nh x√°c
    final_results_for_frontend = []
    ingredients_not_found = []
    successful_mappings = 0
    duplicated_mappings = 0
    error_mappings = 0
    
    for i, res_item in enumerate(all_mapping_results):
        if isinstance(res_item, Exception):
            # L·ªói trong task x·ª≠ l√Ω nguy√™n li·ªáu
            ing_name = requested_ingredients[i]
            logger.error(f"PRODUCT_FIND_TOOL: üí• L·ªói trong task '{ing_name}': {res_item}")
            final_results_for_frontend.append({
                "requested_ingredient": ing_name,
                "product_id": None,
                "product_name": None,
                "status": f"L·ªói x·ª≠ l√Ω: {str(res_item)}"
            })
            ingredients_not_found.append(ing_name)
            error_mappings += 1
            continue

        req_ing = res_item["requested_ingredient"]
        prod_id = res_item.get("product_id")
        status = res_item.get("status", "")
        
        final_results_for_frontend.append(res_item)
        
        # Ph√¢n lo·∫°i k·∫øt qu·∫£ ƒë·ªÉ counting ch√≠nh x√°c
        if prod_id:  # C√≥ product_id = th√†nh c√¥ng
            successful_mappings += 1
        elif "ƒë√£ ƒë∆∞·ª£c g√°n cho nguy√™n li·ªáu kh√°c" in status:  # Duplicate
            duplicated_mappings += 1
            # Kh√¥ng add v√†o ingredients_not_found v√¨ ƒë√£ t√¨m th·∫•y s·∫£n ph·∫©m, ch·ªâ b·ªã tr√πng
        else:  # Th·ª±c s·ª± kh√¥ng t√¨m th·∫•y
            ingredients_not_found.append(req_ing)
            
    # T√≠nh to√°n metrics chi ti·∫øt
    total_processed = len(requested_ingredients)
    actual_products_found = successful_mappings + duplicated_mappings  # T·ªïng s·ªë s·∫£n ph·∫©m th·ª±c s·ª± ƒë∆∞·ª£c t√¨m th·∫•y
    
    logger.info(f"PRODUCT_FIND_TOOL: üéØ HO√ÄN TH√ÄNH X·ª¨ L√ù")
    logger.info(f"  üìä T·ªïng nguy√™n li·ªáu: {total_processed}")
    logger.info(f"  ‚úÖ Mapping th√†nh c√¥ng: {successful_mappings}")
    logger.info(f"  üîÑ T√¨m th·∫•y nh∆∞ng tr√πng l·∫∑p: {duplicated_mappings}")
    logger.info(f"  ‚ùå Kh√¥ng t√¨m th·∫•y: {len(ingredients_not_found)}")
    logger.info(f"  üí• L·ªói x·ª≠ l√Ω: {error_mappings}")
    logger.info(f"  üéØ T·ªïng s·∫£n ph·∫©m t√¨m th·∫•y: {actual_products_found}")
    logger.info(f"  üìà T·ª∑ l·ªá t√¨m th·∫•y s·∫£n ph·∫©m: {actual_products_found/total_processed*100:.1f}%")
    logger.info(f"  üìà T·ª∑ l·ªá mapping th√†nh c√¥ng: {successful_mappings/total_processed*100:.1f}%")
    logger.info(f"  üèÉ Concurrency: {MAX_CONCURRENT_GEMINI_PRODUCT_CALLS} API keys")
    logger.info(f"  üì¶ Batch size: {PRODUCTS_PER_GEMINI_BATCH} s·∫£n ph·∫©m/l√¥")
    
    # Validation check - ƒë·∫£m b·∫£o s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ = s·ªë nguy√™n li·ªáu ƒë·∫ßu v√†o
    assert len(final_results_for_frontend) == total_processed, f"CRITICAL: S·ªë l∆∞·ª£ng results ({len(final_results_for_frontend)}) != s·ªë nguy√™n li·ªáu ({total_processed})"
    assert (successful_mappings + duplicated_mappings + len(ingredients_not_found) + error_mappings) == total_processed, "CRITICAL: T·ªïng c√°c lo·∫°i k·∫øt qu·∫£ kh√¥ng b·∫±ng t·ªïng nguy√™n li·ªáu"

    # L·∫•y danh s√°ch unique product IDs ƒë√£ ƒë∆∞·ª£c map th√†nh c√¥ng
    unique_product_ids = list(used_product_ids)
    
    return {
        "dish_name_identified": dish_name,
        "processed_request": user_request_text,
        "ingredient_mapping_results": final_results_for_frontend,
        "ingredients_not_found_product_id": list(set(ingredients_not_found)),  # ƒê·∫£m b·∫£o unique
        "summary": {
            "total_ingredients_requested": total_processed,
            "successful_mappings": successful_mappings,
            "duplicated_mappings": duplicated_mappings,
            "not_found": len(ingredients_not_found),
            "errors": error_mappings,
            "unique_products_found": len(unique_product_ids),
            "unique_product_ids": unique_product_ids,
            "success_rate": round(successful_mappings/total_processed*100, 1),
            "product_discovery_rate": round(actual_products_found/total_processed*100, 1)
        }
    }


async def main_test_product_async():
    """Test function for async product finding."""
    # Mock CacheService cho testing
    class MockCacheService:
        _products = []  # B·∫Øt ƒë·∫ßu v·ªõi cache tr·ªëng ƒë·ªÉ test auto-loading
        _mock_db_products = [
            {"product_id": "100", "name": "G·∫°o th∆°m Jasmine t√∫i 5kg"},
            {"product_id": "101", "name": "G·∫°o n·∫øp c√°i hoa v√†ng"},
            {"product_id": "200", "name": "Gi√≤ heo r√∫t x∆∞∆°ng CP"},
            {"product_id": "201", "name": "Th·ªãt ba ch·ªâ heo MeatDeli"},
            {"product_id": "300", "name": "C√° l√≥c phi l√™ l√†m s·∫°ch"},
            {"product_id": "301", "name": "C√° di√™u h·ªìng t∆∞∆°i"},
            {"product_id": "1073", "name": "N·∫•m ƒë√¥ng c√¥ t∆∞∆°i VietGap 150g"},
            {"product_id": "1074", "name": "Th√πng 30 g√≥i m√¨ G·∫•u ƒê·ªè rau n·∫•m 62g"}, # S·∫£n ph·∫©m g√¢y nhi·ªÖu
            {"product_id": "1075", "name": "N·∫•m ƒë√πi g√†"},
            {"product_id": "1076", "name": "N·∫•m kim ch√¢m"},
        ]
        
        @classmethod
        def get_all_products_list(cls):
            logger.info(f"MOCK_CACHE: get_all_products_list called. Current cache size: {len(cls._products)}")
            return cls._products if cls._products else None
        
        @classmethod
        def cache_all_products_list(cls, products, ttl=None):
            logger.info(f"MOCK_CACHE: cache_all_products_list called with {len(products)} products.")
            cls._products = products
            return True

    # Mock refresh_products_cache_task
    async def mock_refresh_products_cache_task():
        logger.info("MOCK_REFRESH: Simulating database fetch...")
        # Simulate cache refresh b·∫±ng c√°ch load mock data
        MockCacheService.cache_all_products_list(MockCacheService._mock_db_products)
        return {
            "success": True,
            "products_count": len(MockCacheService._mock_db_products),
            "message": "Mock cache refresh th√†nh c√¥ng"
        }
    
    # Mock CacheService methods
    original_get_method = CacheService.get_all_products_list
    original_cache_method = CacheService.cache_all_products_list
    CacheService.get_all_products_list = MockCacheService.get_all_products_list
    CacheService.cache_all_products_list = MockCacheService.cache_all_products_list
    
    # Mock refresh function
    import app.services.background_products_cache as bg_cache_module
    original_refresh_task = bg_cache_module.refresh_products_cache_task
    bg_cache_module.refresh_products_cache_task = mock_refresh_products_cache_task
    
    try:
        # Test cache availability function
        print("\n=== TEST CACHE AVAILABILITY ===")
        cache_test = await ensure_products_cache_available()
        print(f"Cache test result: {len(cache_test)} s·∫£n ph·∫©m")

        # Test case 1: Y√™u c·∫ßu nhi·ªÅu nguy√™n li·ªáu ƒë·ªÉ test concurrency
        user_input_text_1 = "T√¥i mu·ªën n·∫•u Ch√°o c√° gi√≤ heo, t√¥i c·∫ßn mua G·∫°o th∆°m, G·∫°o n·∫øp, Gi√≤ heo r√∫t x∆∞∆°ng, N·∫°c c√° l√≥c, N·∫•m ƒë√¥ng c√¥, N·∫•m ƒë√πi g√†."
        print(f"\nüß™ PERFORMANCE TEST - X·ª≠ l√Ω y√™u c·∫ßu: '{user_input_text_1}'")
        print(f"üìä Config: MAX_CONCURRENT={MAX_CONCURRENT_GEMINI_PRODUCT_CALLS}, BATCH_SIZE={PRODUCTS_PER_GEMINI_BATCH}")
        start_time_1 = asyncio.get_event_loop().time()
        result_1 = await process_user_request_async(user_input_text_1)
        end_time_1 = asyncio.get_event_loop().time()
        print(f"\n‚ö° Th·ªùi gian x·ª≠ l√Ω: {end_time_1 - start_time_1:.2f} gi√¢y")
        print("\n--- K·∫æT QU·∫¢ X·ª¨ L√ù HI·ªÜU SU·∫§T ---")
        print(json.dumps(result_1, indent=2, ensure_ascii=False))

        # Test case 2: Nguy√™n li·ªáu c√≥ th·ªÉ g√¢y nh·∫ßm l·∫´n
        user_input_text_2 = "T√¥i c·∫ßn n·∫•m ƒë√¥ng c√¥ v√† n·∫•m ƒë√πi g√†."
        print(f"\nƒêang x·ª≠ l√Ω y√™u c·∫ßu 2: '{user_input_text_2}'")
        start_time_2 = asyncio.get_event_loop().time()
        result_2 = await process_user_request_async(user_input_text_2)
        end_time_2 = asyncio.get_event_loop().time()
        print(f"\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω 2: {end_time_2 - start_time_2:.2f} gi√¢y")
        print("\n--- K·∫æT QU·∫¢ X·ª¨ L√ù Y√äU C·∫¶U 2 ---")
        print(json.dumps(result_2, indent=2, ensure_ascii=False))
        
        # Test case 3: Nguy√™n li·ªáu kh√¥ng c√≥ trong cache
        user_input_text_3 = "T√¥i c·∫ßn y·∫øn s√†o Kh√°nh H√≤a."
        print(f"\nƒêang x·ª≠ l√Ω y√™u c·∫ßu 3: '{user_input_text_3}'")
        start_time_3 = asyncio.get_event_loop().time()
        result_3 = await process_user_request_async(user_input_text_3)
        end_time_3 = asyncio.get_event_loop().time()
        print(f"\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω 3: {end_time_3 - start_time_3:.2f} gi√¢y")
        print("\n--- K·∫æT QU·∫¢ X·ª¨ L√ù Y√äU C·∫¶U 3 ---")
        print(json.dumps(result_3, indent=2, ensure_ascii=False))

    except ValueError as ve: 
        print(f"L·ªói c·∫•u h√¨nh: {ve}")
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng mong mu·ªën ·ªü main (product_find_tool): {str(e)}", exc_info=True)
    finally:
        # Kh√¥i ph·ª•c t·∫•t c·∫£ mock functions
        CacheService.get_all_products_list = original_get_method
        CacheService.cache_all_products_list = original_cache_method
        bg_cache_module.refresh_products_cache_task = original_refresh_task


if __name__ == "__main__":
    asyncio.run(main_test_product_async())