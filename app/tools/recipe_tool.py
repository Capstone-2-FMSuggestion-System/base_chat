import os
import logging
import json
import pinecone
import asyncio
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import math
import re
from typing import Tuple, Optional

# ‚≠ê IMPORT API KEY MANAGER ƒë·ªÉ xoay v√≤ng Gemini API keys
from app.services.api_key_manager import get_api_key_manager

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

PINECONE_API_KEY = os.getenv("RECIPE_DB_PINECONE_API_KEY")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

# ‚≠ê KH·ªûI T·∫†O API KEY MANAGER
api_key_manager = get_api_key_manager()

PINECONE_INDEX_NAME = "recipe-index"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
TEXT_KEY_IN_PINECONE = "text"

# ‚≠ê T·ªêI ∆ØU H√ìA: Gi·∫£m m·∫°nh batch size ƒë·ªÉ tr√°nh l·ªói token limit
DOCUMENTS_PER_GEMINI_CALL = 120  # Gi·∫£m t·ª´ 500 xu·ªëng 120 ƒë·ªÉ an to√†n h∆°n
TOTAL_DOCS_IN_PINECONE = 2351

# ‚≠ê Dynamic batching constants - ∆∞u ti√™n character count
MAX_CHAR_PER_BATCH = 350000  # ~116k tokens (gi·∫£ s·ª≠ 3 chars = 1 token), an to√†n cho gemini-1.5-flash
MAX_SAFE_BATCH_SIZE = 100     # Fallback limit n·∫øu dynamic batching fails
MIN_BATCH_SIZE = 20          # Minimum documents per batch

# ‚≠ê CONCURRENCY CONTROL: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng Gemini API calls ƒë·ªìng th·ªùi
MAX_CONCURRENT_GEMINI_CALLS = 7  # B·∫Øt ƒë·∫ßu v·ªõi 4 calls ƒë·ªìng th·ªùi ƒë·ªÉ an to√†n

# C·ªù ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác l√†m s·∫°ch vƒÉn b·∫£n
SANITIZE_INPUT_TEXT = True

# Ki·ªÉm tra API keys
if not PINECONE_API_KEY:
    logger.error("PINECONE_API_KEY kh√¥ng t√¨m th·∫•y trong file .env")
    raise ValueError("PINECONE_API_KEY kh√¥ng t√¨m th·∫•y trong file .env")

# ‚≠ê KI·ªÇM TRA API KEY MANAGER
if not api_key_manager.is_healthy():
    logger.error("‚ùå KH√îNG C√ì GEMINI API KEY N√ÄO ƒê∆Ø·ª¢C C·∫§U H√åNH! Recipe tool s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
    logger.error("Vui l√≤ng c·∫•u h√¨nh √≠t nh·∫•t m·ªôt API key trong .env file")
else:
    logger.info(f"‚úÖ ApiKeyManager ready v·ªõi {api_key_manager.total_keys()} API keys")

def sanitize_text_for_llm(text: str) -> str:
    """L√†m s·∫°ch vƒÉn b·∫£n c·∫£i thi·ªán ƒë·ªÉ lo·∫°i b·ªè c√°c k√Ω t·ª± c√≥ th·ªÉ g√¢y l·ªói JSON."""
    if not text:
        return ""
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒëi·ªÅu khi·ªÉn C0 (00-1F) v√† DEL (7F), ngo·∫°i tr·ª´ TAB, LF, CR
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # Thay th·∫ø d·∫•u ngo·∫∑c k√©p th√¥ng minh b·∫±ng d·∫•u ngo·∫∑c k√©p chu·∫©n
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace("'", "'").replace("'", "'")
    
    # Lo·∫°i b·ªè c√°c escape sequence kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'\\[rn]', ' ', text)
    
    # C·∫Øt b·ªõt n·∫øu qu√° d√†i (m·ªói recipe kh√¥ng n√™n v∆∞·ª£t qu√° 2000 chars)
    if len(text) > 2000:
        text = text[:1997] + "..."
        
    return text

def estimate_tokens(text: str) -> int:
    """∆Ø·ªõc t√≠nh s·ªë token d·ª±a tr√™n character count (3 chars ‚âà 1 token)"""
    return len(text) // 3

def create_dynamic_batches(matches: list) -> list:
    """
    ‚≠ê DYNAMIC BATCHING: T·∫°o batches d·ª±a tr√™n character count thay v√¨ s·ªë l∆∞·ª£ng c·ªë ƒë·ªãnh
    """
    batches = []
    current_batch = []
    current_batch_chars = 0
    
    for match in matches:
        if TEXT_KEY_IN_PINECONE not in match.metadata:
            continue
            
        page_content = match.metadata[TEXT_KEY_IN_PINECONE]
        if SANITIZE_INPUT_TEXT:
            page_content = sanitize_text_for_llm(page_content)
            
        if not page_content:
            continue
            
        # ∆Ø·ªõc t√≠nh character count cho document n√†y (bao g·ªìm wrapper)
        doc_char_count = len(f"--- C√îNG TH·ª®C ID: {match.id} ---\n{page_content}\n--- K·∫æT TH√öC C√îNG TH·ª®C ID: {match.id} ---\n\n")
        
        # Ki·ªÉm tra xem c√≥ th·ªÉ th√™m v√†o batch hi·ªán t·∫°i kh√¥ng
        if (current_batch_chars + doc_char_count > MAX_CHAR_PER_BATCH and current_batch) or len(current_batch) >= MAX_SAFE_BATCH_SIZE:
            # Batch hi·ªán t·∫°i ƒë√£ ƒë·ªß, l∆∞u v√† t·∫°o batch m·ªõi
            if current_batch:  # Ch·ªâ l∆∞u n·∫øu c√≥ content
                batches.append(current_batch)
            current_batch = [match]
            current_batch_chars = doc_char_count
        else:
            # Th√™m v√†o batch hi·ªán t·∫°i
            current_batch.append(match)
            current_batch_chars += doc_char_count
    
    # Th√™m batch cu·ªëi c√πng n·∫øu c√≥
    if current_batch:
        batches.append(current_batch)
    
    return batches

def init_connections() -> Tuple[Optional[pinecone.Index], Optional[HuggingFaceEmbeddings]]:
    """Kh·ªüi t·∫°o k·∫øt n·ªëi v·ªõi Pinecone client v√† embedding model. Gemini s·∫Ω ƒë∆∞·ª£c config trong m·ªói API call."""
    try:
        from pinecone import Pinecone
        pc = Pinecone(api_key=PINECONE_API_KEY)
        
        try:
            index_info = pc.describe_index(PINECONE_INDEX_NAME)
            logger.info(f"Pinecone index '{PINECONE_INDEX_NAME}' ƒë√£ t·ªìn t·∫°i.")
        except Exception as e:
            logger.error(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i: {str(e)}")
            raise ValueError(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i.") from e

        index = pc.Index(PINECONE_INDEX_NAME)
        logger.info(f"ƒê√£ t·∫°o ƒë·ªëi t∆∞·ª£ng Index cho: {PINECONE_INDEX_NAME}")

        logger.info(f"ƒêang t·∫£i m√¥ h√¨nh embedding t·ª´ {EMBEDDING_MODEL_NAME}...")
        embeddings_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("ƒê√£ t·∫£i th√†nh c√¥ng m√¥ h√¨nh embedding.")

        # ‚≠ê KI·ªÇM TRA API KEY MANAGER READINESS
        if api_key_manager.total_keys() == 0:
            logger.error("‚ùå Kh√¥ng c√≥ API key n√†o c·ªßa Gemini ƒë∆∞·ª£c c·∫•u h√¨nh trong ApiKeyManager. Recipe tool c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.")
            # C√≥ th·ªÉ raise exception n·∫øu mu·ªën d·ª´ng h·∫≥n
        else:
            logger.info(f"‚úÖ Gemini integration ready - ApiKeyManager c√≥ {api_key_manager.total_keys()} keys")

        return index, embeddings_model
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o k·∫øt n·ªëi: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def clean_json_response(response_text: str) -> str:
    """‚≠ê C·∫¢I THI·ªÜN: L√†m s·∫°ch response t·ª´ Gemini tr∆∞·ªõc khi parse JSON v·ªõi nhi·ªÅu fix patterns h∆°n"""
    try:
        cleaned = response_text.strip()
        
        # Lo·∫°i b·ªè markdown wrapper
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
            
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
            
        cleaned = cleaned.strip()
        
        # ‚≠ê C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu pattern fix JSON ph·ªï bi·∫øn h∆°n
        
        # 1. Trailing comma tr∆∞·ªõc } ho·∫∑c ]
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
        
        # 2. Missing quotes cho keys (ch·ªâ fix n·∫øu ch∆∞a c√≥ quotes)
        cleaned = re.sub(r'(\w+):\s*(["\[])', r'"\1": \2', cleaned)
        
        # 3. Single quotes thay v√¨ double quotes cho keys v√† strings
        cleaned = re.sub(r"'([^']*?)'(\s*:\s*)", r'"\1"\2', cleaned)  # Keys
        cleaned = re.sub(r':\s*\'([^\']*?)\'', r': "\1"', cleaned)    # String values
        
        # 4. Th√™m d·∫•u ph·∫©y thi·∫øu gi·ªØa objects trong array
        cleaned = re.sub(r'}\s*{', r'}, {', cleaned)
        
        # 5. Lo·∫°i b·ªè control characters c√≥ th·ªÉ g√¢y l·ªói JSON
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', cleaned)
        
        # 6. Fix escaped quotes th·ª´a
        cleaned = re.sub(r'\\"', '"', cleaned)
        
        # 7. ƒê·∫£m b·∫£o JSON b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c ƒë√∫ng format
        if not cleaned.startswith('[') and not cleaned.startswith('{'):
            # T√¨m JSON array ho·∫∑c object ƒë·∫ßu ti√™n
            array_match = re.search(r'(\[[\s\S]*\])', cleaned)
            object_match = re.search(r'(\{[\s\S]*\})', cleaned)
            
            if array_match:
                cleaned = array_match.group(1)
            elif object_match:
                cleaned = object_match.group(1)
        
        return cleaned
    except Exception as e:
        logger.warning(f"L·ªói khi clean JSON response: {e}")
        return response_text.strip()

def parse_json_with_fallback(response_text: str) -> dict:
    """‚≠ê M·ªöI: Parse JSON v·ªõi multiple fallback strategies"""
    try:
        cleaned = clean_json_response(response_text)
        
        # Th·ª≠ parse tr·ª±c ti·∫øp
        try:
            result = json.loads(cleaned)
            return {"success": True, "data": result}
        except json.JSONDecodeError as e:
            logger.warning(f"Direct JSON parse failed: {e}")
        
        # ‚≠ê Fallback 1: T√¨m v√† extract JSON object/array t·ª´ text
        json_patterns = [
            r'\[[\s\S]*?\]',  # JSON array
            r'\{[\s\S]*?\}',  # JSON object
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, cleaned)
            for match in matches:
                try:
                    result = json.loads(match)
                    logger.info("‚úÖ JSON parsed successfully v·ªõi pattern matching")
                    return {"success": True, "data": result}
                except json.JSONDecodeError:
                    continue
        
        # ‚≠ê Fallback 2: N·∫øu l√† object nh∆∞ng mong ƒë·ª£i array, extract array t·ª´ object
        try:
            obj_result = json.loads(cleaned)
            if isinstance(obj_result, dict):
                # T√¨m key ch·ª©a array
                for key, value in obj_result.items():
                    if isinstance(value, list):
                        logger.info(f"‚úÖ Extracted array t·ª´ object key: {key}")
                        return {"success": True, "data": value}
        except json.JSONDecodeError:
            pass
        
        # ‚≠ê Fallback 3: Return structured error
        return {
            "success": False, 
            "error": "Invalid JSON from Gemini", 
            "raw_response": response_text[:500],  # Gi·ªõi h·∫°n ƒë·ªô d√†i
            "cleaned_response": cleaned[:500]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"JSON parsing exception: {str(e)}",
            "raw_response": response_text[:500]
        }

async def call_gemini_api_async(prompt: str, max_retries: int = 3) -> str:
    """‚≠ê ASYNC VERSION v·ªõi API KEY ROTATION v√† c·∫£i thi·ªán prompt engineering cho JSON"""
    prompt_char_count = len(prompt)
    estimated_tokens = estimate_tokens(prompt)
    
    # ‚≠ê L·∫§Y API KEY T·ª™ KEY MANAGER
    current_api_key = api_key_manager.get_next_key()
    if current_api_key is None:
        logger.error("‚ùå Kh√¥ng c√≥ API key Gemini kh·∫£ d·ª•ng t·ª´ ApiKeyManager")
        return json.dumps({"error": "Kh√¥ng c√≥ API key Gemini kh·∫£ d·ª•ng."})
    
    # Log key rotation (an to√†n)
    masked_key = f"{current_api_key[:8]}..." if len(current_api_key) > 8 else "***"
    logger.info(f"üîç G·ªçi Gemini ASYNC v·ªõi key {masked_key} - Chars: {prompt_char_count:,}, Est. tokens: {estimated_tokens:,}")
    
    loop = asyncio.get_running_loop()
    
    for attempt in range(max_retries):
        try:
            # ‚≠ê C·∫§U H√åNH GEMINI V·ªöI KEY HI·ªÜN T·∫†I V√Ä T·∫†O MODEL M·ªöI
            genai.configure(api_key=current_api_key)
            temp_gemini_model = genai.GenerativeModel(GEMINI_MODEL_NAME)
            
            # ‚≠ê CH·∫†Y GEMINI TRONG EXECUTOR ƒë·ªÉ kh√¥ng block event loop
            response = await loop.run_in_executor(
                None,
                lambda: temp_gemini_model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=8192,
                        temperature=0.1
                    )
                )
            )
            
            if response.parts:
                response_text = response.text
                
                # ‚≠ê S·ª¨ D·ª§NG PARSE_JSON_WITH_FALLBACK
                parse_result = parse_json_with_fallback(response_text)
                
                if parse_result["success"]:
                    # Tr·∫£ v·ªÅ JSON string c·ªßa data ƒë√£ parse th√†nh c√¥ng
                    return json.dumps(parse_result["data"], ensure_ascii=False)
                else:
                    # Tr·∫£ v·ªÅ structured error
                    logger.error(f"‚ùå JSON parse failed: {parse_result.get('error')}")
                    if attempt < max_retries - 1:
                        logger.info(f"üîÑ Retry attempt {attempt + 1}/{max_retries} due to JSON parse error")
                        await asyncio.sleep(5)
                        continue
                    return json.dumps({
                        "error": parse_result["error"],
                        "raw_snippet": parse_result.get("raw_response", "")[:200]
                    })
            else:
                logger.warning("Gemini kh√¥ng tr·∫£ v·ªÅ n·ªôi dung.")
                logger.warning(f"Gemini response: {response}")
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    logger.warning(f"B·ªã ch·∫∑n b·ªüi: {response.prompt_feedback.block_reason}")
                return "[]"
                
        except Exception as e:
            error_str = str(e).lower()
            
            if "429" in error_str or "resource_exhausted" in error_str or "quota" in error_str:
                retry_delay = 15 + attempt * 10
                delay_match = re.search(r'retry in (\d+)s', error_str)
                if delay_match: 
                    retry_delay = max(int(delay_match.group(1)), retry_delay)
                logger.warning(f"üö´ Quota limit Gemini, retry sau {retry_delay}s (#{attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    
            elif "invalid_argument" in error_str and ("token" in error_str or "size" in error_str):
                logger.error(f"üí• Prompt qu√° l·ªõn: {estimated_tokens:,} tokens, {prompt_char_count:,} chars")
                return json.dumps({"error_too_large": True, "estimated_tokens": estimated_tokens})
                
            elif "500" in error_str or "internal" in error_str or "unavailable" in error_str:
                retry_delay = 10 + attempt * 15
                logger.warning(f"üîß Server error Gemini, retry sau {retry_delay}s (#{attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
            else:
                logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh Gemini (#{attempt+1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    return json.dumps({"error": f"Gemini API error: {str(e)}"})
                await asyncio.sleep(10)
                
    logger.error(f"üí• Th·∫•t b·∫°i ho√†n to√†n sau {max_retries} l·∫ßn th·ª≠ v·ªõi Gemini")
    return json.dumps({"error": "Kh√¥ng th·ªÉ k·∫øt n·ªëi Gemini sau nhi·ªÅu l·∫ßn th·ª≠"})

async def search_and_filter_recipes_async(user_query: str) -> str:
    """
    ‚≠ê ASYNC VERSION: Search v·ªõi dynamic batching v√† x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô ƒë·ªìng th·ªùi
    """
    try:
        # ‚≠ê CH·∫†Y INIT_CONNECTIONS TRONG EXECUTOR
        loop = asyncio.get_running_loop()
        pinecone_index, embeddings_model = await loop.run_in_executor(None, init_connections)
    except Exception as e:
        logger.error(f"üí• Kh√¥ng th·ªÉ kh·ªüi t·∫°o k·∫øt n·ªëi: {str(e)}")
        return json.dumps({"error": f"Connection init failed: {str(e)}"})

    all_selected_recipes = []
    error_reports = []

    try:
        logger.info(f"üîç ƒêang embed query: '{user_query}'")
        
        # ‚≠ê CH·∫†Y EMBEDDING TRONG EXECUTOR
        query_vector = await loop.run_in_executor(None, embeddings_model.embed_query, user_query)
        logger.info("‚úÖ ƒê√£ embed query th√†nh c√¥ng")

        # ‚≠ê GI·∫¢M PINECONE TOP_K ƒë·ªÉ gi·∫£m t·∫£i
        actual_top_k = min(TOTAL_DOCS_IN_PINECONE, 3000)  # Gi·∫£m t·ª´ 10000 xu·ªëng 3000
        logger.info(f"üîç Query Pinecone v·ªõi top_k={actual_top_k}")
        
        # ‚≠ê CH·∫†Y PINECONE QUERY TRONG EXECUTOR
        query_response = await loop.run_in_executor(
            None,
            lambda: pinecone_index.query(
                vector=query_vector,
                top_k=actual_top_k,
                include_metadata=True,
            )
        )
        
        retrieved_matches = query_response.matches
        logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(retrieved_matches)} documents t·ª´ Pinecone")

        if not retrieved_matches:
            return json.dumps([])

        # ‚≠ê S·ª¨ D·ª§NG DYNAMIC BATCHING
        dynamic_batches = create_dynamic_batches(retrieved_matches)
        logger.info(f"üîÑ Chia th√†nh {len(dynamic_batches)} dynamic batches")

        # ‚≠ê KH·ªûI T·∫†O SEMAPHORE ƒê·ªÇ KI·ªÇM SO√ÅT CONCURRENCY
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_GEMINI_CALLS)
        logger.info(f"üö¶ Kh·ªüi t·∫°o semaphore v·ªõi gi·ªõi h·∫°n {MAX_CONCURRENT_GEMINI_CALLS} concurrent calls")
        
        # ‚≠ê H√ÄM WRAPPER V·ªöI SEMAPHORE CONTROL
        async def process_batch_with_semaphore(batch_prompt: str, batch_num_for_log: int) -> str:
            async with semaphore:
                logger.info(f"üö¶ Batch {batch_num_for_log}: B·∫Øt ƒë·∫ßu x·ª≠ l√Ω (semaphore acquired, {MAX_CONCURRENT_GEMINI_CALLS - semaphore._value} slots used)")
                try:
                    result = await call_gemini_api_async(batch_prompt)
                    # ‚≠ê T√ôY CH·ªåN: Th√™m delay nh·ªè ƒë·ªÉ gi·∫£m t·∫£i API th√™m n·ªØa
                    await asyncio.sleep(0.1)  # 0.1 gi√¢y delay gi·ªØa c√°c calls
                    logger.info(f"üö¶ Batch {batch_num_for_log}: Ho√†n th√†nh x·ª≠ l√Ω (semaphore released)")
                    return result
                except Exception as e:
                    logger.error(f"üö¶ Batch {batch_num_for_log}: L·ªói trong semaphore block - {str(e)}")
                    return json.dumps({"error": f"Batch {batch_num_for_log} failed: {str(e)}"})

        # ‚≠ê T·∫†O TASKS CHO T·∫§T C·∫¢ BATCHES ƒê·ªÇ X·ª¨ L√ù ƒê·ªíNG TH·ªúI V·ªöI SEMAPHORE
        gemini_tasks = []
        batch_prompts = []
        
        for batch_idx, batch_matches in enumerate(dynamic_batches):
            logger.info(f"üì¶ Chu·∫©n b·ªã batch {batch_idx + 1}/{len(dynamic_batches)} ({len(batch_matches)} docs)")
            
            context_parts = []
            
            for match in batch_matches:
                doc_id = match.id
                if TEXT_KEY_IN_PINECONE in match.metadata:
                    page_content = match.metadata[TEXT_KEY_IN_PINECONE]
                    if SANITIZE_INPUT_TEXT:
                        page_content = sanitize_text_for_llm(page_content)
                    
                    if page_content:
                        context_parts.append(f"--- C√îNG TH·ª®C ID: {doc_id} ---\n{page_content}\n--- K·∫æT TH√öC C√îNG TH·ª®C ID: {doc_id} ---")

            if not context_parts:
                logger.info(f"‚è≠Ô∏è Batch {batch_idx + 1} r·ªóng, b·ªè qua")
                continue

            batch_context = "\n\n".join(context_parts)
            estimated_tokens = estimate_tokens(batch_context)
            
            logger.info(f"üìä Batch {batch_idx + 1}: {len(batch_context):,} chars, ~{estimated_tokens:,} tokens")
            
            # Ki·ªÉm tra an to√†n token
            if estimated_tokens > 300000:  # 300k tokens limit cho an to√†n
                logger.error(f"‚ö†Ô∏è Batch {batch_idx + 1} qu√° l·ªõn ({estimated_tokens:,} tokens), b·ªè qua")
                error_reports.append({
                    "error_batch": batch_idx + 1,
                    "message": f"Batch qu√° l·ªõn: {estimated_tokens:,} tokens",
                    "doc_count": len(batch_matches)
                })
                continue

            # ‚≠ê PROMPT T·ªêI ∆ØU H√ìA CHO JSON - C·∫¢I THI·ªÜN THEO Y√äU C·∫¶U
            optimized_prompt = f'''B·∫°n l√† AI chuy√™n gia ·∫©m th·ª±c. Ph√¢n t√≠ch query "{user_query}" v√† ch·ªçn c√°c c√¥ng th·ª©c ph√π h·ª£p NH·∫§T.

‚ö†Ô∏è TUY·ªÜT ƒê·ªêI CH·ªà TR·∫¢ V·ªÄ M·ªòT DANH S√ÅCH JSON (JSON ARRAY) H·ª¢P L·ªÜ. KH√îNG TH√äM b·∫•t k·ª≥ vƒÉn b·∫£n n√†o tr∆∞·ªõc ho·∫∑c sau danh s√°ch JSON.

üìã Y√äU C·∫¶U CH√çNH X√ÅC:
1. Ch·ªâ ch·ªçn c√¥ng th·ª©c th·ª±c s·ª± li√™n quan ƒë·∫øn query
2. Tr√≠ch xu·∫•t: id, name, url, ingredients_summary  
3. ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c chu·ªói trong JSON ƒë∆∞·ª£c ƒë·∫∑t trong d·∫•u ngo·∫∑c k√©p chu·∫©n (\")
4. ƒê·∫£m b·∫£o kh√¥ng c√≥ d·∫•u ph·∫©y th·ª´a ·ªü cu·ªëi danh s√°ch ho·∫∑c cu·ªëi ƒë·ªëi t∆∞·ª£ng
5. N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ n√†o, tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON r·ªóng: []

‚úÖ V√ç D·ª§ V·ªÄ OUTPUT JSON H·ª¢P L·ªÜ:
[
  {{
    "id": "recipe_123", 
    "name": "Salad gi·∫£m c√¢n v·ªõi rau xanh",
    "url": "https://example.com/recipe_123",
    "ingredients_summary": "Rau xanh, c√† chua, d∆∞a chu·ªôt, d·∫ßu oliu"
  }},
  {{
    "id": "recipe_456",
    "name": "Sinh t·ªë rau c·ªß √≠t calo", 
    "url": null,
    "ingredients_summary": "C·∫£i b√≥ x√¥i, chu·ªëi, t√°o, n∆∞·ªõc"
  }}
]

D·ªÆLI·ªÜU C√îNG TH·ª®C:
{batch_context}

üî• CH·ªà TR·∫¢ V·ªÄ JSON ARRAY - KH√îNG GI·∫¢I TH√çCH, KH√îNG MARKDOWN, KH√îNG VƒÇN B·∫¢N TH√äM:'''

            # ‚≠ê TH√äM TASK V√ÄO DANH S√ÅCH ƒê·ªÇ X·ª¨ L√ù ƒê·ªíNG TH·ªúI V·ªöI SEMAPHORE CONTROL
            if optimized_prompt.strip():
                batch_number_for_logging = batch_idx + 1
                gemini_tasks.append(process_batch_with_semaphore(optimized_prompt, batch_number_for_logging))
                batch_prompts.append((batch_number_for_logging, len(batch_matches)))  # Track batch info

        # ‚≠ê X·ª¨ L√ù T·∫§T C·∫¢ BATCHES ƒê·ªíNG TH·ªúI v·ªõi asyncio.gather V√Ä SEMAPHORE CONTROL
        if gemini_tasks:
            logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(gemini_tasks)} batches v·ªõi SEMAPHORE (max {MAX_CONCURRENT_GEMINI_CALLS} concurrent)...")
            
            # G·ªçi t·∫•t c·∫£ tasks ƒë·ªìng th·ªùi v·ªõi return_exceptions=True
            # Semaphore s·∫Ω t·ª± ƒë·ªông ki·ªÉm so√°t s·ªë l∆∞·ª£ng calls th·ª±c s·ª± ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªìng th·ªùi
            gemini_responses_or_exceptions = await asyncio.gather(*gemini_tasks, return_exceptions=True)
            
            # ‚≠ê X·ª¨ L√ù K·∫æT QU·∫¢ T·ª™ asyncio.gather
            for task_idx, (result, batch_info) in enumerate(zip(gemini_responses_or_exceptions, batch_prompts)):
                batch_num, doc_count = batch_info
                
                # Ki·ªÉm tra xem result c√≥ ph·∫£i l√† Exception kh√¥ng
                if isinstance(result, Exception):
                    logger.error(f"üí• Batch {batch_num} failed v·ªõi exception: {str(result)}")
                    error_reports.append({
                        "error_batch": batch_num,
                        "message": f"Task exception: {str(result)}",
                        "doc_count": doc_count
                    })
                    continue
                
                # result l√† gemini_response_text th√†nh c√¥ng
                gemini_response = result
                
                # ‚≠ê X·ª¨ L√ù RESPONSE C·∫¢I THI·ªÜN V·ªöI ERROR_TOO_LARGE HANDLING
                try:
                    # Ki·ªÉm tra error_too_large t·ª´ call_gemini_api_async tr∆∞·ªõc
                    if gemini_response.startswith('{"error_too_large":'):
                        error_data = json.loads(gemini_response)
                        logger.error(f"üí• Batch {batch_num} qu√° l·ªõn cho Gemini: {error_data.get('estimated_tokens', 'unknown')} tokens")
                        logger.warning(f"‚ö†Ô∏è C·∫ßn gi·∫£m MAX_CHAR_PER_BATCH hi·ªán t·∫°i ({MAX_CHAR_PER_BATCH:,}) ho·∫∑c DOCUMENTS_PER_GEMINI_CALL hi·ªán t·∫°i ({DOCUMENTS_PER_GEMINI_CALL})")
                        error_reports.append({
                            "error_batch": batch_num,
                            "error_type": "batch_too_large",
                            "message": f"Batch qu√° l·ªõn: {error_data.get('estimated_tokens', 'unknown')} tokens",
                            "doc_count": doc_count,
                            "suggestion": "Gi·∫£m MAX_CHAR_PER_BATCH ho·∫∑c batch size"
                        })
                        continue
                    
                    # Ki·ªÉm tra c√°c error responses kh√°c t·ª´ call_gemini_api_async
                    if gemini_response.startswith('{"error":'):
                        error_data = json.loads(gemini_response)
                        error_msg = error_data.get('error', 'Unknown Gemini error')
                        logger.error(f"üí• Batch {batch_num} Gemini error: {error_msg}")
                        error_reports.append({
                            "error_batch": batch_num,
                            "error_type": "gemini_api_error",
                            "message": error_msg,
                            "doc_count": doc_count
                        })
                        continue
                    
                    # Parse JSON v·ªõi fallback handling
                    parse_result = parse_json_with_fallback(gemini_response)
                    
                    if parse_result["success"]:
                        batch_recipes = parse_result["data"]
                        
                        # ‚≠ê X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P GEMINI TR·∫¢ V·ªÄ OBJECT THAY V√å LIST
                        if isinstance(batch_recipes, dict):
                            logger.warning(f"‚ö†Ô∏è Batch {batch_num}: Gemini tr·∫£ v·ªÅ object thay v√¨ array")
                            # Th·ª≠ extract array t·ª´ object
                            extracted_array = None
                            for key, value in batch_recipes.items():
                                if isinstance(value, list):
                                    logger.info(f"‚úÖ Batch {batch_num}: Extracted array t·ª´ key '{key}'")
                                    extracted_array = value
                                    break
                            
                            if extracted_array:
                                batch_recipes = extracted_array
                            else:
                                logger.warning(f"‚ö†Ô∏è Batch {batch_num}: Kh√¥ng t√¨m th·∫•y array trong object, b·ªè qua")
                                error_reports.append({
                                    "error_batch": batch_num,
                                    "error_type": "object_instead_of_array",
                                    "message": "Gemini tr·∫£ v·ªÅ object thay v√¨ array v√† kh√¥ng c√≥ array con",
                                    "doc_count": doc_count
                                })
                                continue
                        
                        if isinstance(batch_recipes, list):
                            if batch_recipes:
                                valid_recipes = [r for r in batch_recipes if isinstance(r, dict) and 'id' in r and 'name' in r]
                                all_selected_recipes.extend(valid_recipes)
                                logger.info(f"‚úÖ Batch {batch_num}: {len(valid_recipes)} valid recipes t·ª´ {len(batch_recipes)} items")
                            else:
                                logger.info(f"üì≠ Batch {batch_num}: Kh√¥ng t√¨m th·∫•y recipe ph√π h·ª£p (empty array)")
                        else:
                            logger.warning(f"‚ö†Ô∏è Batch {batch_num}: Response v·∫´n kh√¥ng ph·∫£i array sau x·ª≠ l√Ω: {type(batch_recipes)}")
                            error_reports.append({
                                "error_batch": batch_num,
                                "error_type": "invalid_response_type",
                                "message": f"Response type kh√¥ng h·ª£p l·ªá: {type(batch_recipes)}",
                                "doc_count": doc_count
                            })
                    else:
                        # Parse failed v·ªõi structured error
                        logger.error(f"üí• Batch {batch_num} JSON parse failed: {parse_result.get('error')}")
                        error_reports.append({
                            "error_batch": batch_num,
                            "error_type": "json_parse_failed",
                            "message": parse_result.get('error', 'JSON parse failed'),
                            "raw_snippet": parse_result.get('raw_response', '')[:200],
                            "doc_count": doc_count
                        })

                except json.JSONDecodeError as e:
                    logger.error(f"üí• Batch {batch_num} Critical JSON decode error: {e}")
                    logger.error(f"Raw response sample: {gemini_response[:300]}...")
                    error_reports.append({
                        "error_batch": batch_num,
                        "error_type": "critical_json_error",
                        "message": f"Critical JSON decode error: {str(e)}",
                        "raw_snippet": gemini_response[:200],
                        "doc_count": doc_count
                    })
                except Exception as e:
                    logger.error(f"üí• Batch {batch_num} Unexpected error: {e}")
                    error_reports.append({
                        "error_batch": batch_num,
                        "error_type": "unexpected_error",
                        "message": f"Unexpected error: {str(e)}",
                        "doc_count": doc_count
                    })
        
        # ‚≠ê TR·∫¢ V·ªÄ K·∫æT QU·∫¢ TH√îNG MINH
        logger.info(f"üéØ FINAL RESULT: {len(all_selected_recipes)} recipes, {len(error_reports)} errors")
        
        if not all_selected_recipes and error_reports:
            return json.dumps({
                "message": "Kh√¥ng t√¨m th·∫•y recipe v√† c√≥ l·ªói x·∫£y ra",
                "errors": error_reports[:3]  # Ch·ªâ report 3 l·ªói ƒë·∫ßu
            }, ensure_ascii=False, indent=2)
        elif not all_selected_recipes:
            return json.dumps([])
        elif error_reports:
            # C√≥ recipes nh∆∞ng c≈©ng c√≥ l·ªói - ch·ªâ tr·∫£ v·ªÅ recipes
            logger.warning(f"‚ö†Ô∏è C√≥ {len(error_reports)} l·ªói nh∆∞ng v·∫´n t√¨m ƒë∆∞·ª£c {len(all_selected_recipes)} recipes")
            return json.dumps(all_selected_recipes, ensure_ascii=False, indent=2)
        else:
            return json.dumps(all_selected_recipes, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"üí• Critical error trong search_and_filter_recipes_async: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return json.dumps({"error": f"System error: {str(e)}"})

# ‚≠ê WRAPPER ƒê·ªíNG B·ªò ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i
def search_and_filter_recipes(user_query: str) -> str:
    """
    Wrapper ƒë·ªìng b·ªô cho search_and_filter_recipes_async ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i
    """
    return asyncio.run(search_and_filter_recipes_async(user_query))

async def main_test_async():
    """‚≠ê ASYNC TEST FUNCTION"""
    try:
        query = "M√≥n ƒÉn cho gi·∫£m c√¢n, √≠t calo, nhi·ªÅu rau xanh"
        
        print(f"\nüîç Testing ASYNC v·ªõi query: '{query}'")
        
        import time
        start_time = time.time()
        
        result = await search_and_filter_recipes_async(query)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\n‚ö° ASYNC PERFORMANCE: {duration:.2f} seconds")
        print("\n‚úÖ K·∫æT QU·∫¢:")
        try:
            parsed = json.loads(result)
            if isinstance(parsed, list):
                print(f"T√¨m ƒë∆∞·ª£c {len(parsed)} recipes")
            else:
                print("Response kh√¥ng ph·∫£i list recipes")
            print(json.dumps(parsed, indent=2, ensure_ascii=False))
        except json.JSONDecodeError:
            print("Raw result:", result)

    except ValueError as ve: 
        print(f"üí• L·ªói c·∫•u h√¨nh: {ve}")
    except Exception as e:
        logger.error(f"üí• L·ªói kh√¥ng mong mu·ªën: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    # ‚≠ê CH·∫†Y ASYNC TEST
    asyncio.run(main_test_async())