import os
import logging
import json
import pinecone
import asyncio
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_huggingface import HuggingFaceEmbeddings
import math
import re
from typing import Tuple, Optional

# ‚≠ê IMPORT API KEY MANAGER ƒë·ªÉ xoay v√≤ng Gemini API keys
from app.services.api_key_manager import get_api_key_manager
# ‚≠ê IMPORT GEMINI MODEL POOL ƒë·ªÉ thread-safe API access
from app.tools.gemini_model_pool import get_gemini_model_pool

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

PINECONE_API_KEY = os.getenv("RECIPE_DB_PINECONE_API_KEY")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

# ‚≠ê KH·ªûI T·∫†O API KEY MANAGER V√Ä GEMINI MODEL POOL
api_key_manager = get_api_key_manager()
gemini_model_pool = get_gemini_model_pool(GEMINI_MODEL_NAME)

PINECONE_INDEX_NAME = "recipe-index"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
TEXT_KEY_IN_PINECONE = "text"

# ‚≠ê T·ªêI ∆ØU H√ìA M·∫†NH: Gi·∫£m ƒë√°ng k·ªÉ batch size ƒë·ªÉ tr√°nh l·ªói token limit
DOCUMENTS_PER_GEMINI_CALL = 80   # Gi·∫£m t·ª´ 120 xu·ªëng 80 ƒë·ªÉ an to√†n h∆°n
TOTAL_DOCS_IN_PINECONE = 2351

# ‚≠ê GI·ªöI H·∫†N K·∫æT QU·∫¢: D·ª´ng khi ƒë·∫°t s·ªë l∆∞·ª£ng c√¥ng th·ª©c mong mu·ªën
MAX_RECIPES_RESULT = 20          # Gi·ªõi h·∫°n t·ªëi ƒëa 20 c√¥ng th·ª©c tr·∫£ v·ªÅ

# ‚≠ê Dynamic batching constants - gi·∫£m m·∫°nh ƒë·ªÉ tr√°nh l·ªói JSON
MAX_CHAR_PER_BATCH = 200000      # Gi·∫£m t·ª´ 350k xu·ªëng 200k (~67k tokens)
MAX_SAFE_BATCH_SIZE = 60         # Gi·∫£m t·ª´ 100 xu·ªëng 60
MIN_BATCH_SIZE = 15              # Gi·∫£m t·ª´ 20 xu·ªëng 15

# ‚≠ê GEMINI OUTPUT TOKEN LIMIT - gi·∫£m ƒë·ªÉ ƒë·∫£m b·∫£o JSON response ·ªïn ƒë·ªãnh
MAX_GEMINI_OUTPUT_TOKENS = 4096  # Gi·∫£m t·ª´ 8192 xu·ªëng 4096

# ‚≠ê WORKER POOL CONFIGURATION: S·ªë l∆∞·ª£ng Gemini Worker ƒë·ªÉ x·ª≠ l√Ω song song
# S·∫Ω ƒë∆∞·ª£c t√≠nh ƒë·ªông d·ª±a tr√™n s·ªë API key c√≥ s·∫µn, t·ªëi ƒëa 7 worker
MAX_GEMINI_WORKERS = 7

# ‚≠ê CONFIGURATION CHO PARALLEL PROCESSING
WORKER_SLEEP_BETWEEN_TASKS = 0.1
WORKER_ERROR_SLEEP = 2
WORKER_TIMEOUT_SECONDS = 120

# ‚≠ê SANITIZATION CONFIGURATION
SANITIZE_INPUT_TEXT = True

# ‚≠ê RETRY CONFIGURATION
MAX_RETRIES_PER_BATCH = 3
RETRY_DELAY_SECONDS = 2

# ‚≠ê LOGGING CONFIGURATION
LOG_BATCH_DETAILS = True
LOG_WORKER_DETAILS = True

# ‚≠ê PERFORMANCE MONITORING
ENABLE_PERFORMANCE_LOGGING = True

def get_embedding_model():
    """‚≠ê L·∫•y embedding model t·ª´ global cache ho·∫∑c t·∫°o m·ªõi n·∫øu c·∫ßn"""
    try:
        # Import function ƒë·ªÉ l·∫•y global embedding model
        from main import get_global_embedding_model
        global_model = get_global_embedding_model()
        
        if global_model is not None:
            logger.info("‚úÖ S·ª≠ d·ª•ng pre-loaded embedding model t·ª´ global cache")
            return global_model
        else:
            logger.warning("‚ö†Ô∏è Global embedding model ch∆∞a ƒë∆∞·ª£c load, t·∫°o m·ªõi...")
            return HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
    except ImportError:
        logger.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ import global embedding model, t·∫°o m·ªõi...")
        return HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

def init_connections() -> Tuple[Optional[pinecone.Index], Optional[HuggingFaceEmbeddings]]:
    """Kh·ªüi t·∫°o k·∫øt n·ªëi v·ªõi Pinecone client v√† embedding model. Gemini s·∫Ω ƒë∆∞·ª£c config trong m·ªói API call."""
    try:
        from pinecone import Pinecone
        pc = Pinecone(api_key=PINECONE_API_KEY)
        
        try:
            index_info = pc.describe_index(PINECONE_INDEX_NAME)
            logger.info(f"Pinecone index '{PINECONE_INDEX_NAME}' ƒë√£ t·ªìn t·∫°i.")
        except Exception as e:
            logger.error(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i: {str(e)}")
            raise ValueError(f"Pinecone index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i.") from e

        index = pc.Index(PINECONE_INDEX_NAME)
        logger.info(f"ƒê√£ t·∫°o ƒë·ªëi t∆∞·ª£ng Index cho: {PINECONE_INDEX_NAME}")

        # ‚≠ê S·ª¨ D·ª§NG GLOBAL EMBEDDING MODEL
        embeddings_model = get_embedding_model()
        if embeddings_model:
            logger.info("‚úÖ ƒê√£ s·ª≠ d·ª•ng embedding model (pre-loaded ho·∫∑c m·ªõi t·∫°o)")
        else:
            logger.error("‚ùå Kh√¥ng th·ªÉ t·∫°o embedding model")
            raise ValueError("Kh√¥ng th·ªÉ t·∫°o embedding model")

        # ‚≠ê KI·ªÇM TRA API KEY MANAGER READINESS
        if api_key_manager.total_keys() == 0:
            logger.error("‚ùå Kh√¥ng c√≥ API key n√†o c·ªßa Gemini ƒë∆∞·ª£c c·∫•u h√¨nh trong ApiKeyManager. Recipe tool c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.")
            # C√≥ th·ªÉ raise exception n·∫øu mu·ªën d·ª´ng h·∫≥n
        else:
            logger.info(f"‚úÖ Gemini integration ready - ApiKeyManager c√≥ {api_key_manager.total_keys()} keys")

        return index, embeddings_model
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o k·∫øt n·ªëi: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def sanitize_text_for_llm(text: str) -> str:
    """L√†m s·∫°ch vƒÉn b·∫£n c·∫£i thi·ªán ƒë·ªÉ lo·∫°i b·ªè c√°c k√Ω t·ª± c√≥ th·ªÉ g√¢y l·ªói JSON."""
    if not text:
        return ""
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒëi·ªÅu khi·ªÉn C0 (00-1F) v√† DEL (7F), ngo·∫°i tr·ª´ TAB, LF, CR
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # Thay th·∫ø d·∫•u ngo·∫∑c k√©p th√¥ng minh b·∫±ng d·∫•u ngo·∫∑c k√©p chu·∫©n
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace("'", "'").replace("'", "'")
    
    # Lo·∫°i b·ªè c√°c escape sequence kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'\\[rn]', ' ', text)
    
    # C·∫Øt b·ªõt n·∫øu qu√° d√†i (m·ªói recipe kh√¥ng n√™n v∆∞·ª£t qu√° 2000 chars)
    if len(text) > 2000:
        text = text[:1997] + "..."
        
    return text

def estimate_tokens(text: str) -> int:
    """∆Ø·ªõc t√≠nh s·ªë token d·ª±a tr√™n character count (3 chars ‚âà 1 token)"""
    return len(text) // 3

def create_dynamic_batches(matches: list) -> list:
    """
    ‚≠ê DYNAMIC BATCHING: T·∫°o batches d·ª±a tr√™n character count thay v√¨ s·ªë l∆∞·ª£ng c·ªë ƒë·ªãnh
    """
    batches = []
    current_batch = []
    current_batch_chars = 0
    
    for match in matches:
        if TEXT_KEY_IN_PINECONE not in match.metadata:
            continue
            
        page_content = match.metadata[TEXT_KEY_IN_PINECONE]
        if SANITIZE_INPUT_TEXT:
            page_content = sanitize_text_for_llm(page_content)
            
        if not page_content:
            continue
            
        # ∆Ø·ªõc t√≠nh character count cho document n√†y (bao g·ªìm wrapper)
        doc_char_count = len(f"--- C√îNG TH·ª®C ID: {match.id} ---\n{page_content}\n--- K·∫æT TH√öC C√îNG TH·ª®C ID: {match.id} ---\n\n")
        
        # Ki·ªÉm tra xem c√≥ th·ªÉ th√™m v√†o batch hi·ªán t·∫°i kh√¥ng
        if (current_batch_chars + doc_char_count > MAX_CHAR_PER_BATCH and current_batch) or len(current_batch) >= MAX_SAFE_BATCH_SIZE:
            # Batch hi·ªán t·∫°i ƒë√£ ƒë·ªß, l∆∞u v√† t·∫°o batch m·ªõi
            if current_batch:  # Ch·ªâ l∆∞u n·∫øu c√≥ content
                batches.append(current_batch)
            current_batch = [match]
            current_batch_chars = doc_char_count
        else:
            # Th√™m v√†o batch hi·ªán t·∫°i
            current_batch.append(match)
            current_batch_chars += doc_char_count
    
    # Th√™m batch cu·ªëi c√πng n·∫øu c√≥
    if current_batch:
        batches.append(current_batch)
    
    return batches

def clean_json_response(response_text: str) -> str:
    """‚≠ê C·∫¢I THI·ªÜN: L√†m s·∫°ch response t·ª´ Gemini tr∆∞·ªõc khi parse JSON v·ªõi nhi·ªÅu fix patterns h∆°n"""
    try:
        cleaned = response_text.strip()
        
        # Lo·∫°i b·ªè markdown wrapper
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
            
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
            
        cleaned = cleaned.strip()
        
        # ‚≠ê C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu pattern fix JSON ph·ªï bi·∫øn h∆°n
        
        # 1. Trailing comma tr∆∞·ªõc } ho·∫∑c ]
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
        
        # 2. Missing quotes cho keys (ch·ªâ fix n·∫øu ch∆∞a c√≥ quotes)
        cleaned = re.sub(r'(\w+):\s*(["\[])', r'"\1": \2', cleaned)
        
        # 3. Single quotes thay v√¨ double quotes cho keys v√† strings
        cleaned = re.sub(r"'([^']*?)'(\s*:\s*)", r'"\1"\2', cleaned)  # Keys
        cleaned = re.sub(r':\s*\'([^\']*?)\'', r': "\1"', cleaned)    # String values
        
        # 4. Th√™m d·∫•u ph·∫©y thi·∫øu gi·ªØa objects trong array
        cleaned = re.sub(r'}\s*{', r'}, {', cleaned)
        
        # 5. Lo·∫°i b·ªè control characters c√≥ th·ªÉ g√¢y l·ªói JSON
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', cleaned)
        
        # 6. Fix escaped quotes th·ª´a
        cleaned = re.sub(r'\\"', '"', cleaned)
        
        # 7. ƒê·∫£m b·∫£o JSON b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c ƒë√∫ng format
        if not cleaned.startswith('[') and not cleaned.startswith('{'):
            # T√¨m JSON array ho·∫∑c object ƒë·∫ßu ti√™n
            array_match = re.search(r'(\[[\s\S]*\])', cleaned)
            object_match = re.search(r'(\{[\s\S]*\})', cleaned)
            
            if array_match:
                cleaned = array_match.group(1)
            elif object_match:
                cleaned = object_match.group(1)
        
        return cleaned
    except Exception as e:
        logger.warning(f"L·ªói khi clean JSON response: {e}")
        return response_text.strip()

def parse_json_with_fallback(response_text: str) -> dict:
    """‚≠ê M·ªöI: Parse JSON v·ªõi multiple fallback strategies"""
    try:
        cleaned = clean_json_response(response_text)
        
        # Th·ª≠ parse tr·ª±c ti·∫øp
        try:
            result = json.loads(cleaned)
            return {"success": True, "data": result}
        except json.JSONDecodeError as e:
            logger.warning(f"Direct JSON parse failed: {e}")
        
        # ‚≠ê Fallback 1: T√¨m v√† extract JSON object/array t·ª´ text
        json_patterns = [
            r'\[[\s\S]*?\]',  # JSON array
            r'\{[\s\S]*?\}',  # JSON object
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, cleaned)
            for match in matches:
                try:
                    result = json.loads(match)
                    logger.info("‚úÖ JSON parsed successfully v·ªõi pattern matching")
                    return {"success": True, "data": result}
                except json.JSONDecodeError:
                    continue
        
        # ‚≠ê Fallback 2: N·∫øu l√† object nh∆∞ng mong ƒë·ª£i array, extract array t·ª´ object
        try:
            obj_result = json.loads(cleaned)
            if isinstance(obj_result, dict):
                # T√¨m key ch·ª©a array
                for key, value in obj_result.items():
                    if isinstance(value, list):
                        logger.info(f"‚úÖ Extracted array t·ª´ object key: {key}")
                        return {"success": True, "data": value}
        except json.JSONDecodeError:
            pass
        
        # ‚≠ê Fallback 3: Return structured error
        return {
            "success": False, 
            "error": "Invalid JSON from Gemini", 
            "raw_response": response_text[:500],  # Gi·ªõi h·∫°n ƒë·ªô d√†i
            "cleaned_response": cleaned[:500]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"JSON parsing exception: {str(e)}",
            "raw_response": response_text[:500]
        }

async def call_gemini_api_async(prompt: str, api_key_for_this_call: str, task_description: str, max_retries: int = 3) -> str:
    """‚≠ê ASYNC VERSION v·ªõi API KEY ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ Worker Pool - S·ª¨ D·ª§NG MODEL POOL"""
    prompt_char_count = len(prompt)
    estimated_tokens = estimate_tokens(prompt)
    
    if not api_key_for_this_call:
        logger.error("‚ùå Kh√¥ng c√≥ API key Gemini ƒë∆∞·ª£c truy·ªÅn v√†o")
        return json.dumps({"error": "Kh√¥ng c√≥ API key Gemini kh·∫£ d·ª•ng."})
    
    # Log key rotation (an to√†n)
    masked_key = f"{api_key_for_this_call[:8]}..." if len(api_key_for_this_call) > 8 else "***"
    logger.info(f"üîç {task_description}: G·ªçi Gemini v·ªõi key {masked_key} - Chars: {prompt_char_count:,}, Est. tokens: {estimated_tokens:,}")
    
    loop = asyncio.get_running_loop()
    
    for attempt in range(max_retries):
        try:
            # ‚≠ê S·ª¨ D·ª§NG MODEL POOL THAY V√å genai.configure() (thread-safe)
            temp_gemini_model = gemini_model_pool.get_model_for_key(api_key_for_this_call)
            if not temp_gemini_model:
                logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y model cho key {masked_key}")
                return json.dumps({"error": "Kh√¥ng t√¨m th·∫•y model cho API key"})
            
            # ‚≠ê CH·∫†Y GEMINI TRONG EXECUTOR ƒë·ªÉ kh√¥ng block event loop
            response = await loop.run_in_executor(
                None,
                lambda: temp_gemini_model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=MAX_GEMINI_OUTPUT_TOKENS,
                        temperature=0.1
                    )
                )
            )
            
            if response.parts:
                response_text = response.text
                
                # ‚≠ê S·ª¨ D·ª§NG PARSE_JSON_WITH_FALLBACK
                parse_result = parse_json_with_fallback(response_text)
                
                if parse_result["success"]:
                    # Tr·∫£ v·ªÅ JSON string c·ªßa data ƒë√£ parse th√†nh c√¥ng
                    return json.dumps(parse_result["data"], ensure_ascii=False)
                else:
                    # Tr·∫£ v·ªÅ structured error
                    logger.error(f"‚ùå JSON parse failed: {parse_result.get('error')}")
                    if attempt < max_retries - 1:
                        logger.info(f"üîÑ Retry attempt {attempt + 1}/{max_retries} due to JSON parse error")
                        await asyncio.sleep(5)
                        continue
                    return json.dumps({
                        "error": parse_result["error"],
                        "raw_snippet": parse_result.get("raw_response", "")[:200]
                    })
            else:
                logger.warning("Gemini kh√¥ng tr·∫£ v·ªÅ n·ªôi dung.")
                logger.warning(f"Gemini response: {response}")
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    logger.warning(f"B·ªã ch·∫∑n b·ªüi: {response.prompt_feedback.block_reason}")
                return "[]"
                
        except Exception as e:
            error_str = str(e).lower()
            
            if "429" in error_str or "resource_exhausted" in error_str or "quota" in error_str:
                retry_delay = 15 + attempt * 10
                delay_match = re.search(r'retry in (\d+)s', error_str)
                if delay_match: 
                    retry_delay = max(int(delay_match.group(1)), retry_delay)
                logger.warning(f"üö´ Quota limit Gemini, retry sau {retry_delay}s (#{attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    
            elif "invalid_argument" in error_str and ("token" in error_str or "size" in error_str):
                logger.error(f"üí• Prompt qu√° l·ªõn: {estimated_tokens:,} tokens, {prompt_char_count:,} chars")
                return json.dumps({"error_too_large": True, "estimated_tokens": estimated_tokens})
                
            elif "500" in error_str or "internal" in error_str or "unavailable" in error_str:
                retry_delay = 10 + attempt * 15
                logger.warning(f"üîß Server error Gemini, retry sau {retry_delay}s (#{attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
            else:
                logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh Gemini (#{attempt+1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    return json.dumps({"error": f"Gemini API error: {str(e)}"})
                await asyncio.sleep(10)
                
    logger.error(f"üí• Th·∫•t b·∫°i ho√†n to√†n sau {max_retries} l·∫ßn th·ª≠ v·ªõi Gemini")
    return json.dumps({"error": "Kh√¥ng th·ªÉ k·∫øt n·ªëi Gemini sau nhi·ªÅu l·∫ßn th·ª≠"})

async def search_and_filter_recipes_async(user_query: str) -> str:
    """
    ‚≠ê ASYNC VERSION: Search v·ªõi dynamic batching v√† x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô ƒë·ªìng th·ªùi
    """
    try:
        # ‚≠ê CH·∫†Y INIT_CONNECTIONS TRONG EXECUTOR
        loop = asyncio.get_running_loop()
        pinecone_index, embeddings_model = await loop.run_in_executor(None, init_connections)
    except Exception as e:
        logger.error(f"üí• Kh√¥ng th·ªÉ kh·ªüi t·∫°o k·∫øt n·ªëi: {str(e)}")
        return json.dumps({"error": f"Connection init failed: {str(e)}"})

    all_selected_recipes = []
    error_reports = []

    try:
        logger.info(f"üîç ƒêang embed query: '{user_query}'")
        
        # ‚≠ê CH·∫†Y EMBEDDING TRONG EXECUTOR
        query_vector = await loop.run_in_executor(None, embeddings_model.embed_query, user_query)
        logger.info("‚úÖ ƒê√£ embed query th√†nh c√¥ng")

        # ‚≠ê GI·∫¢M PINECONE TOP_K ƒë·ªÉ gi·∫£m t·∫£i
        actual_top_k = min(TOTAL_DOCS_IN_PINECONE, 3000)  # Gi·∫£m t·ª´ 10000 xu·ªëng 3000
        logger.info(f"üîç Query Pinecone v·ªõi top_k={actual_top_k}")
        
        # ‚≠ê CH·∫†Y PINECONE QUERY TRONG EXECUTOR
        query_response = await loop.run_in_executor(
            None,
            lambda: pinecone_index.query(
                vector=query_vector,
                top_k=actual_top_k,
                include_metadata=True,
            )
        )
        
        retrieved_matches = query_response.matches
        logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(retrieved_matches)} documents t·ª´ Pinecone")

        if not retrieved_matches:
            return json.dumps([])

        # ‚≠ê S·ª¨ D·ª§NG DYNAMIC BATCHING
        dynamic_batches = create_dynamic_batches(retrieved_matches)
        logger.info(f"üîÑ Chia th√†nh {len(dynamic_batches)} dynamic batches")

        # ‚≠ê KH·ªûI T·∫†O WORKER POOL CONFIGURATION
        NUM_GEMINI_WORKERS = min(api_key_manager.total_keys(), MAX_GEMINI_WORKERS)
        if NUM_GEMINI_WORKERS == 0: 
            NUM_GEMINI_WORKERS = 1  # √çt nh·∫•t 1 worker n·∫øu c√≥ key
        
        logger.info(f"ü§ñ Kh·ªüi t·∫°o Worker Pool v·ªõi {NUM_GEMINI_WORKERS} workers (c√≥ {api_key_manager.total_keys()} API keys)")
        logger.info(f"üìä S·∫Ω x·ª≠ l√Ω {len(dynamic_batches)} batches v·ªõi worker pool parallelism")
        
        # ‚≠ê KH·ªûI T·∫†O QUEUE V√Ä RESULT STORAGE
        work_queue = asyncio.Queue()
        results_list = []  # Thread-safe v·ªõi coroutine
        error_reports_list = []
        
        # ‚≠ê T·∫†O C√ÅC TASK_DATA V√Ä ƒê∆ØA V√ÄO QUEUE
        for batch_idx, batch_matches in enumerate(dynamic_batches):
            logger.info(f"üì¶ Chu·∫©n b·ªã batch {batch_idx + 1}/{len(dynamic_batches)} ({len(batch_matches)} docs)")
            
            context_parts = []
            
            for match in batch_matches:
                doc_id = match.id
                if TEXT_KEY_IN_PINECONE in match.metadata:
                    page_content = match.metadata[TEXT_KEY_IN_PINECONE]
                    if SANITIZE_INPUT_TEXT:
                        page_content = sanitize_text_for_llm(page_content)
                    
                    if page_content:
                        context_parts.append(f"--- C√îNG TH·ª®C ID: {doc_id} ---\n{page_content}\n--- K·∫æT TH√öC C√îNG TH·ª®C ID: {doc_id} ---")

            if not context_parts:
                logger.info(f"‚è≠Ô∏è Batch {batch_idx + 1} r·ªóng, b·ªè qua")
                continue

            batch_context = "\n\n".join(context_parts)
            estimated_tokens = estimate_tokens(batch_context)
            
            logger.info(f"üìä Batch {batch_idx + 1}: {len(batch_context):,} chars, ~{estimated_tokens:,} tokens")
            
            # Ki·ªÉm tra an to√†n token
            if estimated_tokens > 300000:  # 300k tokens limit cho an to√†n
                logger.error(f"‚ö†Ô∏è Batch {batch_idx + 1} qu√° l·ªõn ({estimated_tokens:,} tokens), b·ªè qua")
                error_reports_list.append({
                    "error_batch": batch_idx + 1,
                    "message": f"Batch qu√° l·ªõn: {estimated_tokens:,} tokens",
                    "doc_count": len(batch_matches)
                })
                continue

            # ‚≠ê PROMPT T·ªêI ∆ØU H√ìA M·∫†NH CHO JSON - Y√äU C·∫¶U C·ª∞C K·ª≤ CH·∫∂T CH·∫º
            optimized_prompt = f'''NHI·ªÜM V·ª§: Ph√¢n t√≠ch query "{user_query}" v√† ch·ªçn c√°c c√¥ng th·ª©c ph√π h·ª£p NH·∫§T.

üö® QUY T·∫ÆC TUY·ªÜT ƒê·ªêI:
- CH·ªà tr·∫£ v·ªÅ JSON array h·ª£p l·ªá
- KH√îNG th√™m text, gi·∫£i th√≠ch, markdown
- KH√îNG s·ª≠ d·ª•ng d·∫•u ngo·∫∑c k√©p th√¥ng minh (" ")
- CH·ªà s·ª≠ d·ª•ng d·∫•u ngo·∫∑c k√©p ASCII chu·∫©n (")
- KH√îNG c√≥ trailing comma
- N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£: []

üìã ƒê·ªäNH D·∫†NG B·∫ÆT BU·ªòC:
[{{"id":"recipe_id","name":"t√™n c√¥ng th·ª©c","url":"link ho·∫∑c null","ingredients_summary":"nguy√™n li·ªáu ch√≠nh"}}]

‚úÖ V√ç D·ª§ CH√çNH X√ÅC:
[{{"id":"recipe_123","name":"Salad gi·∫£m c√¢n","url":"https://example.com","ingredients_summary":"Rau xanh, c√† chua"}},{{"id":"recipe_456","name":"Sinh t·ªë rau c·ªß","url":null,"ingredients_summary":"C·∫£i b√≥ x√¥i, chu·ªëi"}}]

‚ùå TUY·ªÜT ƒê·ªêI KH√îNG:
- Kh√¥ng markdown: ```json
- Kh√¥ng text th√™m: "D∆∞·ªõi ƒë√¢y l√†..."
- Kh√¥ng trailing comma: }},]
- Kh√¥ng smart quotes: "text"

D·ªÆLI·ªÜU C√îNG TH·ª®C:
{batch_context}

TR·∫¢ V·ªÄ JSON ARRAY:'''

            # ‚≠ê ƒê∆ØA TASK V√ÄO QUEUE
            if optimized_prompt.strip():
                task_data = {
                    "prompt": optimized_prompt,
                    "batch_num": batch_idx + 1,
                    "doc_count": len(batch_matches)
                }
                await work_queue.put(task_data)

        # ‚≠ê H√ÄM GEMINI WORKER V·ªöI EARLY STOPPING
        async def gemini_worker(worker_id: int):
            logger.info(f"ü§ñ Worker {worker_id}: B·∫Øt ƒë·∫ßu ho·∫°t ƒë·ªông.")
            while True:
                try:
                    # ‚≠ê KI·ªÇM TRA EARLY STOPPING: N·∫øu ƒë√£ ƒë·ªß 20 recipes th√¨ d·ª´ng worker
                    if len(results_list) >= MAX_RECIPES_RESULT:
                        logger.info(f"üõë Worker {worker_id}: ƒê√£ ƒë·∫°t {MAX_RECIPES_RESULT} recipes, d·ª´ng worker.")
                        # ƒê√°nh d·∫•u task done v√† tho√°t
                        try:
                            task_data = work_queue.get_nowait()
                            work_queue.task_done()
                        except:
                            pass
                        break
                    
                    task_data = await work_queue.get()
                    if task_data is None:  # T√≠n hi·ªáu d·ª´ng
                        work_queue.task_done()
                        logger.info(f"ü§ñ Worker {worker_id}: Nh·∫≠n t√≠n hi·ªáu d·ª´ng.")
                        break

                    # ‚≠ê KI·ªÇM TRA L·∫†I SAU KHI L·∫§Y TASK (v√¨ c√≥ th·ªÉ worker kh√°c ƒë√£ ƒë·ªß)
                    if len(results_list) >= MAX_RECIPES_RESULT:
                        logger.info(f"üõë Worker {worker_id}: ƒê√£ ƒë·∫°t {MAX_RECIPES_RESULT} recipes sau khi l·∫•y task, b·ªè qua.")
                        work_queue.task_done()
                        break

                    prompt_to_process = task_data["prompt"]
                    batch_num_log = task_data["batch_num"]
                    doc_count_log = task_data["doc_count"]
                    
                    task_description = f"Worker {worker_id} - Batch {batch_num_log}"
                    
                    # M·ªói worker t·ª± l·∫•y key m·ªõi cho m·ªói task n√≥ x·ª≠ l√Ω
                    api_key_for_call = api_key_manager.get_next_key()
                    if not api_key_for_call:
                        logger.error(f"ü§ñ Worker {worker_id}: Kh√¥ng c√≥ API key, b·ªè qua batch {batch_num_log}")
                        error_reports_list.append({
                            "error_batch": batch_num_log, "error_type": "no_api_key",
                            "message": "Kh√¥ng c√≥ API key kh·∫£ d·ª•ng", "doc_count": doc_count_log
                        })
                        work_queue.task_done()
                        continue

                    logger.info(f"ü§ñ Worker {worker_id}: ƒêang x·ª≠ l√Ω Batch {batch_num_log} ({doc_count_log} docs) v·ªõi key ...{api_key_for_call[-4:]}")
                    
                    # G·ªçi API Gemini
                    gemini_response_text = await call_gemini_api_async(
                        prompt_to_process, 
                        api_key_for_call,
                        task_description
                    )
                    
                    # X·ª≠ l√Ω response (parse JSON, v.v...)
                    parse_attempt = parse_json_with_fallback(gemini_response_text)
                    if parse_attempt["success"]:
                        parsed_data = parse_attempt["data"]
                        if isinstance(parsed_data, list):
                            valid_items = [item for item in parsed_data if isinstance(item, dict) and "id" in item]
                            if valid_items:
                                # ‚≠ê KI·ªÇM TRA V√Ä GI·ªöI H·∫†N S·ªê L∆Ø·ª¢NG KHI TH√äM V√ÄO RESULTS
                                current_count = len(results_list)
                                remaining_slots = MAX_RECIPES_RESULT - current_count
                                
                                if remaining_slots > 0:
                                    # Ch·ªâ th√™m s·ªë l∆∞·ª£ng recipes c√≤n thi·∫øu
                                    items_to_add = valid_items[:remaining_slots]
                                    results_list.extend(items_to_add)
                                    
                                    logger.info(f"ü§ñ Worker {worker_id}: Batch {batch_num_log} th√†nh c√¥ng, th√™m {len(items_to_add)}/{len(valid_items)} items. T·ªïng: {len(results_list)}/{MAX_RECIPES_RESULT}")
                                    
                                    # N·∫øu ƒë√£ ƒë·ªß, log th√¥ng b√°o early stopping
                                    if len(results_list) >= MAX_RECIPES_RESULT:
                                        logger.info(f"üéØ Worker {worker_id}: ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {MAX_RECIPES_RESULT} recipes. Early stopping!")
                                else:
                                    logger.info(f"üõë Worker {worker_id}: ƒê√£ ƒë·ªß {MAX_RECIPES_RESULT} recipes, b·ªè qua batch {batch_num_log}")
                            else:
                                logger.info(f"ü§ñ Worker {worker_id}: Batch {batch_num_log} kh√¥ng c√≥ valid items.")
                        else:
                            logger.warning(f"ü§ñ Worker {worker_id}: Batch {batch_num_log} tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng kh√¥ng ph·∫£i list: {type(parsed_data)}")
                            error_reports_list.append({"error_batch": batch_num_log, "error_type": "invalid_gemini_response_format", "doc_count": doc_count_log})
                    else:
                        logger.error(f"ü§ñ Worker {worker_id}: Batch {batch_num_log} l·ªói parse JSON: {parse_attempt.get('error')}")
                        error_reports_list.append({
                            "error_batch": batch_num_log, "error_type": "json_parse_failed", 
                            "message": parse_attempt.get('error'), "doc_count": doc_count_log,
                            "raw_snippet": parse_attempt.get('raw_response')
                        })
                    
                    work_queue.task_done()
                    
                    # ‚≠ê KI·ªÇM TRA EARLY STOPPING SAU KHI XONG TASK
                    if len(results_list) >= MAX_RECIPES_RESULT:
                        logger.info(f"üéØ Worker {worker_id}: ƒê√£ ƒë·∫°t {MAX_RECIPES_RESULT} recipes, d·ª´ng worker s·ªõm.")
                        break
                    
                    # Th·ªùi gian ngh·ªâ nh·ªè sau m·ªói batch c·ªßa m·ªôt worker
                    await asyncio.sleep(WORKER_SLEEP_BETWEEN_TASKS)

                except asyncio.CancelledError:
                    logger.info(f"ü§ñ Worker {worker_id}: B·ªã cancel.")
                    break
                except Exception as e:
                    logger.error(f"ü§ñ Worker {worker_id}: L·ªói kh√¥ng mong mu·ªën: {e}", exc_info=True)
                    if 'task_data' in locals() and task_data and 'batch_num' in task_data:
                         error_reports_list.append({"error_batch": task_data['batch_num'], "error_type": "worker_exception", "message": str(e), "doc_count": task_data.get('doc_count',0)})
                    if 'task_data' in locals() and task_data is not None:
                         work_queue.task_done()
                    await asyncio.sleep(WORKER_ERROR_SLEEP)

        # ‚≠ê KH·ªûI T·∫†O V√Ä CH·∫†Y C√ÅC WORKER V·ªöI EARLY STOPPING
        worker_tasks = []
        for i in range(NUM_GEMINI_WORKERS):
            worker_tasks.append(asyncio.create_task(gemini_worker(i + 1)))

        # ‚≠ê GI√ÅM S√ÅT EARLY STOPPING: Ch·ªù queue x·ª≠ l√Ω ho·∫∑c ƒë·∫°t gi·ªõi h·∫°n
        try:
            while not work_queue.empty() and len(results_list) < MAX_RECIPES_RESULT:
                await asyncio.sleep(0.5)  # Ki·ªÉm tra ƒë·ªãnh k·ª≥
            
            # N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n, cancel c√°c worker c√≤n l·∫°i
            if len(results_list) >= MAX_RECIPES_RESULT:
                logger.info(f"üéØ ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {MAX_RECIPES_RESULT} recipes, d·ª´ng to√†n b·ªô workers s·ªõm.")
                
                # Cancel c√°c worker tasks
                for task in worker_tasks:
                    if not task.done():
                        task.cancel()
                
                # Clear remaining queue items
                try:
                    while not work_queue.empty():
                        await work_queue.get()
                        work_queue.task_done()
                except:
                    pass
            else:
                # Ch·ªù t·∫•t c·∫£ c√°c item trong queue ƒë∆∞·ª£c x·ª≠ l√Ω b√¨nh th∆∞·ªùng
                await work_queue.join()
        
        except Exception as e:
            logger.error(f"üí• L·ªói trong gi√°m s√°t early stopping: {e}")
            await work_queue.join()  # Fallback to normal join

        # G·ª≠i t√≠n hi·ªáu d·ª´ng cho t·∫•t c·∫£ worker (n·∫øu ch∆∞a cancel)
        for _ in range(NUM_GEMINI_WORKERS):
            try:
                await work_queue.put(None)
            except:
                pass

        # Ch·ªù t·∫•t c·∫£ worker ho√†n th√†nh ho·∫∑c cancel
        await asyncio.gather(*worker_tasks, return_exceptions=True)

        logger.info(f"üèÅ T·∫•t c·∫£ c√°c worker ƒë√£ ho√†n th√†nh. T·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ {len(results_list)} recipes...")

        # ‚≠ê L·ªåC TR√ôNG L·∫∂P B·∫∞NG T√äN CHU·∫®N H√ìA
        def normalize_recipe_name(name: str) -> str:
            """Chu·∫©n h√≥a t√™n recipe ƒë·ªÉ so s√°nh tr√πng l·∫∑p"""
            if not name:
                return ""
            # Chuy·ªÉn v·ªÅ lowercase, lo·∫°i b·ªè d·∫•u c√°ch, d·∫•u g·∫°ch ngang, k√Ω t·ª± ƒë·∫∑c bi·ªát
            import unicodedata
            normalized = unicodedata.normalize('NFD', str(name).lower())
            normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')  # Lo·∫°i b·ªè d·∫•u
            normalized = re.sub(r'[^a-z0-9]', '', normalized)  # Ch·ªâ gi·ªØ ch·ªØ v√† s·ªë
            return normalized

        if results_list:
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu l·ªçc tr√πng l·∫∑p t·ª´ {len(results_list)} recipes...")
            
            final_unique_recipes = []
            seen_normalized_names = set()
            
            for recipe_item in results_list:
                if not isinstance(recipe_item, dict) or not recipe_item.get("name"):
                    continue
                    
                normalized_name = normalize_recipe_name(recipe_item["name"])
                if normalized_name and normalized_name not in seen_normalized_names:
                    final_unique_recipes.append(recipe_item)
                    seen_normalized_names.add(normalized_name)
                    
                    # ‚≠ê EARLY STOPPING TRONG L·ªåC TR√ôNG L·∫∂P: D·ª´ng khi ƒë·ªß 20 recipes
                    if len(final_unique_recipes) >= MAX_RECIPES_RESULT:
                        logger.info(f"üéØ ƒê√£ ƒë·∫°t {MAX_RECIPES_RESULT} recipes unique, d·ª´ng l·ªçc tr√πng l·∫∑p s·ªõm.")
                        break
                else:
                    logger.debug(f"ƒê√£ l·ªçc recipe tr√πng l·∫∑p: {recipe_item.get('name', 'Unknown')}")
            
            results_list = final_unique_recipes
            logger.info(f"‚úÖ Sau khi l·ªçc tr√πng l·∫∑p: {len(results_list)} recipes duy nh·∫•t (gi·ªõi h·∫°n t·ªëi ƒëa {MAX_RECIPES_RESULT})")

        # ‚≠ê X·ª¨ L√ù K·∫æT QU·∫¢ CU·ªêI C√ôNG
        if not results_list and error_reports_list:
            return json.dumps({
                "message": "Kh√¥ng c√≥ c√¥ng th·ª©c n√†o ƒë∆∞·ª£c ch·ªçn v√† c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω.",
                "errors": error_reports_list[:5]  # Gi·ªõi h·∫°n s·ªë l·ªói hi·ªÉn th·ªã
            }, ensure_ascii=False, indent=2)
        elif not results_list:
            return json.dumps([])
        elif error_reports_list:
            # C√≥ recipes nh∆∞ng c≈©ng c√≥ l·ªói - ch·ªâ tr·∫£ v·ªÅ recipes
            logger.warning(f"‚ö†Ô∏è C√≥ {len(error_reports_list)} l·ªói nh∆∞ng v·∫´n t√¨m ƒë∆∞·ª£c {len(results_list)} recipes")
            return json.dumps(results_list, ensure_ascii=False, indent=2)
        else:
            return json.dumps(results_list, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"üí• Critical error trong search_and_filter_recipes_async: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return json.dumps({"error": f"System error: {str(e)}"})

# ‚≠ê WRAPPER ƒê·ªíNG B·ªò ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i
def search_and_filter_recipes(user_query: str) -> str:
    """
    Wrapper ƒë·ªìng b·ªô cho search_and_filter_recipes_async ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i
    """
    return asyncio.run(search_and_filter_recipes_async(user_query))

async def main_test_async():
    """‚≠ê ASYNC TEST FUNCTION"""
    try:
        query = "M√≥n ƒÉn cho gi·∫£m c√¢n, √≠t calo, nhi·ªÅu rau xanh"
        
        print(f"\nüîç Testing ASYNC v·ªõi query: '{query}'")
        
        import time
        start_time = time.time()
        
        result = await search_and_filter_recipes_async(query)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\n‚ö° ASYNC PERFORMANCE: {duration:.2f} seconds")
        print("\n‚úÖ K·∫æT QU·∫¢:")
        try:
            parsed = json.loads(result)
            if isinstance(parsed, list):
                print(f"T√¨m ƒë∆∞·ª£c {len(parsed)} recipes")
            else:
                print("Response kh√¥ng ph·∫£i list recipes")
            print(json.dumps(parsed, indent=2, ensure_ascii=False))
        except json.JSONDecodeError:
            print("Raw result:", result)

    except ValueError as ve: 
        print(f"üí• L·ªói c·∫•u h√¨nh: {ve}")
    except Exception as e:
        logger.error(f"üí• L·ªói kh√¥ng mong mu·ªën: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    # ‚≠ê CH·∫†Y ASYNC TEST
    asyncio.run(main_test_async())