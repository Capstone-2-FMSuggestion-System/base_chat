"""
File kh·ªüi ƒë·ªông ch√≠nh cho Medical AI Chat API
File n√†y l√† ƒëi·ªÉm v√†o c·ªßa ·ª©ng d·ª•ng, ch·ª©a c·∫£ ƒë·ªãnh nghƒ©a app FastAPI v√† ƒëi·ªÉm kh·ªüi ch·∫°y
"""
import logging
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from typing import Dict, Any

from app.api.chat import router as chat_router
# from app.api.auth import router as auth_router # Removed
from app.config import settings
from app.db.database import engine, Base
from app.services.llm_service_factory import LLMServiceFactory, LLMServiceType

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)

# T·∫°o instance logger
logger = logging.getLogger(__name__)

# Kh·ªüi t·∫°o LLM Service Factory
llm_factory = LLMServiceFactory()

# ‚≠ê GLOBAL EMBEDDING MODEL CACHE
_global_embedding_model = None

def get_global_embedding_model():
    """L·∫•y global embedding model ƒë√£ ƒë∆∞·ª£c pre-load"""
    return _global_embedding_model

# T·∫°o b·∫£ng trong c∆° s·ªü d·ªØ li·ªáu
Base.metadata.create_all(bind=engine)

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng FastAPI
app = FastAPI(
    title="Medical AI Chatbot API",
    description="API cho h·ªá th·ªëng tr√≤ chuy·ªán y t·∫ø th√¥ng minh",
    version="1.0.0"
)

# Thi·∫øt l·∫≠p CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong m√¥i tr∆∞·ªùng production n√™n h·∫°n ch·∫ø c√°c ngu·ªìn
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Type", "Content-Length", "Access-Control-Allow-Origin"],
)

# ƒêƒÉng k√Ω router
# app.include_router(auth_router, prefix="/api", tags=["auth"]) # Removed
app.include_router(chat_router, prefix="/api", tags=["chat"])


@app.get("/", tags=["health"])
async def root():
    """Ki·ªÉm tra tr·∫°ng th√°i API"""
    return {"message": "Medical AI Chat API ƒëang ho·∫°t ƒë·ªông"}


@app.get("/api/llm/status", tags=["llm"])
async def get_llm_status():
    """L·∫•y tr·∫°ng th√°i d·ªãch v·ª• LLM v√† m√¥ h√¨nh"""
    try:
        # L·∫•y service hi·ªán t·∫°i
        active_service = await llm_factory.initialize()
        
        # Chu·∫©n b·ªã th√¥ng tin ph·∫£n h·ªìi
        status = {
            "llm_service": active_service,
            "service_available": active_service is not None,
            "embedding_model_loaded": _global_embedding_model is not None,
        }
        
        # Th√™m th√¥ng tin v·ªÅ m√¥ h√¨nh n·∫øu ƒëang s·ª≠ d·ª•ng Ollama
        if active_service == "ollama" and hasattr(llm_factory, "model_status"):
            status.update({
                "model_name": llm_factory.model_status.get("model_name"),
                "model_available": llm_factory.model_status.get("available", False),
                "model_message": llm_factory.model_status.get("message", "")
            })
            
        return status
    except Exception as e:
        logger.error(f"L·ªói khi ki·ªÉm tra tr·∫°ng th√°i LLM: {str(e)}")
        return {
            "llm_service": "unavailable",
            "service_available": False,
            "error": str(e)
        }


@app.on_event("startup")
async def startup_event():
    """Kh·ªüi ƒë·ªông c√°c d·ªãch v·ª• v√† ki·ªÉm tra c·∫•u h√¨nh khi ·ª©ng d·ª•ng kh·ªüi ƒë·ªông"""
    global _global_embedding_model
    
    logger.info("Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng v√† ki·ªÉm tra c√°c d·ªãch v·ª•...")
    
    # ‚≠ê PRE-LOAD EMBEDDING MODEL
    try:
        logger.info("üîÑ ƒêang pre-load m√¥ h√¨nh embedding sentence-transformers/all-mpnet-base-v2...")
        from langchain_huggingface import HuggingFaceEmbeddings
        
        _global_embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Test embedding ƒë·ªÉ ƒë·∫£m b·∫£o model ho·∫°t ƒë·ªông
        test_embedding = _global_embedding_model.embed_query("test")
        logger.info(f"‚úÖ ƒê√£ pre-load th√†nh c√¥ng m√¥ h√¨nh embedding (dimension: {len(test_embedding)})")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi pre-load embedding model: {str(e)}")
        _global_embedding_model = None
    
    # Kh·ªüi t·∫°o v√† ki·ªÉm tra d·ªãch v·ª• LLM
    try:
        active_service = await llm_factory.initialize()
        logger.info(f"D·ªãch v·ª• LLM ƒë√£ kh·ªüi ƒë·ªông: {active_service}")
        
        # Ki·ªÉm tra v√† th√¥ng b√°o v·ªÅ m√¥ h√¨nh Ollama n·∫øu c·∫ßn
        if active_service == "ollama" and hasattr(llm_factory, "model_status"):
            model_name = llm_factory.model_status.get("model_name")
            model_available = llm_factory.model_status.get("available", False)
            model_message = llm_factory.model_status.get("message", "")
            
            if not model_available:
                logger.warning(f"C·∫¢NH B√ÅO: {model_message}")
                logger.warning(f"S·ª≠ d·ª•ng l·ªánh sau ƒë·ªÉ t·∫£i m√¥ h√¨nh: ollama pull {model_name}")
            else:
                logger.info(f"M√¥ h√¨nh LLM: {model_name} - Tr·∫°ng th√°i: S·∫µn s√†ng")
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi ƒë·ªông d·ªãch v·ª• LLM: {str(e)}")
        logger.error("H·ªá th·ªëng c√≥ th·ªÉ ho·∫°t ƒë·ªông kh√¥ng ·ªïn ƒë·ªãnh. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn d·ªãch v·ª• LLM.") 


if __name__ == "__main__":
    # Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng FastAPI
    uvicorn.run(
        "main:app", 
        host=settings.API_HOST, 
        port=settings.API_PORT,
        reload=settings.DEBUG_MODE
    ) 