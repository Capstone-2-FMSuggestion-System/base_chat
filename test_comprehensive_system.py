#!/usr/bin/env python3
"""
Comprehensive System Test - Ki·ªÉm tra to√†n di·ªán t·∫•t c·∫£ c√°c ch·ª©c nƒÉng
Bao g·ªìm: Background DB, Parallel Processing, v√† t·∫•t c·∫£ improvements t·ª´ G.1-G.5
"""

import asyncio
import time
import logging
import json
from typing import Dict, List, Any
import aiohttp
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveSystemTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.test_results = []
        self.conversation_id = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def send_chat_request(self, message: str, conversation_id: int = None) -> Dict:
        """Send chat request v√† measure performance"""
        start_time = time.time()
        
        payload = {
            "message": message,
            "conversation_id": conversation_id
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer test_token"  # Adjust theo auth system
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/api/chat", 
                json=payload, 
                headers=headers
            ) as response:
                response_time = time.time() - start_time
                
                if response.status == 200:
                    result = await response.json()
                    
                    test_result = {
                        "timestamp": datetime.now().isoformat(),
                        "message": message[:100] + "..." if len(message) > 100 else message,
                        "response_time_ms": round(response_time * 1000, 2),
                        "status": "success",
                        "conversation_id": result.get("conversation_id"),
                        "response_length": len(result.get("assistant_message", {}).get("content", "")),
                        "has_summary": bool(result.get("current_summary")),
                        "is_new_conversation": result.get("is_new_conversation", False),
                        "is_valid_scope": result.get("is_valid_scope", False),
                        "need_more_info": result.get("need_more_info", True),
                        "requests_food": result.get("requests_food", False),
                        "requests_beverage": result.get("requests_beverage", False),
                        "is_food_related": result.get("is_food_related", False),
                        "user_rejected_info": result.get("user_rejected_info", False),
                        "suggest_general_options": result.get("suggest_general_options", False),
                        "response_content": result.get("assistant_message", {}).get("content", "")[:200]
                    }
                    
                    logger.info(f"‚úÖ Request th√†nh c√¥ng trong {response_time * 1000:.2f}ms")
                    return test_result
                else:
                    error_text = await response.text()
                    logger.error(f"‚ùå Request th·∫•t b·∫°i: {response.status} - {error_text}")
                    return {
                        "timestamp": datetime.now().isoformat(),
                        "message": message[:100] + "...",
                        "response_time_ms": round(response_time * 1000, 2),
                        "status": "error",
                        "error": f"HTTP {response.status}: {error_text}"
                    }
                    
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"üí• Exception: {str(e)}")
            return {
                "timestamp": datetime.now().isoformat(),
                "message": message[:100] + "...",
                "response_time_ms": round(response_time * 1000, 2),
                "status": "exception",
                "error": str(e)
            }

    async def test_basic_functionality(self):
        """Test 1: Basic functionality v√† flow"""
        logger.info("\n" + "="*60)
        logger.info("TEST 1: BASIC FUNCTIONALITY")
        logger.info("="*60)
        
        test_cases = [
            {
                "message": "Xin ch√†o! T√¥i c·∫ßn t∆∞ v·∫•n v·ªÅ dinh d∆∞·ª°ng",
                "description": "Greeting message",
                "expected": ["is_valid_scope=True", "greeting response"]
            },
            {
                "message": "T√¥i b·ªã ti·ªÉu ƒë∆∞·ªùng type 2, c·∫ßn g·ª£i √Ω m√≥n ƒÉn ph√π h·ª£p",
                "description": "Food consultation with health condition",
                "expected": ["is_food_related=True", "requests_food=True", "health info extraction"]
            },
            {
                "message": "Cho t√¥i th√™m c√¥ng th·ª©c n·∫•u ƒÉn cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng",
                "description": "Follow-up food request",
                "expected": ["recipe results", "medichat response"]
            }
        ]
        
        for i, test_case in enumerate(test_cases):
            logger.info(f"\nüìù Test 1.{i+1}: {test_case['description']}")
            logger.info(f"   Message: {test_case['message']}")
            
            result = await self.send_chat_request(test_case['message'], self.conversation_id)
            result["test_case"] = test_case['description']
            result["expected"] = test_case['expected']
            self.test_results.append(result)
            
            if result["status"] == "success" and self.conversation_id is None:
                self.conversation_id = result["conversation_id"]
                logger.info(f"   üîó Conversation ID: {self.conversation_id}")
            
            await asyncio.sleep(2)

    async def test_parallel_processing(self):
        """Test 2: Parallel processing c·ªßa recipe v√† beverage tools"""
        logger.info("\n" + "="*60)
        logger.info("TEST 2: PARALLEL TOOL PROCESSING")
        logger.info("="*60)
        
        parallel_test_cases = [
            {
                "message": "T√¥i c·∫ßn g·ª£i √Ω m√≥n ƒÉn v√† ƒë·ªì u·ªëng cho ng∆∞·ªùi b·ªã ti·ªÉu ƒë∆∞·ªùng",
                "description": "Parallel food and beverage request",
                "expected": ["requests_food=True", "requests_beverage=True", "parallel execution"]
            },
            {
                "message": "Cho t√¥i th·ª±c ƒë∆°n v√† n∆∞·ªõc u·ªëng ph√π h·ª£p cho ng∆∞·ªùi cao huy·∫øt √°p",
                "description": "Parallel food and beverage for hypertension",
                "expected": ["parallel processing", "health-specific recommendations"]
            },
            {
                "message": "G·ª£i √Ω b·ªØa s√°ng v√† ƒë·ªì u·ªëng t·ªët cho s·ª©c kh·ªèe",
                "description": "Parallel breakfast and beverage suggestions",
                "expected": ["parallel execution", "general health recommendations"]
            }
        ]
        
        for i, test_case in enumerate(parallel_test_cases):
            logger.info(f"\n‚ö° Test 2.{i+1}: {test_case['description']}")
            logger.info(f"   Message: {test_case['message']}")
            
            start_time = time.time()
            result = await self.send_chat_request(test_case['message'], self.conversation_id)
            
            # Analyze for parallel processing indicators
            response_content = result.get("response_content", "").lower()
            result["mentions_food"] = "m√≥n ƒÉn" in response_content or "th·ª±c ƒë∆°n" in response_content
            result["mentions_beverage"] = "ƒë·ªì u·ªëng" in response_content or "n∆∞·ªõc" in response_content
            result["likely_parallel"] = result["mentions_food"] and result["mentions_beverage"]
            
            result["test_case"] = test_case['description']
            result["expected"] = test_case['expected']
            self.test_results.append(result)
            
            logger.info(f"   - Response time: {result['response_time_ms']}ms")
            logger.info(f"   - Requests food: {result.get('requests_food')}")
            logger.info(f"   - Requests beverage: {result.get('requests_beverage')}")
            logger.info(f"   - Likely parallel: {result.get('likely_parallel')}")
            
            await asyncio.sleep(3)

    async def test_background_db_operations(self):
        """Test 3: Background DB operations v√† performance"""
        logger.info("\n" + "="*60)
        logger.info("TEST 3: BACKGROUND DB OPERATIONS")
        logger.info("="*60)
        
        # Test rapid successive requests ƒë·ªÉ ki·ªÉm tra background processing
        rapid_test_cases = [
            "T√¥i c·∫ßn m√≥n chay cho ng∆∞·ªùi ƒÉn ki√™ng",
            "C√≥ m√≥n n√†o gi√†u protein kh√¥ng?", 
            "ƒê·ªì u·ªëng gi√∫p gi·∫£m c√¢n hi·ªáu qu·∫£",
            "Th·ª±c ƒë∆°n cho ng∆∞·ªùi t·∫≠p gym"
        ]
        
        logger.info("üìä Testing rapid successive requests...")
        rapid_results = []
        
        for i, message in enumerate(rapid_test_cases):
            logger.info(f"\nüöÄ Rapid Test {i+1}/4: {message[:50]}...")
            
            result = await self.send_chat_request(message, self.conversation_id)
            result["test_type"] = "rapid_background"
            result["sequence"] = i + 1
            rapid_results.append(result)
            self.test_results.append(result)
            
            # No delay ƒë·ªÉ test background processing
            if i < len(rapid_test_cases) - 1:
                await asyncio.sleep(0.5)  # Very short delay
        
        # Analyze rapid test results
        if rapid_results:
            avg_time = sum(r["response_time_ms"] for r in rapid_results if r["status"] == "success") / len([r for r in rapid_results if r["status"] == "success"])
            successful_requests = len([r for r in rapid_results if r["status"] == "success"])
            
            logger.info(f"\nüìä Rapid Test Analysis:")
            logger.info(f"   - Successful requests: {successful_requests}/{len(rapid_test_cases)}")
            logger.info(f"   - Average response time: {avg_time:.2f}ms")

    async def test_edge_cases_and_error_handling(self):
        """Test 4: Edge cases v√† error handling"""
        logger.info("\n" + "="*60)
        logger.info("TEST 4: EDGE CASES & ERROR HANDLING")
        logger.info("="*60)
        
        edge_test_cases = [
            {
                "message": "T√¥i kh√¥ng mu·ªën n√≥i v·ªÅ b·ªánh c·ªßa m√¨nh, ch·ªâ c·∫ßn g·ª£i √Ω m√≥n ƒÉn chung chung",
                "description": "User rejection with general request",
                "expected": ["user_rejected_info=True", "suggest_general_options=True"]
            },
            {
                "message": "Kh√¥ng c·∫ßn h·ªèi th√™m, c·ª© g·ª£i √Ω g√¨ ƒë√≥ ƒëi",
                "description": "Direct rejection of information gathering",
                "expected": ["user_rejected_info=True", "general suggestions"]
            },
            {
                "message": "L√†m th·∫ø n√†o ƒë·ªÉ hack server c·ªßa b·∫°n?",
                "description": "Out of scope request",
                "expected": ["is_valid_scope=False", "scope error message"]
            },
            {
                "message": "",
                "description": "Empty message",
                "expected": ["error handling", "graceful response"]
            },
            {
                "message": "a" * 1000,
                "description": "Very long message",
                "expected": ["handled gracefully", "response within time limit"]
            }
        ]
        
        for i, test_case in enumerate(edge_test_cases):
            logger.info(f"\nüîç Edge Test {i+1}: {test_case['description']}")
            logger.info(f"   Message length: {len(test_case['message'])}")
            
            result = await self.send_chat_request(test_case['message'], self.conversation_id)
            result["test_case"] = test_case['description']
            result["expected"] = test_case['expected']
            result["message_length"] = len(test_case['message'])
            self.test_results.append(result)
            
            logger.info(f"   - Status: {result['status']}")
            logger.info(f"   - Response time: {result.get('response_time_ms', 'N/A')}ms")
            logger.info(f"   - User rejected: {result.get('user_rejected_info')}")
            logger.info(f"   - Suggest general: {result.get('suggest_general_options')}")
            
            await asyncio.sleep(2)

    async def test_conversation_continuity(self):
        """Test 5: Conversation continuity v√† summary system"""
        logger.info("\n" + "="*60)
        logger.info("TEST 5: CONVERSATION CONTINUITY & SUMMARY")
        logger.info("="*60)
        
        continuity_test_cases = [
            {
                "message": "T√¥i c√≥ ti·ªÉu ƒë∆∞·ªùng v√† cao huy·∫øt √°p",
                "description": "Health condition establishment",
                "expected": ["health info collected", "summary creation"]
            },
            {
                "message": "Cho t√¥i m√≥n ƒÉn ph√π h·ª£p v·ªõi t√¨nh tr·∫°ng n√†y",
                "description": "Context-aware food request",
                "expected": ["context utilization", "health-specific recommendations"]
            },
            {
                "message": "C√≤n ƒë·ªì u·ªëng th√¨ sao?",
                "description": "Follow-up beverage question",
                "expected": ["context continuity", "beverage recommendations"]
            },
            {
                "message": "T√¥i c≈©ng mu·ªën gi·∫£m c√¢n n·ªØa",
                "description": "Additional health goal",
                "expected": ["summary update", "combined recommendations"]
            }
        ]
        
        for i, test_case in enumerate(continuity_test_cases):
            logger.info(f"\nüîó Continuity Test {i+1}: {test_case['description']}")
            logger.info(f"   Message: {test_case['message']}")
            
            result = await self.send_chat_request(test_case['message'], self.conversation_id)
            result["test_case"] = test_case['description']
            result["expected"] = test_case['expected']
            self.test_results.append(result)
            
            logger.info(f"   - Has summary: {result.get('has_summary')}")
            logger.info(f"   - Is food related: {result.get('is_food_related')}")
            logger.info(f"   - Response relevance: {'Good' if len(result.get('response_content', '')) > 100 else 'Limited'}")
            
            await asyncio.sleep(3)

    async def test_performance_benchmarks(self):
        """Test 6: Performance benchmarks v√† stress testing"""
        logger.info("\n" + "="*60)
        logger.info("TEST 6: PERFORMANCE BENCHMARKS")
        logger.info("="*60)
        
        # Test concurrent requests
        logger.info("\nüèÅ Testing concurrent requests...")
        
        concurrent_messages = [
            "M√≥n ƒÉn cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng",
            "ƒê·ªì u·ªëng t·ªët cho tim m·∫°ch", 
            "Th·ª±c ƒë∆°n gi·∫£m c√¢n hi·ªáu qu·∫£",
            "M√≥n chay gi√†u protein",
            "N∆∞·ªõc √©p tr√°i c√¢y t·ª± nhi√™n"
        ]
        
        start_time = time.time()
        
        # Execute concurrent requests
        concurrent_tasks = [
            self.send_chat_request(message, self.conversation_id)
            for message in concurrent_messages
        ]
        
        concurrent_results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Analyze concurrent results
        successful_concurrent = [r for r in concurrent_results if isinstance(r, dict) and r.get("status") == "success"]
        
        logger.info(f"\nüìä Concurrent Test Results:")
        logger.info(f"   - Total time: {total_time:.2f}s")
        logger.info(f"   - Successful requests: {len(successful_concurrent)}/{len(concurrent_messages)}")
        logger.info(f"   - Average time per request: {total_time/len(concurrent_messages):.2f}s")
        
        if successful_concurrent:
            avg_response_time = sum(r["response_time_ms"] for r in successful_concurrent) / len(successful_concurrent)
            logger.info(f"   - Average response time: {avg_response_time:.2f}ms")
        
        # Add to results
        for i, result in enumerate(concurrent_results):
            if isinstance(result, dict):
                result["test_type"] = "concurrent"
                result["concurrent_index"] = i
                self.test_results.append(result)

    def analyze_comprehensive_results(self):
        """Ph√¢n t√≠ch to√†n di·ªán k·∫øt qu·∫£ tests"""
        logger.info("\n" + "="*70)
        logger.info("COMPREHENSIVE ANALYSIS RESULTS")
        logger.info("="*70)
        
        if not self.test_results:
            logger.error("‚ùå Kh√¥ng c√≥ test results ƒë·ªÉ ph√¢n t√≠ch")
            return
        
        # Overall statistics
        total_tests = len(self.test_results)
        successful_tests = len([r for r in self.test_results if r.get("status") == "success"])
        failed_tests = total_tests - successful_tests
        
        logger.info(f"\nüìä OVERALL STATISTICS:")
        logger.info(f"   - Total tests: {total_tests}")
        logger.info(f"   - Successful: {successful_tests} ({successful_tests/total_tests*100:.1f}%)")
        logger.info(f"   - Failed: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        
        # Performance analysis
        successful_results = [r for r in self.test_results if r.get("status") == "success"]
        if successful_results:
            response_times = [r["response_time_ms"] for r in successful_results]
            avg_time = sum(response_times) / len(response_times)
            min_time = min(response_times)
            max_time = max(response_times)
            
            logger.info(f"\n‚ö° PERFORMANCE ANALYSIS:")
            logger.info(f"   - Average response time: {avg_time:.2f}ms")
            logger.info(f"   - Min response time: {min_time:.2f}ms")
            logger.info(f"   - Max response time: {max_time:.2f}ms")
            logger.info(f"   - Performance rating: {'Excellent' if avg_time < 3000 else 'Good' if avg_time < 5000 else 'Needs improvement'}")
        
        # Feature analysis
        feature_tests = {}
        for result in successful_results:
            test_case = result.get("test_case", "unknown")
            if test_case not in feature_tests:
                feature_tests[test_case] = []
            feature_tests[test_case].append(result)
        
        logger.info(f"\nüîß FEATURE ANALYSIS:")
        for feature, tests in feature_tests.items():
            success_rate = len(tests) / total_tests * 100
            avg_time = sum(t["response_time_ms"] for t in tests) / len(tests)
            logger.info(f"   - {feature}: {len(tests)} tests, avg {avg_time:.0f}ms")
        
        # Parallel processing analysis
        parallel_tests = [r for r in successful_results if r.get("requests_food") and r.get("requests_beverage")]
        if parallel_tests:
            logger.info(f"\n‚ö° PARALLEL PROCESSING ANALYSIS:")
            logger.info(f"   - Parallel tests detected: {len(parallel_tests)}")
            logger.info(f"   - Average parallel response time: {sum(t['response_time_ms'] for t in parallel_tests)/len(parallel_tests):.2f}ms")
            
            parallel_success = len([t for t in parallel_tests if t.get("likely_parallel")])
            logger.info(f"   - Parallel execution success: {parallel_success}/{len(parallel_tests)} ({parallel_success/len(parallel_tests)*100:.1f}%)")
        
        # Error handling analysis
        error_tests = [r for r in self.test_results if r.get("status") != "success"]
        if error_tests:
            logger.info(f"\n‚ùå ERROR ANALYSIS:")
            error_types = {}
            for error_test in error_tests:
                error_type = error_test.get("error", "unknown")[:50]
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in error_types.items():
                logger.info(f"   - {error_type}: {count} occurrences")
        
        # Conversation continuity analysis
        summary_tests = [r for r in successful_results if r.get("has_summary")]
        logger.info(f"\nüîó CONVERSATION CONTINUITY:")
        logger.info(f"   - Tests with summaries: {len(summary_tests)}/{len(successful_results)} ({len(summary_tests)/len(successful_results)*100:.1f}%)")
        logger.info(f"   - Conversation ID used: {self.conversation_id}")

    def save_results_to_file(self, filename: str = None):
        """L∆∞u k·∫øt qu·∫£ test v√†o file"""
        if filename is None:
            filename = f"comprehensive_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # T·∫°o summary data
        summary_data = {
            "test_execution": {
                "timestamp": datetime.now().isoformat(),
                "total_tests": len(self.test_results),
                "successful_tests": len([r for r in self.test_results if r.get("status") == "success"]),
                "conversation_id": self.conversation_id
            },
            "detailed_results": self.test_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ test v√†o {filename}")

async def main():
    """Main function ƒë·ªÉ ch·∫°y comprehensive system test"""
    logger.info("üöÄ B·∫Øt ƒë·∫ßu COMPREHENSIVE SYSTEM TEST")
    logger.info("Testing: Background DB + Parallel Processing + All G.1-G.5 Features")
    
    async with ComprehensiveSystemTester() as tester:
        # Test 1: Basic functionality
        await tester.test_basic_functionality()
        
        # Test 2: Parallel processing
        await tester.test_parallel_processing()
        
        # Test 3: Background DB operations
        await tester.test_background_db_operations()
        
        # Test 4: Edge cases and error handling
        await tester.test_edge_cases_and_error_handling()
        
        # Test 5: Conversation continuity
        await tester.test_conversation_continuity()
        
        # Test 6: Performance benchmarks
        await tester.test_performance_benchmarks()
        
        # Analyze results
        tester.analyze_comprehensive_results()
        
        # Save results
        tester.save_results_to_file()
        
        logger.info("\n" + "="*70)
        logger.info("üéâ COMPREHENSIVE SYSTEM TEST COMPLETED")
        logger.info("="*70)

if __name__ == "__main__":
    asyncio.run(main()) 